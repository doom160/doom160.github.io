{"status":"ok","feed":{"url":"https://medium.com/feed/@kay.renfa","title":"Stories by Kay Ren Fa on Medium","link":"https://medium.com/@kay.renfa?source=rss-2d401c9d729d------2","author":"","description":"Stories by Kay Ren Fa on Medium","image":"https://cdn-images-1.medium.com/fit/c/150/150/0*uNhiU4oiC7gjJDxT"},"items":[{"title":"Dynamic nightmare for Terraforming AWS WAFV2","pubDate":"2023-09-11 11:04:59","link":"https://medium.com/@kay.renfa/dynamic-nightmare-for-terraforming-aws-wafv2-d0513600c25d?source=rss-2d401c9d729d------2","guid":"https://medium.com/p/d0513600c25d","author":"Kay Ren Fa","thumbnail":"","description":"\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/512/0*qioP08OhthASQeug\"></figure><p>A couple of weeks ago, I started working on implementing Web Application Firewall (WAF) on our ALB. A quick introduction of WAF, it is an AWS resource that can be associated to Cloudfront, ALB and/or API Gateway API. It can be used to inspect all request that go through the resource and then it blocks, challenges, captcha, counts or allows the request if it matches certain rules. The rules can be self defined like IP Set, Geolocation or managed rules like Cross-site scripting, SQL Injection.</p>\n<p>Going through the <a href=\"https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/wafv2_web_acl.html\">Terraform documentation</a>, I was blown away how tedious it is to implement the resources. Most of the AWS resources provides granular sub-resource, for example for IAM, there is one for policy, role and one to associate both together.</p>\n<pre>resource \"aws_iam_group\" \"group\" {<br>  name = \"test-group\"<br>}<br><br>data \"aws_iam_policy_document\" \"policy\" {<br>  statement {<br>    effect    = \"Allow\"<br>    actions   = [\"ec2:Describe*\"]<br>    resources = [\"*\"]<br>  }<br>}<br><br>resource \"aws_iam_policy\" \"policy\" {<br>  name        = \"test-policy\"<br>  description = \"A test policy\"<br>  policy      = data.aws_iam_policy_document.policy.json<br>}<br><br>resource \"aws_iam_policy_attachment\" \"test-attach\" {<br>  name       = \"test-attachment\"<br>  groups     = [aws_iam_group.group.name]<br>  policy_arn = aws_iam_policy.policy.arn<br>}</pre>\n<p>Whereas for WAF, it is one single resource that tries to do everything at one go which is very different from others. Even for AWS step function that can have complex rules, it takes in a JSON input of the whole state machine. AWS CodeDeploy/CodePipeline that has a long set of attributes or nested blocks, it wasn\u2019t that painful to implement. Hence, it blew my mind that it doesn\u2019t follow the same design principles like the others. Going through both <a href=\"https://docs.aws.amazon.com/cli/latest/reference/wafv2/update-web-acl.html\">AWS documentation</a> and <a href=\"https://github.com/hashicorp/terraform-provider-aws/blob/b45c86f8550effb632ca60335538ce2a04164d5f/internal/service/wafv2/web_acl.go\">Terraform source code</a>, I guess it is mostly AWS \u201cfault\u201d to implement such a convoluted resource since there are no sub-resource to\u00a0create.</p>\n<p>The reason why it grinds my gear is because having a smaller set of \u201cresources\u201d allows us to have more flexibility to add extensibility from variables without much hassle. For wafv2_acl`, we will have to add numerous dynamic blocks for us to have extensibility. Not saying dynamic blocks are bad, but having too many makes the code unreadable and even harder to visualise the end output as dynamic iterates and generates a set of\u00a0code.</p>\n<p>To refresh everyone\u2019s memory how dynamic block works on Terraform. Dynamic can be used to generate copies of block. Below are 3 snippet of code, where first 2 generate the same output, and 3rd\u00a0doesn\u2019t.</p>\n<pre>//Version A<br>resource \"aws_something\" \"main\" {<br>  rule {<br>    attribute = \"valueA\"<br>  }<br>  rule {<br>    attribute = \"valueB\"<br>  }<br>  rule {<br>    attribute = \"valueC\"<br>  }<br>}<br><br>//Version B<br>variable \"input_variable\" {<br>  type    = list<br>  default = [\"valueA\", \"valueB\", \"valueC\"]<br>}<br><br>resource \"aws_something\" \"main_same\" {<br>  dynamic \"rule\"{<br>    for_each = var.input_variable<br>    content {<br>      attribute = rule.value<br>    }<br>  }<br>}<br><br>//Version C - Wrong example as it create an array of map<br>resource \"aws_something\" \"main_wrong\" { <br>  rule = [for s in var.input_variable : {attribute:s}]<br>}<br>/*<br>resource \"aws_something\" \"main_wrong_output\" { <br>  rule = [<br>          {attribute:\"valueA\"},<br>          {attribute:\"valueB\"}, <br>          {attribute:\"valueC\"}<br>        ]<br>}<br>*/</pre>\n<p>If you compare Version C and Version A, there is a slight difference. Version C takes in an array, but Version A itself is a block of \u201crule\u201d. I have slight bias-ness for using map object as provides more extensibility than dynamic. WAF for terraform further amplify this problem by a few fold with by needing multiple nested\u00a0blocks.</p>\n<p>Though I agree that WAF provides an extensive customisation and not everyone has the same use case. But I think for 80% of the users are simple minded people like me. I use it with a simple use case, scan all requests, block requests that match the rules, else allow. As mentioned, everyone has different needs, hence, there is no silver bullet to implement WAF.</p>\n<p>Fear not, for this 80% group of people, I have simplified the implementation.</p>\n<pre>resource \"aws_wafv2_web_acl\" \"main\" {<br>  name  = \"waf-${var.name}-acl-alb\"<br>  scope = \"REGIONAL\"<br><br>  default_action {<br>    allow {}<br>  }<br><br>  rule {<br>    name     = \"waf-block-ipset\"<br>    priority = 0<br>    action {<br>      block {}<br>    }<br>    statement {<br>      ip_set_reference_statement {<br>        arn = aws_wafv2_ip_set.block.arn<br>      }<br>    }<br>    visibility_config {<br>      cloudwatch_metrics_enabled = true<br>      metric_name                = \"waf-block-ipset\"<br>      sampled_requests_enabled   = true<br>    }<br>  }<br><br>  dynamic \"rule\" {<br>    for_each = toset(var.rules)<br>    content {<br>      name     = rule.value.name<br>      priority = rule.value.priority<br>      override_action {<br>        none {}<br>      }<br>      statement {<br>        managed_rule_group_statement {<br>          name        = rule.value.name<br>          vendor_name = rule.value.vendor_name<br>          dynamic \"managed_rule_group_configs\" {<br>            for_each = rule.value.name == \"AWSManagedRulesBotControlRuleSet\" ? [1] : []<br>            content {<br>              aws_managed_rules_bot_control_rule_set {<br>                inspection_level = \"COMMON\"<br>              }<br>            }<br>          }<br>          dynamic \"rule_action_override\" {<br>            for_each = rule.value.allow<br>            content {<br>              name = rule_action_override.value<br>              action_to_use {<br>                allow {}<br>              }<br>            }<br>          }<br>          dynamic \"rule_action_override\" {<br>            for_each = rule.value.block<br>            content {<br>              name = rule_action_override.value<br>              action_to_use {<br>                block {}<br>              }<br>            }<br>          }<br>          dynamic \"rule_action_override\" {<br>            for_each = rule.value.count<br>            content {<br>              name = rule_action_override.value<br>              action_to_use {<br>                count {}<br>              }<br>            }<br>          }<br>          dynamic \"rule_action_override\" {<br>            for_each = rule.value.challenge<br>            content {<br>              name = rule_action_override.value<br>              action_to_use {<br>                challenge {}<br>              }<br>            }<br>          }<br>          dynamic \"rule_action_override\" {<br>            for_each = rule.value.captcha<br>            content {<br>              name = rule_action_override.value<br>              action_to_use {<br>                captcha {}<br>              }<br>            }<br>          }<br>        }<br>      }<br>      visibility_config {<br>        cloudwatch_metrics_enabled = true<br>        metric_name                = rule.value.name<br>        sampled_requests_enabled   = true<br>      }<br>    }<br>  }<br>  visibility_config {<br>    cloudwatch_metrics_enabled = true<br>    metric_name                = \"cw-${var.name}-waf-alb\"<br>    sampled_requests_enabled   = true<br>  }<br>}<br><br>resource \"aws_wafv2_web_acl_association\" \"main\" {<br>  resource_arn = var.alb.alb_arn<br>  web_acl_arn  = aws_wafv2_web_acl.main.arn<br>}<br><br>resource \"aws_wafv2_ip_set\" \"block\" {<br>  name               = \"waf-${var.name}-block-ipset-alb\"<br>  scope              = \"REGIONAL\"<br>  ip_address_version = \"IPV4\"<br>  addresses          = var.block_ip_set<br>}<br><br>variable \"rules\" {<br>  type = list(any)<br>  default = [{<br>    name        = \"AWSManagedRulesCommonRuleSet\"<br>    vendor_name = \"AWS\"<br>    priority    = 1<br>    allow = [<br>      \"SizeRestrictions_BODY\",<br>      \"CrossSiteScripting_BODY\"<br>    ]<br>    block = [<br>      \"NoUserAgent_HEADER\",<br>      \"UserAgent_BadBots_HEADER\",<br>      \"SizeRestrictions_QUERYSTRING\",<br>      \"SizeRestrictions_Cookie_HEADER\",<br>      \"SizeRestrictions_URIPATH\",<br>      \"EC2MetaDataSSRF_BODY\",<br>      \"EC2MetaDataSSRF_COOKIE\",<br>      \"EC2MetaDataSSRF_URIPATH\",<br>      \"EC2MetaDataSSRF_QUERYARGUMENTS\",<br>      \"GenericLFI_QUERYARGUMENTS\",<br>      \"CrossSiteScripting_URIPATH\",<br>      \"GenericLFI_URIPATH\",<br>      \"GenericLFI_BODY\",<br>      \"RestrictedExtensions_URIPATH\",<br>      \"RestrictedExtensions_QUERYARGUMENTS\",<br>      \"GenericRFI_QUERYARGUMENTS\",<br>      \"GenericRFI_BODY\",<br>      \"GenericRFI_URIPATH\",<br>      \"CrossSiteScripting_COOKIE\",<br>      \"CrossSiteScripting_QUERYARGUMENTS\",<br>    ]<br>    captcha   = []<br>    challenge = []<br>    count     = []<br>    }, {<br>    name        = \"AWSManagedRulesLinuxRuleSet\"<br>    vendor_name = \"AWS\"<br>    priority    = 2<br>    allow       = []<br>    block = [<br>      \"LFI_URIPATH\",<br>      \"LFI_QUERYSTRING\",<br>      \"LFI_HEADER\"<br>    ]<br>    captcha   = []<br>    challenge = []<br>    count     = []<br>    }, {<br>    name        = \"AWSManagedRulesUnixRuleSet\"<br>    vendor_name = \"AWS\"<br>    priority    = 3<br>    allow       = []<br>    block = [<br>      \"UNIXShellCommandsVariables_QUERYARGUMENTS\",<br>      \"UNIXShellCommandsVariables_BODY\",<br>    ]<br>    captcha   = []<br>    challenge = []<br>    count     = []<br>    }, {<br>    name        = \"AWSManagedRulesSQLiRuleSet\"<br>    vendor_name = \"AWS\"<br>    priority    = 4<br>    allow       = []<br>    block = [<br>      \"SQLi_QUERYARGUMENTS\",<br>      \"SQLiExtendedPatterns_QUERYARGUMENTS\",<br>      \"SQLi_BODY\",<br>      \"SQLiExtendedPatterns_BODY\",<br>      \"SQLi_COOKIE\",<br>    ]<br>    captcha   = []<br>    challenge = []<br>    count     = []<br>    }, {<br>    name        = \"AWSManagedRulesKnownBadInputsRuleSet\"<br>    vendor_name = \"AWS\"<br>    priority    = 5<br>    allow       = []<br>    block = [<br>      \"JavaDeserializationRCE_HEADER\",<br>      \"JavaDeserializationRCE_BODY\",<br>      \"JavaDeserializationRCE_URIPATH\",<br>      \"JavaDeserializationRCE_QUERYSTRING\",<br>      \"Host_localhost_HEADER\",<br>      \"PROPFIND_METHOD\",<br>      \"ExploitablePaths_URIPATH\",<br>      \"Log4JRCE_HEADER\",<br>      \"Log4JRCE_QUERYSTRING\",<br>      \"Log4JRCE_BODY\",<br>      \"Log4JRCE_URIPATH\",<br>    ]<br>    captcha   = []<br>    challenge = []<br>    count     = []<br>    }, {<br>    name        = \"AWSManagedRulesBotControlRuleSet\"<br>    vendor_name = \"AWS\"<br>    priority    = 6<br>    allow = [<br>      \"SignalAutomatedBrowser\",<br>      \"CategoryHttpLibrary\",<br>      \"SignalNonBrowserUserAgent\"<br>    ]<br>    block     = []<br>    captcha   = []<br>    challenge = []<br>    count = [<br>      \"CategoryAdvertising\",<br>      \"CategoryArchiver\",<br>      \"CategoryContentFetcher\",<br>      \"CategoryEmailClient\",<br>      \"CategoryLinkChecker\",<br>      \"CategoryMiscellaneous\",<br>      \"CategoryMonitoring\",<br>      \"CategoryScrapingFramework\",<br>      \"CategorySearchEngine\",<br>      \"CategorySecurity\",<br>      \"CategorySeo\",<br>      \"CategorySocialMedia\",<br>      \"CategoryAI\",<br>      \"SignalKnownBotDataCenter\"<br>    ]<br>    }<br>  ]<br>}<br><br>variable \"alb\" {<br>  type        = map(any)<br>  description = \"Map of Application (with alb) dependency outputs\"<br>}<br><br>variable \"name\" {<br>  default     = \"resource\"<br>  type        = string<br>}<br><br>variable \"block_ip_set\" {<br>  default     = []<br>  type        = list(string)<br>  description = \"List of IP to block\"<br>}</pre>\n<p>If you see the `aws_wafv2_web_acl` resource, it has layers of dynamic block. But with the simple list of variables, it is much easier to configure the rules many assumption that we reuse the same name as the rules, and little to no customisation to these rules. At least its less painful to customise the input now! With this code, you can have \u201ccount\u201d for dev and staging environment and \u201cblock\u201d for production with the same implementation.</p>\n<p>I hope the above code solves some of the pain point when introducing WAF ACL. Do note that above does not cover all scenario, hence, feel free to extend to the code! Til next\u00a0time!</p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=d0513600c25d\" width=\"1\" height=\"1\" alt=\"\">\n","content":"\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/512/0*qioP08OhthASQeug\"></figure><p>A couple of weeks ago, I started working on implementing Web Application Firewall (WAF) on our ALB. A quick introduction of WAF, it is an AWS resource that can be associated to Cloudfront, ALB and/or API Gateway API. It can be used to inspect all request that go through the resource and then it blocks, challenges, captcha, counts or allows the request if it matches certain rules. The rules can be self defined like IP Set, Geolocation or managed rules like Cross-site scripting, SQL Injection.</p>\n<p>Going through the <a href=\"https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/wafv2_web_acl.html\">Terraform documentation</a>, I was blown away how tedious it is to implement the resources. Most of the AWS resources provides granular sub-resource, for example for IAM, there is one for policy, role and one to associate both together.</p>\n<pre>resource \"aws_iam_group\" \"group\" {<br>  name = \"test-group\"<br>}<br><br>data \"aws_iam_policy_document\" \"policy\" {<br>  statement {<br>    effect    = \"Allow\"<br>    actions   = [\"ec2:Describe*\"]<br>    resources = [\"*\"]<br>  }<br>}<br><br>resource \"aws_iam_policy\" \"policy\" {<br>  name        = \"test-policy\"<br>  description = \"A test policy\"<br>  policy      = data.aws_iam_policy_document.policy.json<br>}<br><br>resource \"aws_iam_policy_attachment\" \"test-attach\" {<br>  name       = \"test-attachment\"<br>  groups     = [aws_iam_group.group.name]<br>  policy_arn = aws_iam_policy.policy.arn<br>}</pre>\n<p>Whereas for WAF, it is one single resource that tries to do everything at one go which is very different from others. Even for AWS step function that can have complex rules, it takes in a JSON input of the whole state machine. AWS CodeDeploy/CodePipeline that has a long set of attributes or nested blocks, it wasn\u2019t that painful to implement. Hence, it blew my mind that it doesn\u2019t follow the same design principles like the others. Going through both <a href=\"https://docs.aws.amazon.com/cli/latest/reference/wafv2/update-web-acl.html\">AWS documentation</a> and <a href=\"https://github.com/hashicorp/terraform-provider-aws/blob/b45c86f8550effb632ca60335538ce2a04164d5f/internal/service/wafv2/web_acl.go\">Terraform source code</a>, I guess it is mostly AWS \u201cfault\u201d to implement such a convoluted resource since there are no sub-resource to\u00a0create.</p>\n<p>The reason why it grinds my gear is because having a smaller set of \u201cresources\u201d allows us to have more flexibility to add extensibility from variables without much hassle. For wafv2_acl`, we will have to add numerous dynamic blocks for us to have extensibility. Not saying dynamic blocks are bad, but having too many makes the code unreadable and even harder to visualise the end output as dynamic iterates and generates a set of\u00a0code.</p>\n<p>To refresh everyone\u2019s memory how dynamic block works on Terraform. Dynamic can be used to generate copies of block. Below are 3 snippet of code, where first 2 generate the same output, and 3rd\u00a0doesn\u2019t.</p>\n<pre>//Version A<br>resource \"aws_something\" \"main\" {<br>  rule {<br>    attribute = \"valueA\"<br>  }<br>  rule {<br>    attribute = \"valueB\"<br>  }<br>  rule {<br>    attribute = \"valueC\"<br>  }<br>}<br><br>//Version B<br>variable \"input_variable\" {<br>  type    = list<br>  default = [\"valueA\", \"valueB\", \"valueC\"]<br>}<br><br>resource \"aws_something\" \"main_same\" {<br>  dynamic \"rule\"{<br>    for_each = var.input_variable<br>    content {<br>      attribute = rule.value<br>    }<br>  }<br>}<br><br>//Version C - Wrong example as it create an array of map<br>resource \"aws_something\" \"main_wrong\" { <br>  rule = [for s in var.input_variable : {attribute:s}]<br>}<br>/*<br>resource \"aws_something\" \"main_wrong_output\" { <br>  rule = [<br>          {attribute:\"valueA\"},<br>          {attribute:\"valueB\"}, <br>          {attribute:\"valueC\"}<br>        ]<br>}<br>*/</pre>\n<p>If you compare Version C and Version A, there is a slight difference. Version C takes in an array, but Version A itself is a block of \u201crule\u201d. I have slight bias-ness for using map object as provides more extensibility than dynamic. WAF for terraform further amplify this problem by a few fold with by needing multiple nested\u00a0blocks.</p>\n<p>Though I agree that WAF provides an extensive customisation and not everyone has the same use case. But I think for 80% of the users are simple minded people like me. I use it with a simple use case, scan all requests, block requests that match the rules, else allow. As mentioned, everyone has different needs, hence, there is no silver bullet to implement WAF.</p>\n<p>Fear not, for this 80% group of people, I have simplified the implementation.</p>\n<pre>resource \"aws_wafv2_web_acl\" \"main\" {<br>  name  = \"waf-${var.name}-acl-alb\"<br>  scope = \"REGIONAL\"<br><br>  default_action {<br>    allow {}<br>  }<br><br>  rule {<br>    name     = \"waf-block-ipset\"<br>    priority = 0<br>    action {<br>      block {}<br>    }<br>    statement {<br>      ip_set_reference_statement {<br>        arn = aws_wafv2_ip_set.block.arn<br>      }<br>    }<br>    visibility_config {<br>      cloudwatch_metrics_enabled = true<br>      metric_name                = \"waf-block-ipset\"<br>      sampled_requests_enabled   = true<br>    }<br>  }<br><br>  dynamic \"rule\" {<br>    for_each = toset(var.rules)<br>    content {<br>      name     = rule.value.name<br>      priority = rule.value.priority<br>      override_action {<br>        none {}<br>      }<br>      statement {<br>        managed_rule_group_statement {<br>          name        = rule.value.name<br>          vendor_name = rule.value.vendor_name<br>          dynamic \"managed_rule_group_configs\" {<br>            for_each = rule.value.name == \"AWSManagedRulesBotControlRuleSet\" ? [1] : []<br>            content {<br>              aws_managed_rules_bot_control_rule_set {<br>                inspection_level = \"COMMON\"<br>              }<br>            }<br>          }<br>          dynamic \"rule_action_override\" {<br>            for_each = rule.value.allow<br>            content {<br>              name = rule_action_override.value<br>              action_to_use {<br>                allow {}<br>              }<br>            }<br>          }<br>          dynamic \"rule_action_override\" {<br>            for_each = rule.value.block<br>            content {<br>              name = rule_action_override.value<br>              action_to_use {<br>                block {}<br>              }<br>            }<br>          }<br>          dynamic \"rule_action_override\" {<br>            for_each = rule.value.count<br>            content {<br>              name = rule_action_override.value<br>              action_to_use {<br>                count {}<br>              }<br>            }<br>          }<br>          dynamic \"rule_action_override\" {<br>            for_each = rule.value.challenge<br>            content {<br>              name = rule_action_override.value<br>              action_to_use {<br>                challenge {}<br>              }<br>            }<br>          }<br>          dynamic \"rule_action_override\" {<br>            for_each = rule.value.captcha<br>            content {<br>              name = rule_action_override.value<br>              action_to_use {<br>                captcha {}<br>              }<br>            }<br>          }<br>        }<br>      }<br>      visibility_config {<br>        cloudwatch_metrics_enabled = true<br>        metric_name                = rule.value.name<br>        sampled_requests_enabled   = true<br>      }<br>    }<br>  }<br>  visibility_config {<br>    cloudwatch_metrics_enabled = true<br>    metric_name                = \"cw-${var.name}-waf-alb\"<br>    sampled_requests_enabled   = true<br>  }<br>}<br><br>resource \"aws_wafv2_web_acl_association\" \"main\" {<br>  resource_arn = var.alb.alb_arn<br>  web_acl_arn  = aws_wafv2_web_acl.main.arn<br>}<br><br>resource \"aws_wafv2_ip_set\" \"block\" {<br>  name               = \"waf-${var.name}-block-ipset-alb\"<br>  scope              = \"REGIONAL\"<br>  ip_address_version = \"IPV4\"<br>  addresses          = var.block_ip_set<br>}<br><br>variable \"rules\" {<br>  type = list(any)<br>  default = [{<br>    name        = \"AWSManagedRulesCommonRuleSet\"<br>    vendor_name = \"AWS\"<br>    priority    = 1<br>    allow = [<br>      \"SizeRestrictions_BODY\",<br>      \"CrossSiteScripting_BODY\"<br>    ]<br>    block = [<br>      \"NoUserAgent_HEADER\",<br>      \"UserAgent_BadBots_HEADER\",<br>      \"SizeRestrictions_QUERYSTRING\",<br>      \"SizeRestrictions_Cookie_HEADER\",<br>      \"SizeRestrictions_URIPATH\",<br>      \"EC2MetaDataSSRF_BODY\",<br>      \"EC2MetaDataSSRF_COOKIE\",<br>      \"EC2MetaDataSSRF_URIPATH\",<br>      \"EC2MetaDataSSRF_QUERYARGUMENTS\",<br>      \"GenericLFI_QUERYARGUMENTS\",<br>      \"CrossSiteScripting_URIPATH\",<br>      \"GenericLFI_URIPATH\",<br>      \"GenericLFI_BODY\",<br>      \"RestrictedExtensions_URIPATH\",<br>      \"RestrictedExtensions_QUERYARGUMENTS\",<br>      \"GenericRFI_QUERYARGUMENTS\",<br>      \"GenericRFI_BODY\",<br>      \"GenericRFI_URIPATH\",<br>      \"CrossSiteScripting_COOKIE\",<br>      \"CrossSiteScripting_QUERYARGUMENTS\",<br>    ]<br>    captcha   = []<br>    challenge = []<br>    count     = []<br>    }, {<br>    name        = \"AWSManagedRulesLinuxRuleSet\"<br>    vendor_name = \"AWS\"<br>    priority    = 2<br>    allow       = []<br>    block = [<br>      \"LFI_URIPATH\",<br>      \"LFI_QUERYSTRING\",<br>      \"LFI_HEADER\"<br>    ]<br>    captcha   = []<br>    challenge = []<br>    count     = []<br>    }, {<br>    name        = \"AWSManagedRulesUnixRuleSet\"<br>    vendor_name = \"AWS\"<br>    priority    = 3<br>    allow       = []<br>    block = [<br>      \"UNIXShellCommandsVariables_QUERYARGUMENTS\",<br>      \"UNIXShellCommandsVariables_BODY\",<br>    ]<br>    captcha   = []<br>    challenge = []<br>    count     = []<br>    }, {<br>    name        = \"AWSManagedRulesSQLiRuleSet\"<br>    vendor_name = \"AWS\"<br>    priority    = 4<br>    allow       = []<br>    block = [<br>      \"SQLi_QUERYARGUMENTS\",<br>      \"SQLiExtendedPatterns_QUERYARGUMENTS\",<br>      \"SQLi_BODY\",<br>      \"SQLiExtendedPatterns_BODY\",<br>      \"SQLi_COOKIE\",<br>    ]<br>    captcha   = []<br>    challenge = []<br>    count     = []<br>    }, {<br>    name        = \"AWSManagedRulesKnownBadInputsRuleSet\"<br>    vendor_name = \"AWS\"<br>    priority    = 5<br>    allow       = []<br>    block = [<br>      \"JavaDeserializationRCE_HEADER\",<br>      \"JavaDeserializationRCE_BODY\",<br>      \"JavaDeserializationRCE_URIPATH\",<br>      \"JavaDeserializationRCE_QUERYSTRING\",<br>      \"Host_localhost_HEADER\",<br>      \"PROPFIND_METHOD\",<br>      \"ExploitablePaths_URIPATH\",<br>      \"Log4JRCE_HEADER\",<br>      \"Log4JRCE_QUERYSTRING\",<br>      \"Log4JRCE_BODY\",<br>      \"Log4JRCE_URIPATH\",<br>    ]<br>    captcha   = []<br>    challenge = []<br>    count     = []<br>    }, {<br>    name        = \"AWSManagedRulesBotControlRuleSet\"<br>    vendor_name = \"AWS\"<br>    priority    = 6<br>    allow = [<br>      \"SignalAutomatedBrowser\",<br>      \"CategoryHttpLibrary\",<br>      \"SignalNonBrowserUserAgent\"<br>    ]<br>    block     = []<br>    captcha   = []<br>    challenge = []<br>    count = [<br>      \"CategoryAdvertising\",<br>      \"CategoryArchiver\",<br>      \"CategoryContentFetcher\",<br>      \"CategoryEmailClient\",<br>      \"CategoryLinkChecker\",<br>      \"CategoryMiscellaneous\",<br>      \"CategoryMonitoring\",<br>      \"CategoryScrapingFramework\",<br>      \"CategorySearchEngine\",<br>      \"CategorySecurity\",<br>      \"CategorySeo\",<br>      \"CategorySocialMedia\",<br>      \"CategoryAI\",<br>      \"SignalKnownBotDataCenter\"<br>    ]<br>    }<br>  ]<br>}<br><br>variable \"alb\" {<br>  type        = map(any)<br>  description = \"Map of Application (with alb) dependency outputs\"<br>}<br><br>variable \"name\" {<br>  default     = \"resource\"<br>  type        = string<br>}<br><br>variable \"block_ip_set\" {<br>  default     = []<br>  type        = list(string)<br>  description = \"List of IP to block\"<br>}</pre>\n<p>If you see the `aws_wafv2_web_acl` resource, it has layers of dynamic block. But with the simple list of variables, it is much easier to configure the rules many assumption that we reuse the same name as the rules, and little to no customisation to these rules. At least its less painful to customise the input now! With this code, you can have \u201ccount\u201d for dev and staging environment and \u201cblock\u201d for production with the same implementation.</p>\n<p>I hope the above code solves some of the pain point when introducing WAF ACL. Do note that above does not cover all scenario, hence, feel free to extend to the code! Til next\u00a0time!</p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=d0513600c25d\" width=\"1\" height=\"1\" alt=\"\">\n","enclosure":{},"categories":["terraform","aws"]},{"title":"Multiple ECS Deploy via Gitlab","pubDate":"2023-08-09 06:24:50","link":"https://medium.com/@kay.renfa/multiple-ecs-deploy-via-gitlab-5334ce258472?source=rss-2d401c9d729d------2","guid":"https://medium.com/p/5334ce258472","author":"Kay Ren Fa","thumbnail":"","description":"\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/512/0*8sCTAQyjMY6CHlap\"></figure><p>Gitlab provides a way to allow one to <a href=\"https://docs.gitlab.com/ee/ci/cloud_deployment/ecs/deploy_to_aws_ecs.html\">deploy your containerised application to ECS</a>. Documentation briefly explains to you that you need to provide some of the variables like the ECS Cluster, Service, etc but it doesn\u2019t go into detail how it works under the\u00a0hood.</p>\n<p>In my opinion, it will be challenging if I do not know what is happening in the deployment process. Specifically, does it handle pushing the images? How does it handle the task definition? Is it able to deploy multiple services under single job? Does it allow blue-green deployment? And so\u00a0on.</p>\n<p>In order for me to clear my doubt, I had to zoom into Gitlab\u2019s implementation of their template code. For the curious minds, it can be found\u00a0<a href=\"https://gitlab.com/gitlab-org/cloud-deploy/-/blob/master/aws/src/bin/ecs?ref_type=heads\">here</a>.</p>\n<p>After studying the code, I start to understand what is happening. The code provides two ways to\u00a0deploy.</p>\n<p>The first way is when you don\u2019t specify CI_AWS_ECS_TASK_DEFINITION_FILE variable which it will then fetching task definition from CI_AWS_ECS_TASK_DEFINITION\u00a0. Using the latest task definition, it will do couple of jq operation to replace the first container image name to your image name and tag. This is really neat for basic usage as it registers a new revision every time which means it gives you a way to rollback your service to previous revision. Alternative will be just redeploying your code via Gitlab. Just by using this method, you are better than many teams because some might just go via the shortcut manner, reusing the same tag such as latest and then stopping the task to refresh the container images.</p>\n<p>Second way is simply provide task definition as a file. It reads the file and deploy as it is. I guess the general expectation to use this method is to commit your task definition file into your git repository. This essentially means you will end up doing what I just mentioned which is fixate on a particular image tag. Alternatively, you might need to overwrite the default job and add before_script to dynamically generate these new task definition file.</p>\n<p>In summary, these are some of the shortfalls</p>\n<ol>\n<li>It doesn\u2019t handle image\u00a0pushing</li>\n<li>It doesn\u2019t allow deployment of multiple ECS\u00a0Services</li>\n<li>It doesn\u2019t support blue green deployment</li>\n</ol>\n<p>As covered in my recent post on <a href=\"https://medium.com/@kay.renfa/ci-cd-pipeline-design-principles-crafts-a62a3f79c660\">CI/CD Pipeline Design Principles</a>, I mentioned about one point on completeness. One of my use case is I have 3 AWS account which means I will have 3 ECR (i.e. development, staging and production). Hence, it doesn\u2019t fulfil my criteria because I need one stage to push my image before I can deploy the new task definition and\u00a0service.</p>\n<p>Another challenge as part of completeness is that my application is single git repository that builds multiple container images and these images are tightly coupled, with frontend and backend service. Hence, I don\u2019t want it to be stuck in a position where the application is partially deploy as it might have unforeseen problem. Hence, the current method doesn\u2019t meet my\u00a0needs.</p>\n<p>Thirdly, I have a use case where the developers want to display the semantic version of the code deployed to the environment on the frontend application. This means that I will need a way to dynamically modify the task definition based on variable in additional to the image\u00a0tag.</p>\n<p>Lastly, one of the image I deployed is used for Database migration which I run it as ECS task which triggers from Eventbridge image pushevent. This task gets created, apply database migration and end gracefully. Essentially, this is not an ECS\u00a0Service.</p>\n<p>How to best tackle these problem with the least amount of work? Well I had to re-engineer the ECS deploy method that gitlab has provided and reusing that they have provided.</p>\n<p>To summarise what are my 4\u00a0needs:</p>\n<ol>\n<li>To authenticate, pull, retag, re-authenticate and push multiple images to destination ECR</li>\n<li>To have a way to deploy multiple\u00a0services</li>\n<li>To have a way to modify multiple task definitions before deploying</li>\n<li>Not all ECS Service need to be deployed, just need the image\u00a0push</li>\n</ol>\n<p>I developed the below snippet of code to tackle all of these\u00a0problem.</p>\n<pre>.ecs_deploy:<br>  image: \"${CI_TEMPLATE_REGISTRY_HOST}/gitlab-org/cloud-deploy/aws-ecs:latest\"<br>  services:<br>     - docker:20.10.16-dind<br>  variables:<br>    DOCKER_HOST: tcp://docker:2376<br>    DOCKER_TLS_CERTDIR: \"/certs\"<br>    DOCKER_TLS_VERIFY: 1<br>    DOCKER_CERT_PATH: \"$DOCKER_TLS_CERTDIR/client\"<br>    SRC_AWS_PROFILE: \"default\"  # Source AWS Profile<br>    SRC_IMAGES_AND_TAG: \"\"      # Container image and tag i.e. without account number<br>    SRC_ECR_REGISTRY: \"\"        # ECR Registry URL<br>    DEST_ECR_REGISTRY: \"\"       # Destination ECR Registry URL<br>    DEST_AWS_PROFILE: \"default\" # Destination AWS Profile<br>    DEST_IMAGES_AND_TAG: \"\"     # Destination Image and tag<br>    DEST_TASK_DEFINITIONS: \"\"   # Destination Task definition to pull from<br>    DEST_ECS_SERVICE: \"\"        # Destination ECS Service<br>    CI_AWS_ECS_CLUSTER: \"\"      # Destination ECS Cluster Name<br>    APP_CONTAINER_NAME: \"app\"   # Main Application Container Name<br>  before_script:<br>    - until docker info; do sleep 1; done<br>  script:<br>    - if [[ -z \"$SRC_ECR_REGISTRY\" || -z \"$SRC_IMAGES_AND_TAG\" || -z \"$DEST_ECR_REGISTRY\" || -z \"$DEST_IMAGES_AND_TAG\" || -z \"$DEST_TASK_DEFINITIONS\" || -z \"$DEST_ECS_SERVICE\" || -z \"$CI_AWS_ECS_CLUSTER\" || -z \"$DEST_ECS_SERVICE\" ]]; then echo \".ecs_deploy - missing mandatory variables\"; env; exit 1; fi<br>    - IMAGE_VERSION=\"${CI_COMMIT_TAG:-$CI_COMMIT_SHORT_SHA}\"<br>    - ARRAY_SRC_IMAGES_AND_TAG=(${SRC_IMAGES_AND_TAG//,/ })<br>    - ARRAY_DEST_IMAGES_AND_TAG=(${DEST_IMAGES_AND_TAG//,/ })<br>    - ARRAY_DEST_TASK_DEFINITIONS=(${DEST_TASK_DEFINITIONS//,/ })<br>    - ARRAY_DEST_ECS_SERVICE=(${DEST_ECS_SERVICE//,/ })<br>    - | <br>      if [ ${#ARRAY_SRC_IMAGES_AND_TAG[@]} -ne ${#ARRAY_DEST_IMAGES_AND_TAG[@]} ];<br>      then<br>          echo ecs_deploy: Unequal number of images<br>          exit 1<br>      fi<br>      if [ ${#ARRAY_DEST_TASK_DEFINITIONS[@]} -ne ${#ARRAY_DEST_IMAGES_AND_TAG[@]} ];<br>      then<br>          echo ecs_deploy: Unequal number of task definition<br>          exit 1<br>      fi<br>      if [ ${#ARRAY_DEST_ECS_SERVICE[@]} -ne ${#ARRAY_DEST_IMAGES_AND_TAG[@]} ];<br>      then<br>          echo ecs_deploy: Unequal number of ecs service<br>          exit 1<br>      fi<br># Pulling image from different source<br>    - |<br>      if [[ ! -z \"$SRC_AWS_PROFILE\" ]]<br>      then <br>        export AWS_PROFILE=$SRC_AWS_PROFILE<br>        aws ecr get-login-password --region ap-southeast-1 | docker login --username AWS --password-stdin $SRC_ECR_REGISTRY<br>      else<br>        crane auth login -u $CI_REGISTRY_USER -p $CI_REGISTRY_PASSWORD $CI_REGISTRY<br>      fi<br># Pulling all images<br>    - |<br>      for i in \"${!ARRAY_SRC_IMAGES_AND_TAG[@]}\"; do<br>        SRC_IMAGE_NAME=${ARRAY_SRC_IMAGES_AND_TAG[i]%%:*}<br>        SRC_IMAGE_TAG=${ARRAY_SRC_IMAGES_AND_TAG[i]##*:}<br>        DEST_IMAGE_NAME=${ARRAY_DEST_IMAGES_AND_TAG[i]%%:*}<br>        DEST_IMAGE_TAG=${ARRAY_DEST_IMAGES_AND_TAG[i]##*:}<br>        docker pull $SRC_ECR_REGISTRY/${ARRAY_SRC_IMAGES_AND_TAG[i]}<br>        docker tag $SRC_ECR_REGISTRY/${ARRAY_SRC_IMAGES_AND_TAG[i]} $DEST_ECR_REGISTRY/${ARRAY_DEST_IMAGES_AND_TAG[i]} <br>        docker tag $SRC_ECR_REGISTRY/${ARRAY_SRC_IMAGES_AND_TAG[i]} $DEST_ECR_REGISTRY/$DEST_IMAGE_NAME:$IMAGE_VERSION<br>        docker tag $SRC_ECR_REGISTRY/${ARRAY_SRC_IMAGES_AND_TAG[i]} $DEST_ECR_REGISTRY/$DEST_IMAGE_NAME:$CI_COMMIT_SHA<br>      done<br>    - export AWS_PROFILE=$DEST_AWS_PROFILE<br>    - aws ecr get-login-password --region ap-southeast-1 | docker login --username AWS --password-stdin $DEST_ECR_REGISTRY<br># Pushing all images, by tagging based on what was given, git hash and git tag if any<br>    - |<br>      for i in \"${!ARRAY_DEST_IMAGES_AND_TAG[@]}\"; do<br>        DEST_IMAGE_NAME=${ARRAY_DEST_IMAGES_AND_TAG[i]%%:*}<br>        DEST_IMAGE_TAG=${ARRAY_DEST_IMAGES_AND_TAG[i]##*:}<br>        docker push $DEST_ECR_REGISTRY/${ARRAY_DEST_IMAGES_AND_TAG[i]}<br>        docker push $DEST_ECR_REGISTRY/$DEST_IMAGE_NAME:$IMAGE_VERSION<br>        docker push $DEST_ECR_REGISTRY/$DEST_IMAGE_NAME:$CI_COMMIT_SHA<br>      done<br>    - mkdir -p manifests<br># Pull the latest task definition, not the one currently in use<br># Replace the image tag with image:githash<br># Replace env variable to VERSION_COMMIT to commit id<br># Replace env variable to VERSION_TAG to git tag if any, else git hash<br># Replace redudant fields if not it will throw error<br># if we provide nil, it will not trigger deployment. i.e. we just want to push image<br>    - |<br>      for i in \"${!ARRAY_DEST_TASK_DEFINITIONS[@]}\"; do<br>        if [[ ${ARRAY_DEST_TASK_DEFINITIONS[i]} != \"nil\" ]]<br>        then<br>          echo \"ecs_deploy: Generating Task Definition ${ARRAY_DEST_ECS_SERVICE[i]} to ${CI_AWS_ECS_CLUSTER}\"<br>          DEST_IMAGE_NAME=${ARRAY_DEST_IMAGES_AND_TAG[i]%%:*}<br>          DEST_IMAGE_TAG=${ARRAY_DEST_IMAGES_AND_TAG[i]##*:}<br>          aws ecs describe-task-definition --output json --task-definition ${ARRAY_DEST_TASK_DEFINITIONS[i]} --query taskDefinition &gt; template_${ARRAY_DEST_TASK_DEFINITIONS[i]}.json<br>          jq --arg APP_CONTAINER_NAME $APP_CONTAINER_NAME --arg IMAGE_NAME \"$DEST_ECR_REGISTRY/$DEST_IMAGE_NAME:$CI_COMMIT_SHA\" --arg IMAGE_VERSION $IMAGE_VERSION --arg DEST_IMAGE_NAME $DEST_IMAGE_NAME --arg CI_COMMIT_SHA $CI_COMMIT_SHA 'del(.taskDefinitionArn, .revision, .status, .requiresAttributes, .compatibilities, .registeredAt, .registeredBy) | .containerDefinitions[] |= <br>            if .name == $APP_CONTAINER_NAME then<br>              .image = $IMAGE_NAME |<br>              (.environment[] | select(.name == \"VERSION_COMMIT\")).value = $CI_COMMIT_SHA |<br>              (.environment[] | select(.name == \"VERSION_TAG\")).value = $IMAGE_VERSION<br>            else<br>              .<br>            end' template_${ARRAY_DEST_TASK_DEFINITIONS[i]}.json &gt; manifests/${ARRAY_DEST_TASK_DEFINITIONS[i]}.json<br>          export CI_AWS_ECS_TASK_DEFINITION_FILE=manifests/${ARRAY_DEST_TASK_DEFINITIONS[i]}.json<br>          export CI_AWS_ECS_SERVICE=${ARRAY_DEST_ECS_SERVICE[i]}<br>          echo \"ecs_deploy: Deploying to ${ARRAY_DEST_ECS_SERVICE[i]} to ${CI_AWS_ECS_CLUSTER}\"<br>          ecs update-task-definition<br>        else<br>          echo \"ecs_deploy: Skipping deployment for ${ARRAY_DEST_IMAGES_AND_TAG[i]}\"<br>        fi<br>      done<br>  artifacts:<br>    paths:<br>      - manifests<br>  tags:<br>    - privileged</pre>\n<p>In order to start using the above code, firstly you need to have Gitlab runner that supports privileged to run docker in docker service. This is essentially required to retag the\u00a0image.</p>\n<p>Fortunately, this image provided by Gitlab has docker installed, which I do not need to build another image just to get it to work. But if you don\u2019t have a way to run docker in docker, you might want to rebuild one with crane or skopeo\u00a0.</p>\n<p>As Gitlab doesn\u2019t have a way to provide a list of string values, I am chaining a list of values with comma delimited which I will then split these values and loop\u00a0them.</p>\n<p>There are couple of validation at the start to ensure the number ECS Service count matches number of Task Definition and Container Images.</p>\n<p>It starts off to authenticate with container registry, pull the images, retag and re-authenticate with destination ECR and push the images. As you can see here, I am pulling 1 tag but pushing 3 tags i.e. commit id, provided tag and git tag if any else short git commit\u00a0id.</p>\n<p>Next, it will pull the destination task definition. I am using jq to manipulate the task definition by replacing the container image for the container what has the same name provided in APP_CONTAINER_NAME. This is a different approach as Gitlab way, by not assuming the first container is the container that I want to update. The jq command also update VERSION_TAG and VERSION_COMMIT which inject the git tag and git hash into the task definiton. This way, the application can look up these environment variables to display the semantic version on the\u00a0UI.</p>\n<p>Once the jq operation is done, it saves as a file and we will call Gitlab\u2019s ecs update-task-definiton\u00a0. This will then use the second deployment method i.e. deploy task definition via local\u00a0file.</p>\n<p>I have also added a condition where if I provide nil to DEST_TASK_DEFINITIONS, it will skip the ECS deployment. This gives me the flexibility to not needing provide 3 ECS services.</p>\n<p>Lastly, the job also publish the generated task definition as artifact, which may be useful for verification.</p>\n<p>This method also give me the flexibility to not commit the task definition into the git repository. Ideally, I want to manage the task definition in my terraform code and let gitlab handle the updating of the remaining values. If there are changes on the terraform code, I do not need to update the task definition into the application git repository. This essentially makes my terraform code base the single source of\u00a0truth!</p>\n<p>To end this article, I hope some of these thought process will be helpful, especially the part when I relate back the previous article I have written. Do feel free to reach out if this has been useful to your\u00a0work!</p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=5334ce258472\" width=\"1\" height=\"1\" alt=\"\">\n","content":"\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/512/0*8sCTAQyjMY6CHlap\"></figure><p>Gitlab provides a way to allow one to <a href=\"https://docs.gitlab.com/ee/ci/cloud_deployment/ecs/deploy_to_aws_ecs.html\">deploy your containerised application to ECS</a>. Documentation briefly explains to you that you need to provide some of the variables like the ECS Cluster, Service, etc but it doesn\u2019t go into detail how it works under the\u00a0hood.</p>\n<p>In my opinion, it will be challenging if I do not know what is happening in the deployment process. Specifically, does it handle pushing the images? How does it handle the task definition? Is it able to deploy multiple services under single job? Does it allow blue-green deployment? And so\u00a0on.</p>\n<p>In order for me to clear my doubt, I had to zoom into Gitlab\u2019s implementation of their template code. For the curious minds, it can be found\u00a0<a href=\"https://gitlab.com/gitlab-org/cloud-deploy/-/blob/master/aws/src/bin/ecs?ref_type=heads\">here</a>.</p>\n<p>After studying the code, I start to understand what is happening. The code provides two ways to\u00a0deploy.</p>\n<p>The first way is when you don\u2019t specify CI_AWS_ECS_TASK_DEFINITION_FILE variable which it will then fetching task definition from CI_AWS_ECS_TASK_DEFINITION\u00a0. Using the latest task definition, it will do couple of jq operation to replace the first container image name to your image name and tag. This is really neat for basic usage as it registers a new revision every time which means it gives you a way to rollback your service to previous revision. Alternative will be just redeploying your code via Gitlab. Just by using this method, you are better than many teams because some might just go via the shortcut manner, reusing the same tag such as latest and then stopping the task to refresh the container images.</p>\n<p>Second way is simply provide task definition as a file. It reads the file and deploy as it is. I guess the general expectation to use this method is to commit your task definition file into your git repository. This essentially means you will end up doing what I just mentioned which is fixate on a particular image tag. Alternatively, you might need to overwrite the default job and add before_script to dynamically generate these new task definition file.</p>\n<p>In summary, these are some of the shortfalls</p>\n<ol>\n<li>It doesn\u2019t handle image\u00a0pushing</li>\n<li>It doesn\u2019t allow deployment of multiple ECS\u00a0Services</li>\n<li>It doesn\u2019t support blue green deployment</li>\n</ol>\n<p>As covered in my recent post on <a href=\"https://medium.com/@kay.renfa/ci-cd-pipeline-design-principles-crafts-a62a3f79c660\">CI/CD Pipeline Design Principles</a>, I mentioned about one point on completeness. One of my use case is I have 3 AWS account which means I will have 3 ECR (i.e. development, staging and production). Hence, it doesn\u2019t fulfil my criteria because I need one stage to push my image before I can deploy the new task definition and\u00a0service.</p>\n<p>Another challenge as part of completeness is that my application is single git repository that builds multiple container images and these images are tightly coupled, with frontend and backend service. Hence, I don\u2019t want it to be stuck in a position where the application is partially deploy as it might have unforeseen problem. Hence, the current method doesn\u2019t meet my\u00a0needs.</p>\n<p>Thirdly, I have a use case where the developers want to display the semantic version of the code deployed to the environment on the frontend application. This means that I will need a way to dynamically modify the task definition based on variable in additional to the image\u00a0tag.</p>\n<p>Lastly, one of the image I deployed is used for Database migration which I run it as ECS task which triggers from Eventbridge image pushevent. This task gets created, apply database migration and end gracefully. Essentially, this is not an ECS\u00a0Service.</p>\n<p>How to best tackle these problem with the least amount of work? Well I had to re-engineer the ECS deploy method that gitlab has provided and reusing that they have provided.</p>\n<p>To summarise what are my 4\u00a0needs:</p>\n<ol>\n<li>To authenticate, pull, retag, re-authenticate and push multiple images to destination ECR</li>\n<li>To have a way to deploy multiple\u00a0services</li>\n<li>To have a way to modify multiple task definitions before deploying</li>\n<li>Not all ECS Service need to be deployed, just need the image\u00a0push</li>\n</ol>\n<p>I developed the below snippet of code to tackle all of these\u00a0problem.</p>\n<pre>.ecs_deploy:<br>  image: \"${CI_TEMPLATE_REGISTRY_HOST}/gitlab-org/cloud-deploy/aws-ecs:latest\"<br>  services:<br>     - docker:20.10.16-dind<br>  variables:<br>    DOCKER_HOST: tcp://docker:2376<br>    DOCKER_TLS_CERTDIR: \"/certs\"<br>    DOCKER_TLS_VERIFY: 1<br>    DOCKER_CERT_PATH: \"$DOCKER_TLS_CERTDIR/client\"<br>    SRC_AWS_PROFILE: \"default\"  # Source AWS Profile<br>    SRC_IMAGES_AND_TAG: \"\"      # Container image and tag i.e. without account number<br>    SRC_ECR_REGISTRY: \"\"        # ECR Registry URL<br>    DEST_ECR_REGISTRY: \"\"       # Destination ECR Registry URL<br>    DEST_AWS_PROFILE: \"default\" # Destination AWS Profile<br>    DEST_IMAGES_AND_TAG: \"\"     # Destination Image and tag<br>    DEST_TASK_DEFINITIONS: \"\"   # Destination Task definition to pull from<br>    DEST_ECS_SERVICE: \"\"        # Destination ECS Service<br>    CI_AWS_ECS_CLUSTER: \"\"      # Destination ECS Cluster Name<br>    APP_CONTAINER_NAME: \"app\"   # Main Application Container Name<br>  before_script:<br>    - until docker info; do sleep 1; done<br>  script:<br>    - if [[ -z \"$SRC_ECR_REGISTRY\" || -z \"$SRC_IMAGES_AND_TAG\" || -z \"$DEST_ECR_REGISTRY\" || -z \"$DEST_IMAGES_AND_TAG\" || -z \"$DEST_TASK_DEFINITIONS\" || -z \"$DEST_ECS_SERVICE\" || -z \"$CI_AWS_ECS_CLUSTER\" || -z \"$DEST_ECS_SERVICE\" ]]; then echo \".ecs_deploy - missing mandatory variables\"; env; exit 1; fi<br>    - IMAGE_VERSION=\"${CI_COMMIT_TAG:-$CI_COMMIT_SHORT_SHA}\"<br>    - ARRAY_SRC_IMAGES_AND_TAG=(${SRC_IMAGES_AND_TAG//,/ })<br>    - ARRAY_DEST_IMAGES_AND_TAG=(${DEST_IMAGES_AND_TAG//,/ })<br>    - ARRAY_DEST_TASK_DEFINITIONS=(${DEST_TASK_DEFINITIONS//,/ })<br>    - ARRAY_DEST_ECS_SERVICE=(${DEST_ECS_SERVICE//,/ })<br>    - | <br>      if [ ${#ARRAY_SRC_IMAGES_AND_TAG[@]} -ne ${#ARRAY_DEST_IMAGES_AND_TAG[@]} ];<br>      then<br>          echo ecs_deploy: Unequal number of images<br>          exit 1<br>      fi<br>      if [ ${#ARRAY_DEST_TASK_DEFINITIONS[@]} -ne ${#ARRAY_DEST_IMAGES_AND_TAG[@]} ];<br>      then<br>          echo ecs_deploy: Unequal number of task definition<br>          exit 1<br>      fi<br>      if [ ${#ARRAY_DEST_ECS_SERVICE[@]} -ne ${#ARRAY_DEST_IMAGES_AND_TAG[@]} ];<br>      then<br>          echo ecs_deploy: Unequal number of ecs service<br>          exit 1<br>      fi<br># Pulling image from different source<br>    - |<br>      if [[ ! -z \"$SRC_AWS_PROFILE\" ]]<br>      then <br>        export AWS_PROFILE=$SRC_AWS_PROFILE<br>        aws ecr get-login-password --region ap-southeast-1 | docker login --username AWS --password-stdin $SRC_ECR_REGISTRY<br>      else<br>        crane auth login -u $CI_REGISTRY_USER -p $CI_REGISTRY_PASSWORD $CI_REGISTRY<br>      fi<br># Pulling all images<br>    - |<br>      for i in \"${!ARRAY_SRC_IMAGES_AND_TAG[@]}\"; do<br>        SRC_IMAGE_NAME=${ARRAY_SRC_IMAGES_AND_TAG[i]%%:*}<br>        SRC_IMAGE_TAG=${ARRAY_SRC_IMAGES_AND_TAG[i]##*:}<br>        DEST_IMAGE_NAME=${ARRAY_DEST_IMAGES_AND_TAG[i]%%:*}<br>        DEST_IMAGE_TAG=${ARRAY_DEST_IMAGES_AND_TAG[i]##*:}<br>        docker pull $SRC_ECR_REGISTRY/${ARRAY_SRC_IMAGES_AND_TAG[i]}<br>        docker tag $SRC_ECR_REGISTRY/${ARRAY_SRC_IMAGES_AND_TAG[i]} $DEST_ECR_REGISTRY/${ARRAY_DEST_IMAGES_AND_TAG[i]} <br>        docker tag $SRC_ECR_REGISTRY/${ARRAY_SRC_IMAGES_AND_TAG[i]} $DEST_ECR_REGISTRY/$DEST_IMAGE_NAME:$IMAGE_VERSION<br>        docker tag $SRC_ECR_REGISTRY/${ARRAY_SRC_IMAGES_AND_TAG[i]} $DEST_ECR_REGISTRY/$DEST_IMAGE_NAME:$CI_COMMIT_SHA<br>      done<br>    - export AWS_PROFILE=$DEST_AWS_PROFILE<br>    - aws ecr get-login-password --region ap-southeast-1 | docker login --username AWS --password-stdin $DEST_ECR_REGISTRY<br># Pushing all images, by tagging based on what was given, git hash and git tag if any<br>    - |<br>      for i in \"${!ARRAY_DEST_IMAGES_AND_TAG[@]}\"; do<br>        DEST_IMAGE_NAME=${ARRAY_DEST_IMAGES_AND_TAG[i]%%:*}<br>        DEST_IMAGE_TAG=${ARRAY_DEST_IMAGES_AND_TAG[i]##*:}<br>        docker push $DEST_ECR_REGISTRY/${ARRAY_DEST_IMAGES_AND_TAG[i]}<br>        docker push $DEST_ECR_REGISTRY/$DEST_IMAGE_NAME:$IMAGE_VERSION<br>        docker push $DEST_ECR_REGISTRY/$DEST_IMAGE_NAME:$CI_COMMIT_SHA<br>      done<br>    - mkdir -p manifests<br># Pull the latest task definition, not the one currently in use<br># Replace the image tag with image:githash<br># Replace env variable to VERSION_COMMIT to commit id<br># Replace env variable to VERSION_TAG to git tag if any, else git hash<br># Replace redudant fields if not it will throw error<br># if we provide nil, it will not trigger deployment. i.e. we just want to push image<br>    - |<br>      for i in \"${!ARRAY_DEST_TASK_DEFINITIONS[@]}\"; do<br>        if [[ ${ARRAY_DEST_TASK_DEFINITIONS[i]} != \"nil\" ]]<br>        then<br>          echo \"ecs_deploy: Generating Task Definition ${ARRAY_DEST_ECS_SERVICE[i]} to ${CI_AWS_ECS_CLUSTER}\"<br>          DEST_IMAGE_NAME=${ARRAY_DEST_IMAGES_AND_TAG[i]%%:*}<br>          DEST_IMAGE_TAG=${ARRAY_DEST_IMAGES_AND_TAG[i]##*:}<br>          aws ecs describe-task-definition --output json --task-definition ${ARRAY_DEST_TASK_DEFINITIONS[i]} --query taskDefinition &gt; template_${ARRAY_DEST_TASK_DEFINITIONS[i]}.json<br>          jq --arg APP_CONTAINER_NAME $APP_CONTAINER_NAME --arg IMAGE_NAME \"$DEST_ECR_REGISTRY/$DEST_IMAGE_NAME:$CI_COMMIT_SHA\" --arg IMAGE_VERSION $IMAGE_VERSION --arg DEST_IMAGE_NAME $DEST_IMAGE_NAME --arg CI_COMMIT_SHA $CI_COMMIT_SHA 'del(.taskDefinitionArn, .revision, .status, .requiresAttributes, .compatibilities, .registeredAt, .registeredBy) | .containerDefinitions[] |= <br>            if .name == $APP_CONTAINER_NAME then<br>              .image = $IMAGE_NAME |<br>              (.environment[] | select(.name == \"VERSION_COMMIT\")).value = $CI_COMMIT_SHA |<br>              (.environment[] | select(.name == \"VERSION_TAG\")).value = $IMAGE_VERSION<br>            else<br>              .<br>            end' template_${ARRAY_DEST_TASK_DEFINITIONS[i]}.json &gt; manifests/${ARRAY_DEST_TASK_DEFINITIONS[i]}.json<br>          export CI_AWS_ECS_TASK_DEFINITION_FILE=manifests/${ARRAY_DEST_TASK_DEFINITIONS[i]}.json<br>          export CI_AWS_ECS_SERVICE=${ARRAY_DEST_ECS_SERVICE[i]}<br>          echo \"ecs_deploy: Deploying to ${ARRAY_DEST_ECS_SERVICE[i]} to ${CI_AWS_ECS_CLUSTER}\"<br>          ecs update-task-definition<br>        else<br>          echo \"ecs_deploy: Skipping deployment for ${ARRAY_DEST_IMAGES_AND_TAG[i]}\"<br>        fi<br>      done<br>  artifacts:<br>    paths:<br>      - manifests<br>  tags:<br>    - privileged</pre>\n<p>In order to start using the above code, firstly you need to have Gitlab runner that supports privileged to run docker in docker service. This is essentially required to retag the\u00a0image.</p>\n<p>Fortunately, this image provided by Gitlab has docker installed, which I do not need to build another image just to get it to work. But if you don\u2019t have a way to run docker in docker, you might want to rebuild one with crane or skopeo\u00a0.</p>\n<p>As Gitlab doesn\u2019t have a way to provide a list of string values, I am chaining a list of values with comma delimited which I will then split these values and loop\u00a0them.</p>\n<p>There are couple of validation at the start to ensure the number ECS Service count matches number of Task Definition and Container Images.</p>\n<p>It starts off to authenticate with container registry, pull the images, retag and re-authenticate with destination ECR and push the images. As you can see here, I am pulling 1 tag but pushing 3 tags i.e. commit id, provided tag and git tag if any else short git commit\u00a0id.</p>\n<p>Next, it will pull the destination task definition. I am using jq to manipulate the task definition by replacing the container image for the container what has the same name provided in APP_CONTAINER_NAME. This is a different approach as Gitlab way, by not assuming the first container is the container that I want to update. The jq command also update VERSION_TAG and VERSION_COMMIT which inject the git tag and git hash into the task definiton. This way, the application can look up these environment variables to display the semantic version on the\u00a0UI.</p>\n<p>Once the jq operation is done, it saves as a file and we will call Gitlab\u2019s ecs update-task-definiton\u00a0. This will then use the second deployment method i.e. deploy task definition via local\u00a0file.</p>\n<p>I have also added a condition where if I provide nil to DEST_TASK_DEFINITIONS, it will skip the ECS deployment. This gives me the flexibility to not needing provide 3 ECS services.</p>\n<p>Lastly, the job also publish the generated task definition as artifact, which may be useful for verification.</p>\n<p>This method also give me the flexibility to not commit the task definition into the git repository. Ideally, I want to manage the task definition in my terraform code and let gitlab handle the updating of the remaining values. If there are changes on the terraform code, I do not need to update the task definition into the application git repository. This essentially makes my terraform code base the single source of\u00a0truth!</p>\n<p>To end this article, I hope some of these thought process will be helpful, especially the part when I relate back the previous article I have written. Do feel free to reach out if this has been useful to your\u00a0work!</p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=5334ce258472\" width=\"1\" height=\"1\" alt=\"\">\n","enclosure":{},"categories":["aws","aws-ecs","gitlab"]},{"title":"CI/CD Pipeline Design Principles\u200a\u2014\u200aCRAFTS","pubDate":"2023-08-08 08:33:28","link":"https://medium.com/@kay.renfa/ci-cd-pipeline-design-principles-crafts-a62a3f79c660?source=rss-2d401c9d729d------2","guid":"https://medium.com/p/a62a3f79c660","author":"Kay Ren Fa","thumbnail":"","description":"\n<h3>CI/CD Pipeline Design Principles\u200a\u2014\u200aCRAFTS</h3>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/512/0*-ULlcf1NEt8nSkwg\"></figure><p>As a DevOps Engineer, working on CI/CD Pipelines is pretty much our bread and butter responsibility. In most occasion, most engineers don\u2019t have the opportunity to build from scratch, as they usually inherit pipeline that has been frequently fine-tuned over\u00a0time.</p>\n<p>Luck for me, I had the opportunity to design pipeline numerous time during the course of my career which I has learned from many mistakes and come up with couple of guiding principles when designing and working on pipelines.</p>\n<p>In this article, I will not be covering any technical implementation (there are too many widely available), instead, will share the principles that I have picked\u00a0up.</p>\n<p>To give more into context on why I am covering this topic. Couple of months back, my organization is enforcing a organization wide standardisation to use the same set of CI/CD orchestration tooling. My team was using a mix bag of teamcity, bamboo for CI/CD pipeline and bitbucket for SCM and we had to migrate them all to Gitlab and that is when we got our hands dirty\u00a0again.</p>\n<p>I have years of working with Jenkin, developing declarative pipeline, writing in groovy using shared libraries for Tanzu, Openshift, Andriod and IOS Pipelines. Jenkins used to be a popular software for CI/CD but it has grew short in capability and plugin support as compared to the new tools in the\u00a0market.</p>\n<p>Nevertheless, using with Gitlab is rather new to me which I need to go through its documentation understand its functionality. We successfully completed the migration all pipeline within 2 months with little roadblocks. I won\u2019t go into detail what was done to make it a success, rather, I want to share what was the thought process regardless it is Gitlab or other\u00a0tools.</p>\n<h3>The Components</h3>\n<p>Generally a Pipeline comprises of these components, and they play a part how you design your pipeline.</p>\n<ol>\n<li>CI/CD Orchestration tool</li>\n<li>Source Code Management (SCM)</li>\n<li>Branching Strategy</li>\n<li>Runners/Agents</li>\n<li>Deployment Strategy</li>\n</ol>\n<h4>CI/CD Orchestration tool</h4>\n<p>Each tool has it way of using them. It is important to study how the it expect you to use them and do couple of Proof of concept before solidify your\u00a0design.</p>\n<p>For instance, Jenkins decouples from SCM, hence, it expects you to manually setup project whenever there is a new repository (of course there is Github or other integrations which solves some of these pain\u00a0points).</p>\n<p>Gitlab and Github, on the other hand, contain pipeline code in the repository which automatically loads the pipeline\u00a0code.</p>\n<p>Gitlab has a myriad range of rule set such as triggering pipeline when certain file has been modified. This greatly changes the rule how you can design your pipeline, especially more towards monorepo\u00a0design.</p>\n<p>Another point to mention is some has its own way of support viewing of test reports, such as junit test. Jenkins has a way to publish HTML reports to view the unit test reports, whereas Gitlab ingest json output from test stages and display as its own\u00a0format.</p>\n<p>Each tools has its own way syntax of developing pipeline, such as groovy, or\u00a0.gitlab-ci.yaml and so on. For the case of Jenkins, it will then require you to pick up a \u201cscripting\u201d language and Jenkins\u00a0DSL.</p>\n<h4>SCM</h4>\n<p>While there are tools that combines orchestration tool with SCM, there are some that are standalone like Gerrit, <em>old</em> Bitbucket.</p>\n<p>SCM may play a part as some may have different merging strategy support for pull request, or webhooks to orchestration tool.</p>\n<p>I generally prefer tools that combines SCM and orchestration tool as it integrate well within it own ecosystem without the need of constant polling, and therefore you get near instantaneous invocation of pipeline.</p>\n<h4>Branching Strategy</h4>\n<p>Branch strategy shapes how the team develops, operates and pipeline\u00a0run.</p>\n<p>Before I did the migration, my application which comprises of 2 teams, which has somewhat similar branching strategy. They develop on their feature branch and merge to development branch and development branch will be deployed to CI environment to run a series of regression test before the commit get promoted to master branch and then it get promoted again after it get deployed to staging environment and lastly promoted to release branch. In between these promotion, it is either automated or manually code merge or cherry-picking.</p>\n<p>This migration gave me the opportunity to correct their branching strategy to solely trunk based development. Though there was slight resistant at the start on how \u201cbad/ untested\u201d code or feature leaks, or they cant \u201cmanipulate\u201d the branch anymore but it was good that we moved\u00a0forward.</p>\n<p>Trunk based development should still be the de facto standard because we should keep the branching as lean as possible without the need to manage nor manipulate and any \u201cbugs\u201d should be fixed on the trunk as early as possible. Feature toggle plays a very important role to toggle broken/unready code.</p>\n<h4>Runners</h4>\n<p>All pipelines has to be run on some server. There are 2 forms of runners, <strong>Dedicated runner</strong> and <strong>Container based\u00a0runner</strong>.</p>\n<p><strong>Dedicated runners</strong> are VMs that is used by all pipelines. It reuses the same workspace. Generally these are the less favoured way of running pipeline because you need to ensure you have the necessary tools in the VM, which in most of the time, abd they are snowflakes in nature, hence, there might be slight disparity in tools version. Some orchestration tools allow multiple builds on the same runner, which introduce more flakiness to failure these jobs co-share the CPU and Memory within the same VM. While it may be faster by reusing the same workspace without the need to re-pull the whole codebase, it is prone to \u201cmanipulation\u201d where one may SSH into the server to modify the workspace which can introduce false positive success and false negative failure in the build. Shallow clone can also be used to workaround full git clone\u00a0issue.</p>\n<p><strong>Container based runners</strong> are cleaner and consistent approach as it retains the clean slate of the image each time you run the jobs. Most importantly, team should follow good container practices by properly tagging the image rather than using latest\u00a0.</p>\n<p>One of the key factor that my team had little roadblocks was also because I manage my own agents rather than using shared runners. As I setup my own kubernetes executors on Gitlab, I can easily diagnose issues with kubectl top/describe/exec/port-forward\u00a0.</p>\n<h4>Deployment Strategy</h4>\n<p>There are many deployment strategy and it is best to consider what really suit your team and product. Do consider this series of questions</p>\n<ol>\n<li>Can you afford downtime?</li>\n<li>Can you afford running a lot more resources for smooth transition? Such as running twice amount of compute resource.</li>\n<li>Is your team developing the product with backward compatibility? Such as having v1 and v2\u00a0api?</li>\n<li>Does your team accept small amount of failure just to test out a small subset of audience?</li>\n</ol>\n<p>Each deployment strategy will affect the way your code your pipeline. For complicated strategies like canary deployment, at bare minimum you will need to have two\u00a0stages:</p>\n<ol>\n<li>Deploy the application and route X% of the traffic to the new\u00a0version.</li>\n<li>You will need to halt the deployment for the verification before you rollback or switch the application to the\u00a0version.</li>\n</ol>\n<p>A good list of deployment strategies can be\u00a0<a href=\"https://www.baeldung.com/ops/deployment-strategies\">here</a>.</p>\n<p>There can be more other factors in the play such as application tech stack but I won\u2019t be covering\u00a0that.</p>\n<p>As mentioned, there are some guiding principles I abide when designing the pipeline using this acronyms\u00a0<strong>CRAFTS</strong>.</p>\n<ol>\n<li>Completeness</li>\n<li>Rapid</li>\n<li>Audit-ability</li>\n<li>Feedback</li>\n<li>Traceability</li>\n<li>Simplicity</li>\n</ol>\n<h4>Completeness</h4>\n<p>While some of these point are talking about the whole pipeline, this point solely focus on deployment as it is the ultimate goal for CI/CD. Hence, what constitute as a complete deployment?</p>\n<p>Ideally, it should be a whole new clean slate instead of incremental. This reduces the chances of missing parts and ensures rollbacks are consistent too.</p>\n<p>Imagine a deployment strategy is to unzip an artifact and copy the content to a directory.</p>\n<p>Sounds complete safe but what is the risk here? As copying files only overwrites files but it doesn\u2019t remove files that are supposed to be there as per the commit. It may result in failure as the \u201cextra\u201d files may also fail your application. Hence, it introduces risk to failure during deployment, hence it is not complete per\u00a0se.</p>\n<p>Whereas for container deployments are generally complete as it the filesystems are \u201creset\u201d each\u00a0time.</p>\n<p><strong>Rapid</strong></p>\n<p>It is no brainer that everyone wants fast feedback from pipeline. Various tool allows parallelism in the build stages which speeds up the testing stages, others allow caching to avoid re-building or re-downloading dependencies. These are good techniques to not repeat time consuming stages.</p>\n<p>One example would be docker build caches saves the trouble of rebuilding all layers, and only start from the stage where changes are\u00a0made.</p>\n<p>Another method to make pipelines rapid is to reduce lead time between stages. That is to have as little manual process and human intervention such as parameterizing certain values manually. A good pipeline should be as automated as possible with little stop point and abort immediately whenever there is any failure. This also ties back to the 4th point on feedback so that the team can rapidly react and make the necessary changes.</p>\n<p>Teams should strike a balance between the type of test as too many tests may slow things down and too little may result in insufficient test coverages. Developers should also regularly revisit their test cases to remove depreciated test scenarios.</p>\n<p>While Rapid is the goal we want to achieve, at times certain test are incredible long such as load testing, DAST. Generally you want to park them out of the main pipeline for more of the schedule based jobs. They are good to have but not mission critical in general that should bar anyone from merging the code. Imagine a scenario where these test takes 2hours, can you imagine how it will affect overall team velocity to wait for these test to complete in order to\u00a0proceed?</p>\n<h4>Audit-ability</h4>\n<p>I suppose this is not a commonly thought of point but I felt this is very important. Why this is important is because a good pipeline should also have control in place to prevent people from finding any loophole though no shame that I have done this numerous\u00a0time.</p>\n<p>It is important is to ensure that there are check in place or gatekeepers to prevent any loophole from occurring.</p>\n<p>For instance, only code that is from the trunk and the particular commit has gone through all code scan with no failure can deploy to production. The reason is not all commit in the trunk passes the scan, hence, it might be a minefield of faulty code. Hence, you has to pick the right commit that is safe to use after code merge, therefore the checks should be in place to prevent wrong code from going in to production.</p>\n<p>You may also need to ensure you \u201clock\u201d the pipeline files, and/or print out which commit you are using if you have an external repository for pipeline code. This is to ensure there is no temporary switch of commit to \u201cmalicious\u201d commit that may grant bad actors to disable some of the important checks.</p>\n<h4>Feedback</h4>\n<p>Feedback plays an important role to let them know what went wrong in their code or deployment. There are two forms of feedback.</p>\n<p>Jobs should always fail during any testing such as static code analysis, unit test when things are wrong. Tests should not be flaky which will lead the developers to rerun the jobs hoping it will work the next time. It should always fail stages when something is wrong and prevent subsequent stages such as deployment from proceeding.</p>\n<p>Another form of feedback is to send notifications such as slack, telegram, email, or a form of dashboard. This will be extremely useful to get the team to look into the issues immediately.</p>\n<h4>Traceability</h4>\n<p>By best practice, deployment should reuse the same deployment artifacts across all environments and it is the configurations that set the application apart.</p>\n<p>Quite importantly, there should be a way to determine if an artifact is tied to a particular commit, build and not tampered. This can be done by signing the artifact after build is complete like cosigning.</p>\n<p>You can also make it impossible to overwrite existing artifacts such as ECR immutable tags, Nexus repositories restricting overwriting files. These write only once features ensure no artifact can be fabricated from another\u00a0source.</p>\n<p>By naming the artifact based on the commit Id or git tag, it makes it easy to traceback to the version of the code that is used to produce the files. Certain SCM also associates pipeline build to commit Id which further enhances the traceability of the input and\u00a0output.</p>\n<h4>Simplicity</h4>\n<p>A well-designed pipeline is intuitive to users. It should be simple enough for people to understand and use. It should also provide enough flexibility for one to use it for whatever\u00a0purpose.</p>\n<p>One flexibility will always be to deploy a feature branch changes to an environment for review. Ideally we should do most of our testing locally, certain scenario may not be achievable through local testing or limited compute power. Hence, you may still need to deploy to certain environment to perform end-to-end testing. Hence, giving team a flexibility to deploy on need basis, ideally on their own namespace will further empower the developers.</p>\n<p>One of my team\u2019s need is to be able to apply hotfix whenever possible. That means to have a backdoor to deploy to production without the need to go through the usual series of test. Though it is not by trunk based development practice to have hotfixbranch, but I do understand the challenge to transit directly to trunk based development as the team doesn\u2019t deploy the the trunk to production frequently enough. Hence, I made it as a \u201cbreakglass\u201d scenario whenever we have to do\u00a0one.</p>\n<p>Simplicity can also be drilled all the way down the the error message whenever a job fails. As pipeline development may happen together with feature developments, pipeline may randomly fail due to new changes. Developers may quick to judge that it is pipeline issue than code issue. Pipeline code are usually bunch of bash commands than developers aren\u2019t familiar, hence, these messages play an important part for them understand what is happening rather than random exit 1\u00a0errors.</p>\n<p>As a closing note, developing a pipeline is no different from application design. There is no silver bullet on how pipeline should be written as every team are bounded by different constraints such as teams culture, tech stack, CI/CD tools, but general principles as shared above do come into play. I hope some of these points will bring value to your\u00a0work.</p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=a62a3f79c660\" width=\"1\" height=\"1\" alt=\"\">\n","content":"\n<h3>CI/CD Pipeline Design Principles\u200a\u2014\u200aCRAFTS</h3>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/512/0*-ULlcf1NEt8nSkwg\"></figure><p>As a DevOps Engineer, working on CI/CD Pipelines is pretty much our bread and butter responsibility. In most occasion, most engineers don\u2019t have the opportunity to build from scratch, as they usually inherit pipeline that has been frequently fine-tuned over\u00a0time.</p>\n<p>Luck for me, I had the opportunity to design pipeline numerous time during the course of my career which I has learned from many mistakes and come up with couple of guiding principles when designing and working on pipelines.</p>\n<p>In this article, I will not be covering any technical implementation (there are too many widely available), instead, will share the principles that I have picked\u00a0up.</p>\n<p>To give more into context on why I am covering this topic. Couple of months back, my organization is enforcing a organization wide standardisation to use the same set of CI/CD orchestration tooling. My team was using a mix bag of teamcity, bamboo for CI/CD pipeline and bitbucket for SCM and we had to migrate them all to Gitlab and that is when we got our hands dirty\u00a0again.</p>\n<p>I have years of working with Jenkin, developing declarative pipeline, writing in groovy using shared libraries for Tanzu, Openshift, Andriod and IOS Pipelines. Jenkins used to be a popular software for CI/CD but it has grew short in capability and plugin support as compared to the new tools in the\u00a0market.</p>\n<p>Nevertheless, using with Gitlab is rather new to me which I need to go through its documentation understand its functionality. We successfully completed the migration all pipeline within 2 months with little roadblocks. I won\u2019t go into detail what was done to make it a success, rather, I want to share what was the thought process regardless it is Gitlab or other\u00a0tools.</p>\n<h3>The Components</h3>\n<p>Generally a Pipeline comprises of these components, and they play a part how you design your pipeline.</p>\n<ol>\n<li>CI/CD Orchestration tool</li>\n<li>Source Code Management (SCM)</li>\n<li>Branching Strategy</li>\n<li>Runners/Agents</li>\n<li>Deployment Strategy</li>\n</ol>\n<h4>CI/CD Orchestration tool</h4>\n<p>Each tool has it way of using them. It is important to study how the it expect you to use them and do couple of Proof of concept before solidify your\u00a0design.</p>\n<p>For instance, Jenkins decouples from SCM, hence, it expects you to manually setup project whenever there is a new repository (of course there is Github or other integrations which solves some of these pain\u00a0points).</p>\n<p>Gitlab and Github, on the other hand, contain pipeline code in the repository which automatically loads the pipeline\u00a0code.</p>\n<p>Gitlab has a myriad range of rule set such as triggering pipeline when certain file has been modified. This greatly changes the rule how you can design your pipeline, especially more towards monorepo\u00a0design.</p>\n<p>Another point to mention is some has its own way of support viewing of test reports, such as junit test. Jenkins has a way to publish HTML reports to view the unit test reports, whereas Gitlab ingest json output from test stages and display as its own\u00a0format.</p>\n<p>Each tools has its own way syntax of developing pipeline, such as groovy, or\u00a0.gitlab-ci.yaml and so on. For the case of Jenkins, it will then require you to pick up a \u201cscripting\u201d language and Jenkins\u00a0DSL.</p>\n<h4>SCM</h4>\n<p>While there are tools that combines orchestration tool with SCM, there are some that are standalone like Gerrit, <em>old</em> Bitbucket.</p>\n<p>SCM may play a part as some may have different merging strategy support for pull request, or webhooks to orchestration tool.</p>\n<p>I generally prefer tools that combines SCM and orchestration tool as it integrate well within it own ecosystem without the need of constant polling, and therefore you get near instantaneous invocation of pipeline.</p>\n<h4>Branching Strategy</h4>\n<p>Branch strategy shapes how the team develops, operates and pipeline\u00a0run.</p>\n<p>Before I did the migration, my application which comprises of 2 teams, which has somewhat similar branching strategy. They develop on their feature branch and merge to development branch and development branch will be deployed to CI environment to run a series of regression test before the commit get promoted to master branch and then it get promoted again after it get deployed to staging environment and lastly promoted to release branch. In between these promotion, it is either automated or manually code merge or cherry-picking.</p>\n<p>This migration gave me the opportunity to correct their branching strategy to solely trunk based development. Though there was slight resistant at the start on how \u201cbad/ untested\u201d code or feature leaks, or they cant \u201cmanipulate\u201d the branch anymore but it was good that we moved\u00a0forward.</p>\n<p>Trunk based development should still be the de facto standard because we should keep the branching as lean as possible without the need to manage nor manipulate and any \u201cbugs\u201d should be fixed on the trunk as early as possible. Feature toggle plays a very important role to toggle broken/unready code.</p>\n<h4>Runners</h4>\n<p>All pipelines has to be run on some server. There are 2 forms of runners, <strong>Dedicated runner</strong> and <strong>Container based\u00a0runner</strong>.</p>\n<p><strong>Dedicated runners</strong> are VMs that is used by all pipelines. It reuses the same workspace. Generally these are the less favoured way of running pipeline because you need to ensure you have the necessary tools in the VM, which in most of the time, abd they are snowflakes in nature, hence, there might be slight disparity in tools version. Some orchestration tools allow multiple builds on the same runner, which introduce more flakiness to failure these jobs co-share the CPU and Memory within the same VM. While it may be faster by reusing the same workspace without the need to re-pull the whole codebase, it is prone to \u201cmanipulation\u201d where one may SSH into the server to modify the workspace which can introduce false positive success and false negative failure in the build. Shallow clone can also be used to workaround full git clone\u00a0issue.</p>\n<p><strong>Container based runners</strong> are cleaner and consistent approach as it retains the clean slate of the image each time you run the jobs. Most importantly, team should follow good container practices by properly tagging the image rather than using latest\u00a0.</p>\n<p>One of the key factor that my team had little roadblocks was also because I manage my own agents rather than using shared runners. As I setup my own kubernetes executors on Gitlab, I can easily diagnose issues with kubectl top/describe/exec/port-forward\u00a0.</p>\n<h4>Deployment Strategy</h4>\n<p>There are many deployment strategy and it is best to consider what really suit your team and product. Do consider this series of questions</p>\n<ol>\n<li>Can you afford downtime?</li>\n<li>Can you afford running a lot more resources for smooth transition? Such as running twice amount of compute resource.</li>\n<li>Is your team developing the product with backward compatibility? Such as having v1 and v2\u00a0api?</li>\n<li>Does your team accept small amount of failure just to test out a small subset of audience?</li>\n</ol>\n<p>Each deployment strategy will affect the way your code your pipeline. For complicated strategies like canary deployment, at bare minimum you will need to have two\u00a0stages:</p>\n<ol>\n<li>Deploy the application and route X% of the traffic to the new\u00a0version.</li>\n<li>You will need to halt the deployment for the verification before you rollback or switch the application to the\u00a0version.</li>\n</ol>\n<p>A good list of deployment strategies can be\u00a0<a href=\"https://www.baeldung.com/ops/deployment-strategies\">here</a>.</p>\n<p>There can be more other factors in the play such as application tech stack but I won\u2019t be covering\u00a0that.</p>\n<p>As mentioned, there are some guiding principles I abide when designing the pipeline using this acronyms\u00a0<strong>CRAFTS</strong>.</p>\n<ol>\n<li>Completeness</li>\n<li>Rapid</li>\n<li>Audit-ability</li>\n<li>Feedback</li>\n<li>Traceability</li>\n<li>Simplicity</li>\n</ol>\n<h4>Completeness</h4>\n<p>While some of these point are talking about the whole pipeline, this point solely focus on deployment as it is the ultimate goal for CI/CD. Hence, what constitute as a complete deployment?</p>\n<p>Ideally, it should be a whole new clean slate instead of incremental. This reduces the chances of missing parts and ensures rollbacks are consistent too.</p>\n<p>Imagine a deployment strategy is to unzip an artifact and copy the content to a directory.</p>\n<p>Sounds complete safe but what is the risk here? As copying files only overwrites files but it doesn\u2019t remove files that are supposed to be there as per the commit. It may result in failure as the \u201cextra\u201d files may also fail your application. Hence, it introduces risk to failure during deployment, hence it is not complete per\u00a0se.</p>\n<p>Whereas for container deployments are generally complete as it the filesystems are \u201creset\u201d each\u00a0time.</p>\n<p><strong>Rapid</strong></p>\n<p>It is no brainer that everyone wants fast feedback from pipeline. Various tool allows parallelism in the build stages which speeds up the testing stages, others allow caching to avoid re-building or re-downloading dependencies. These are good techniques to not repeat time consuming stages.</p>\n<p>One example would be docker build caches saves the trouble of rebuilding all layers, and only start from the stage where changes are\u00a0made.</p>\n<p>Another method to make pipelines rapid is to reduce lead time between stages. That is to have as little manual process and human intervention such as parameterizing certain values manually. A good pipeline should be as automated as possible with little stop point and abort immediately whenever there is any failure. This also ties back to the 4th point on feedback so that the team can rapidly react and make the necessary changes.</p>\n<p>Teams should strike a balance between the type of test as too many tests may slow things down and too little may result in insufficient test coverages. Developers should also regularly revisit their test cases to remove depreciated test scenarios.</p>\n<p>While Rapid is the goal we want to achieve, at times certain test are incredible long such as load testing, DAST. Generally you want to park them out of the main pipeline for more of the schedule based jobs. They are good to have but not mission critical in general that should bar anyone from merging the code. Imagine a scenario where these test takes 2hours, can you imagine how it will affect overall team velocity to wait for these test to complete in order to\u00a0proceed?</p>\n<h4>Audit-ability</h4>\n<p>I suppose this is not a commonly thought of point but I felt this is very important. Why this is important is because a good pipeline should also have control in place to prevent people from finding any loophole though no shame that I have done this numerous\u00a0time.</p>\n<p>It is important is to ensure that there are check in place or gatekeepers to prevent any loophole from occurring.</p>\n<p>For instance, only code that is from the trunk and the particular commit has gone through all code scan with no failure can deploy to production. The reason is not all commit in the trunk passes the scan, hence, it might be a minefield of faulty code. Hence, you has to pick the right commit that is safe to use after code merge, therefore the checks should be in place to prevent wrong code from going in to production.</p>\n<p>You may also need to ensure you \u201clock\u201d the pipeline files, and/or print out which commit you are using if you have an external repository for pipeline code. This is to ensure there is no temporary switch of commit to \u201cmalicious\u201d commit that may grant bad actors to disable some of the important checks.</p>\n<h4>Feedback</h4>\n<p>Feedback plays an important role to let them know what went wrong in their code or deployment. There are two forms of feedback.</p>\n<p>Jobs should always fail during any testing such as static code analysis, unit test when things are wrong. Tests should not be flaky which will lead the developers to rerun the jobs hoping it will work the next time. It should always fail stages when something is wrong and prevent subsequent stages such as deployment from proceeding.</p>\n<p>Another form of feedback is to send notifications such as slack, telegram, email, or a form of dashboard. This will be extremely useful to get the team to look into the issues immediately.</p>\n<h4>Traceability</h4>\n<p>By best practice, deployment should reuse the same deployment artifacts across all environments and it is the configurations that set the application apart.</p>\n<p>Quite importantly, there should be a way to determine if an artifact is tied to a particular commit, build and not tampered. This can be done by signing the artifact after build is complete like cosigning.</p>\n<p>You can also make it impossible to overwrite existing artifacts such as ECR immutable tags, Nexus repositories restricting overwriting files. These write only once features ensure no artifact can be fabricated from another\u00a0source.</p>\n<p>By naming the artifact based on the commit Id or git tag, it makes it easy to traceback to the version of the code that is used to produce the files. Certain SCM also associates pipeline build to commit Id which further enhances the traceability of the input and\u00a0output.</p>\n<h4>Simplicity</h4>\n<p>A well-designed pipeline is intuitive to users. It should be simple enough for people to understand and use. It should also provide enough flexibility for one to use it for whatever\u00a0purpose.</p>\n<p>One flexibility will always be to deploy a feature branch changes to an environment for review. Ideally we should do most of our testing locally, certain scenario may not be achievable through local testing or limited compute power. Hence, you may still need to deploy to certain environment to perform end-to-end testing. Hence, giving team a flexibility to deploy on need basis, ideally on their own namespace will further empower the developers.</p>\n<p>One of my team\u2019s need is to be able to apply hotfix whenever possible. That means to have a backdoor to deploy to production without the need to go through the usual series of test. Though it is not by trunk based development practice to have hotfixbranch, but I do understand the challenge to transit directly to trunk based development as the team doesn\u2019t deploy the the trunk to production frequently enough. Hence, I made it as a \u201cbreakglass\u201d scenario whenever we have to do\u00a0one.</p>\n<p>Simplicity can also be drilled all the way down the the error message whenever a job fails. As pipeline development may happen together with feature developments, pipeline may randomly fail due to new changes. Developers may quick to judge that it is pipeline issue than code issue. Pipeline code are usually bunch of bash commands than developers aren\u2019t familiar, hence, these messages play an important part for them understand what is happening rather than random exit 1\u00a0errors.</p>\n<p>As a closing note, developing a pipeline is no different from application design. There is no silver bullet on how pipeline should be written as every team are bounded by different constraints such as teams culture, tech stack, CI/CD tools, but general principles as shared above do come into play. I hope some of these points will bring value to your\u00a0work.</p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=a62a3f79c660\" width=\"1\" height=\"1\" alt=\"\">\n","enclosure":{},"categories":["ci-cd-pipeline","devops"]},{"title":"AWS Security Group with Terraform","pubDate":"2022-12-25 04:15:52","link":"https://medium.com/@kay.renfa/aws-security-group-with-terraform-4805b955ad5c?source=rss-2d401c9d729d------2","guid":"https://medium.com/p/4805b955ad5c","author":"Kay Ren Fa","thumbnail":"","description":"\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/512/0*rLUUHTXTz6WvOrcD\"></figure><p>In this article, I will cover the different ways to define security groups in AWS in Terraform. Though it sounds like a straightforward topic, many times, team will end up exposing more traffic than they should, such as using AWS default egress rules. I will cover when you should use CIDR range and when you should use Security Group ID. I will cover my interpretation of secure yet dynamic security\u00a0group.</p>\n<h3>Introduction</h3>\n<p>Security Group contains a collection of security group rules that defines the set of traffic that is allowed inbound or outbound, defined as ingress and egress respectively.</p>\n<p>Each security group rules are made up of 3\u00a0parts:</p>\n<ul>\n<li>Type of protocol (UDP/TCP)</li>\n<li>Port Number</li>\n<li>CIDR Range (IPV4/IPV6) or Security Group\u00a0ID</li>\n</ul>\n<p>Any traffic not specified will be blocked. Security group rules are stateful, hence, you do not need to handle for ephemeral port. By default, if no egress rules defined, AWS will append a All Traffic 0.0.0.0/0. Many times, people will neglect the need to also implement secure egress\u00a0rules.</p>\n<h3>The Scenarios</h3>\n<p>Typically, you will need to define 2 sets of rules, one for the consumer (egress), one for the provider (ingress). This is common when the resources are \u201cwithin your control\u201d.</p>\n<p>For instance, EC2 -&gt; MySQL scenario, you should have these 2 set of\u00a0rules</p>\n<ul>\n<li>EC2 Egress TCP 3306 to\u00a0MySQL</li>\n<li>MySQL Ingress TCP 3306 to\u00a0EC2</li>\n</ul>\n<p>However, there are scenarios where you only have 1 set of rules, such as accessing endpoints that is \u201cout of your control\u201d, such as static IP from external endpoint.</p>\n<ul><li>EC2 Egress TCP 443 to External Static\u00a0Endpoint</li></ul>\n<p>What is the differences between these 2 scenario? In the former case, you have access to add the rules to these 2 set of resources. They can be within the same VPC, or even peered VPC. In the latter case, you do not have the access to the External endpoint, hence, you can only add 1 set of\u00a0rules.</p>\n<p>Simple enough? Lets dive even deeper than\u00a0that!</p>\n<h3>Advance</h3>\n<p>In the latter scenario, the only way is to use CIDR range when adding the Egress rule. Reason being, you have no way to point to the other endpoint\u2019s security_group_id.</p>\n<p>However, for former scenario, you will have the 2 options, CIDR Range and Security Group ID. I will rank following approaches from least secure to the most secure\u00a0method:</p>\n<ol>\n<li>Anywhere</li>\n<li>VPC CIDR</li>\n<li>Subnet CIDR</li>\n<li>Static IP</li>\n<li>Security Group\u00a0ID</li>\n</ol>\n<p>If you notice, in the same ordering, it is also the reverse order of easiest to hardest to implement.</p>\n<ul>\n<li>Anywhere is easiest to implement as it covers all IP range by setting 0.0.0.0/0</li>\n<li>VPC CIDR is a broad IP range that covers all IP within the VPC, which you can query via data_source. However, note that VPC supports multiple VPC CIDR\u00a0ranges</li>\n<li>Subnet CIDR becomes more slightly more complicated as AWS advocates services to be running across 3 AZs. Hence, you will need to cover all 3 IP\u00a0ranges</li>\n<li>Static IP like most likely to be implemented as a terraform variable. But Static IP are more as flexible as there are chances where the \u201cstatic\u201d IP change out of the\u00a0blue.</li>\n</ul>\n<h3>The Forbidden Fruit</h3>\n<p>Why is security_group_id is the hardest to implement? For the case of CIDR range, it does not reference the rules to a resource. However, it does for security_group_id. Then it becomes more complicated as it becomes cyclical dependency.</p>\n<p>Using the earlier example EC2 -&gt; RDS. EC2 will need to refer to RDS security_group_id, and RDS will need to refer to EC2 security_group_id. In most terraform modules for security group, it does not allow you to do that as there will result in a cyclical dependency.</p>\n<p>The perks of using security_group_id is that you don\u2019t need to be bothered by IP address at all. You have one security group specifically for EC2 for egress to RDS and you have one for for ingress to EC2. Only thing that you need to do is to add the security group to the resource and it will gain the access that it\u00a0should.</p>\n<p>One may argue, this sounds too good to be true but it may be prone to be abused. If an attacker who managed to gain access, they just need to attach the necessary security group to resources and immediately gain the access. Whereas CIDR range approach, it MAY require them to modify the security group rule to open wider range. My take is that using security_group_id is not the problem, but granting the attacker the wrong IAM policy is. Truth to the matter, any attacker with administrator permission can do anything they want, hence, it won\u2019t defer in terms of whichever approach.</p>\n<h3>Implementation</h3>\n<p>If you are enticed by the benefit of security_group_id, how can we then tackle the cyclical dependency issue?</p>\n<p>The way to handle it is to use the native resource which is security_group and security_group_rule. While you can put all rules within security_group resource, to achieve what we want is to have 2 separately resources. Reason is that you need both security_group resources to be created before you can references each\u00a0other.</p>\n<pre>resource \"aws_security_group\" \"rds-sg\" {<br>  name        = \"sgrp-rds\"<br>  description = \"Security group for RDS\"<br>  vpc_id      = var.vpc_id<br>}<br><br>resource \"aws_security_group_rule\" \"rds-sg\" {<br>  type                     = \"ingress\"<br>  description              = \"Security Group for RDS\"<br>  from_port                = var.port<br>  to_port                  = var.port<br>  protocol                 = \"tcp\"<br>  source_security_group_id = aws_security_group.rds-sg-for-app.id<br>  security_group_id        = aws_security_group.rds-sg.id<br>}<br><br>resource \"aws_security_group\" \"rds-sg-for-app\" {<br>  name        = \"sgrp-app-to-rds\"<br>  description = \"Security Group for App to RDS\"<br>  vpc_id      = var.vpc_id<br>}<br><br>resource \"aws_security_group_rule\" \"rds-sg-for-app\" {<br>  type                     = \"egress\"<br>  description              = \"Security Group for App to RDS\"<br>  from_port                = var.port<br>  to_port                  = var.port<br>  protocol                 = \"tcp\"<br>  source_security_group_id = aws_security_group.rds-sg.id<br>  security_group_id        = aws_security_group.rds-sg-for-app.id<br>}</pre>\n<p>So for above scenario, assuming I have a RDS Terraform module, I will have these 4 sets of resources. This will create 2 security groups that opens for each\u00a0other.</p>\n<p>Then for my EC2 instances, I will just need to bind the aws_security_group.rds-sg-for-app security group ID. That can be done via data.terraform_remote_state. As simple as that, you have a very secure and dynamic security\u00a0setup.</p>\n<h3>Extra materials</h3>\n<p>You can also point security group id to itself, that is using the self attribute. That is more relevant when you have a setup needs to communicate within itself such as synchronising states.</p>\n<p>You can also point your security group rules to prefix_list_id. This allows you to point to AWS regional endpoints instead of opening to Anywhere.</p>\n<h3>Final thoughts</h3>\n<p>In my opinion, we should use security_group_id for security group rules whenever possible as it gives more secure and flexible setup without bothering about IP address. You can do the same for AWS PrivateLink setting egress to your PrivateLink security\u00a0group.</p>\n<p>CIDR range give you a quick and dirty way to implement and test connectivity, but it shouldn\u2019t be the final setup for production environment unless you have no alternative way to do\u00a0so.</p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=4805b955ad5c\" width=\"1\" height=\"1\" alt=\"\">\n","content":"\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/512/0*rLUUHTXTz6WvOrcD\"></figure><p>In this article, I will cover the different ways to define security groups in AWS in Terraform. Though it sounds like a straightforward topic, many times, team will end up exposing more traffic than they should, such as using AWS default egress rules. I will cover when you should use CIDR range and when you should use Security Group ID. I will cover my interpretation of secure yet dynamic security\u00a0group.</p>\n<h3>Introduction</h3>\n<p>Security Group contains a collection of security group rules that defines the set of traffic that is allowed inbound or outbound, defined as ingress and egress respectively.</p>\n<p>Each security group rules are made up of 3\u00a0parts:</p>\n<ul>\n<li>Type of protocol (UDP/TCP)</li>\n<li>Port Number</li>\n<li>CIDR Range (IPV4/IPV6) or Security Group\u00a0ID</li>\n</ul>\n<p>Any traffic not specified will be blocked. Security group rules are stateful, hence, you do not need to handle for ephemeral port. By default, if no egress rules defined, AWS will append a All Traffic 0.0.0.0/0. Many times, people will neglect the need to also implement secure egress\u00a0rules.</p>\n<h3>The Scenarios</h3>\n<p>Typically, you will need to define 2 sets of rules, one for the consumer (egress), one for the provider (ingress). This is common when the resources are \u201cwithin your control\u201d.</p>\n<p>For instance, EC2 -&gt; MySQL scenario, you should have these 2 set of\u00a0rules</p>\n<ul>\n<li>EC2 Egress TCP 3306 to\u00a0MySQL</li>\n<li>MySQL Ingress TCP 3306 to\u00a0EC2</li>\n</ul>\n<p>However, there are scenarios where you only have 1 set of rules, such as accessing endpoints that is \u201cout of your control\u201d, such as static IP from external endpoint.</p>\n<ul><li>EC2 Egress TCP 443 to External Static\u00a0Endpoint</li></ul>\n<p>What is the differences between these 2 scenario? In the former case, you have access to add the rules to these 2 set of resources. They can be within the same VPC, or even peered VPC. In the latter case, you do not have the access to the External endpoint, hence, you can only add 1 set of\u00a0rules.</p>\n<p>Simple enough? Lets dive even deeper than\u00a0that!</p>\n<h3>Advance</h3>\n<p>In the latter scenario, the only way is to use CIDR range when adding the Egress rule. Reason being, you have no way to point to the other endpoint\u2019s security_group_id.</p>\n<p>However, for former scenario, you will have the 2 options, CIDR Range and Security Group ID. I will rank following approaches from least secure to the most secure\u00a0method:</p>\n<ol>\n<li>Anywhere</li>\n<li>VPC CIDR</li>\n<li>Subnet CIDR</li>\n<li>Static IP</li>\n<li>Security Group\u00a0ID</li>\n</ol>\n<p>If you notice, in the same ordering, it is also the reverse order of easiest to hardest to implement.</p>\n<ul>\n<li>Anywhere is easiest to implement as it covers all IP range by setting 0.0.0.0/0</li>\n<li>VPC CIDR is a broad IP range that covers all IP within the VPC, which you can query via data_source. However, note that VPC supports multiple VPC CIDR\u00a0ranges</li>\n<li>Subnet CIDR becomes more slightly more complicated as AWS advocates services to be running across 3 AZs. Hence, you will need to cover all 3 IP\u00a0ranges</li>\n<li>Static IP like most likely to be implemented as a terraform variable. But Static IP are more as flexible as there are chances where the \u201cstatic\u201d IP change out of the\u00a0blue.</li>\n</ul>\n<h3>The Forbidden Fruit</h3>\n<p>Why is security_group_id is the hardest to implement? For the case of CIDR range, it does not reference the rules to a resource. However, it does for security_group_id. Then it becomes more complicated as it becomes cyclical dependency.</p>\n<p>Using the earlier example EC2 -&gt; RDS. EC2 will need to refer to RDS security_group_id, and RDS will need to refer to EC2 security_group_id. In most terraform modules for security group, it does not allow you to do that as there will result in a cyclical dependency.</p>\n<p>The perks of using security_group_id is that you don\u2019t need to be bothered by IP address at all. You have one security group specifically for EC2 for egress to RDS and you have one for for ingress to EC2. Only thing that you need to do is to add the security group to the resource and it will gain the access that it\u00a0should.</p>\n<p>One may argue, this sounds too good to be true but it may be prone to be abused. If an attacker who managed to gain access, they just need to attach the necessary security group to resources and immediately gain the access. Whereas CIDR range approach, it MAY require them to modify the security group rule to open wider range. My take is that using security_group_id is not the problem, but granting the attacker the wrong IAM policy is. Truth to the matter, any attacker with administrator permission can do anything they want, hence, it won\u2019t defer in terms of whichever approach.</p>\n<h3>Implementation</h3>\n<p>If you are enticed by the benefit of security_group_id, how can we then tackle the cyclical dependency issue?</p>\n<p>The way to handle it is to use the native resource which is security_group and security_group_rule. While you can put all rules within security_group resource, to achieve what we want is to have 2 separately resources. Reason is that you need both security_group resources to be created before you can references each\u00a0other.</p>\n<pre>resource \"aws_security_group\" \"rds-sg\" {<br>  name        = \"sgrp-rds\"<br>  description = \"Security group for RDS\"<br>  vpc_id      = var.vpc_id<br>}<br><br>resource \"aws_security_group_rule\" \"rds-sg\" {<br>  type                     = \"ingress\"<br>  description              = \"Security Group for RDS\"<br>  from_port                = var.port<br>  to_port                  = var.port<br>  protocol                 = \"tcp\"<br>  source_security_group_id = aws_security_group.rds-sg-for-app.id<br>  security_group_id        = aws_security_group.rds-sg.id<br>}<br><br>resource \"aws_security_group\" \"rds-sg-for-app\" {<br>  name        = \"sgrp-app-to-rds\"<br>  description = \"Security Group for App to RDS\"<br>  vpc_id      = var.vpc_id<br>}<br><br>resource \"aws_security_group_rule\" \"rds-sg-for-app\" {<br>  type                     = \"egress\"<br>  description              = \"Security Group for App to RDS\"<br>  from_port                = var.port<br>  to_port                  = var.port<br>  protocol                 = \"tcp\"<br>  source_security_group_id = aws_security_group.rds-sg.id<br>  security_group_id        = aws_security_group.rds-sg-for-app.id<br>}</pre>\n<p>So for above scenario, assuming I have a RDS Terraform module, I will have these 4 sets of resources. This will create 2 security groups that opens for each\u00a0other.</p>\n<p>Then for my EC2 instances, I will just need to bind the aws_security_group.rds-sg-for-app security group ID. That can be done via data.terraform_remote_state. As simple as that, you have a very secure and dynamic security\u00a0setup.</p>\n<h3>Extra materials</h3>\n<p>You can also point security group id to itself, that is using the self attribute. That is more relevant when you have a setup needs to communicate within itself such as synchronising states.</p>\n<p>You can also point your security group rules to prefix_list_id. This allows you to point to AWS regional endpoints instead of opening to Anywhere.</p>\n<h3>Final thoughts</h3>\n<p>In my opinion, we should use security_group_id for security group rules whenever possible as it gives more secure and flexible setup without bothering about IP address. You can do the same for AWS PrivateLink setting egress to your PrivateLink security\u00a0group.</p>\n<p>CIDR range give you a quick and dirty way to implement and test connectivity, but it shouldn\u2019t be the final setup for production environment unless you have no alternative way to do\u00a0so.</p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=4805b955ad5c\" width=\"1\" height=\"1\" alt=\"\">\n","enclosure":{},"categories":["aws"]},{"title":"ECS Service Task Reload when Parameter Store Changes","pubDate":"2022-11-23 04:31:32","link":"https://medium.com/@kay.renfa/ecs-service-task-reload-when-parameter-store-changes-c5b42e85cd65?source=rss-2d401c9d729d------2","guid":"https://medium.com/p/c5b42e85cd65","author":"Kay Ren Fa","thumbnail":"","description":"\n<p>One of the really neat feature of Kubernetes is that any changes to Secrets and ConfigMaps will result in recreation of the Pods. However, ECS does not natively come with such functionality.</p>\n<p>While you can hook Parameter Store changes to EventBridge, it doesn\u2019t come with the capability to stop \u201coutdated\u201d ECS task. Hence, another approach is to do it via Step Function. One of the use case of such frequent Parameter Store changes can be feature toggling which is frequently used in testing environments to simulate different scenarios.</p>\n<p>In this article, I will show one of such use case where I have to change couple of Parameter Store values and reloading the ECS\u00a0tasks.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*wIrgEjz7Iy3oVUuv5ph3rA.png\"><figcaption>Step Function</figcaption></figure><p>The general idea from the above\u00a0is</p>\n<ol>\n<li>Enabling/Disabling of certain functionality is passed as input.<br><em>{\u201cenable_feature\u201d: \u201ctrue\u201d}</em>\n</li>\n<li>Based on the choice given, it will route of either of the flow which will update the parameters</li>\n<li>Once Parameters are updated, it fetches all ECS Tasks ID from the given ECS Service. In this example, I am hardcoding to one ECS Service, but it can be easily enhanced to be more\u00a0dynamic</li>\n<li>Once ECS Task ID is retrieved, you will need to use the intrinsic function to split the string for the TASK ID as List Task returns you the ARN of the task. Using the Task ID, you can stop them in order for ECS Service to recreate new ones. In this example, I am not introducing maximum parallelism as I do not require such functionality. But if you are doing this for high availability environments, do add delays and parallelism as ECS tasks may take a while to be\u00a0ready.</li>\n</ol>\n<p>The step function code will look like\u00a0such</p>\n<pre>{<br>  \"Comment\": \"A description of my state machine\",<br>  \"StartAt\": \"IS_FEATURE_ENABLED\",<br>  \"States\": {<br>    \"IS_FEATURE_ENABLED\": {<br>      \"Type\": \"Choice\",<br>      \"Choices\": [<br>        {<br>          \"Variable\": \"$.enable_feature\",<br>          \"StringMatches\": \"true\",<br>          \"Next\": \"ENABLE_FEATURE_PARALLEL\"<br>        }<br>      ],<br>      \"Default\": \"DISABLE_FEATURE_PARALLEL\"<br>    },<br>    \"ENABLE_FEATURE_PARALLEL\": {<br>      \"Type\": \"Parallel\",<br>      \"Branches\": [<br>        {<br>          \"StartAt\": \"ENABLE_FEATURE_A_PROVIDER\",<br>          \"States\": {<br>            \"ENABLE_FEATURE_A_PROVIDER\": {<br>              \"Type\": \"Task\",<br>              \"End\": true,<br>              \"Parameters\": {<br>                \"Name\": \"/param/A\",<br>                \"Value\": \"valueA\",<br>                \"Overwrite\": true<br>              },<br>              \"Resource\": \"arn:aws:states:::aws-sdk:ssm:putParameter\"<br>            }<br>          }<br>        },<br>        {<br>          \"StartAt\": \"ENABLE_FEATURE_B_PROVIDER\",<br>          \"States\": {<br>            \"ENABLE_FEATURE_B_PROVIDER\": {<br>              \"Type\": \"Task\",<br>              \"End\": true,<br>              \"Parameters\": {<br>                \"Name\": \"/param/B\",<br>                \"Value\": \"valueB\",<br>                \"Overwrite\": true<br>              },<br>              \"Resource\": \"arn:aws:states:::aws-sdk:ssm:putParameter\"<br>            }<br>          }<br>        }<br>      ],<br>      \"Next\": \"Pass\"<br>    },<br>    \"DISABLE_FEATURE_PARALLEL\": {<br>      \"Type\": \"Parallel\",<br>      \"Branches\": [<br>        {<br>          \"StartAt\": \"DISABLE_FEATURE_A_PROVIDER\",<br>          \"States\": {<br>            \"DISABLE_FEATURE_A_PROVIDER\": {<br>              \"Type\": \"Task\",<br>              \"End\": true,<br>              \"Parameters\": {<br>                \"Name\": \"/param/A\",<br>                \"Value\": \"valueX\",<br>                \"Overwrite\": true<br>              },<br>              \"Resource\": \"arn:aws:states:::aws-sdk:ssm:putParameter\"<br>            }<br>          }<br>        },<br>        {<br>          \"StartAt\": \"DISABLE_FEATURE_B_PROVIDER\",<br>          \"States\": {<br>            \"DISABLE_FEATURE_B_PROVIDER\": {<br>              \"Type\": \"Task\",<br>              \"End\": true,<br>              \"Parameters\": {<br>                \"Name\": \"/param/B\",<br>                \"Value\": \"valueZ\",<br>                \"Overwrite\": true<br>              },<br>              \"Resource\": \"arn:aws:states:::aws-sdk:ssm:putParameter\"<br>            }<br>          }<br>        }<br>      ],<br>      \"Next\": \"Pass\"<br>    },<br>    \"Pass\": {<br>      \"Type\": \"Pass\",<br>      \"Next\": \"ListTasks\"<br>    },<br>    \"ListTasks\": {<br>      \"Type\": \"Task\",<br>      \"Next\": \"Map\",<br>      \"Parameters\": {<br>        \"Cluster\": \"YOUR_ECS_CLUSTER_NAME\",<br>        \"ServiceName\": \"YOUR_ECS_SERVICE_NAME\"<br>      },<br>      \"Resource\": \"arn:aws:states:::aws-sdk:ecs:listTasks\"<br>    },<br>    \"Map\": {<br>      \"Type\": \"Map\",<br>      \"Iterator\": {<br>        \"StartAt\": \"StopTask\",<br>        \"States\": {<br>          \"StopTask\": {<br>            \"Type\": \"Task\",<br>            \"End\": true,<br>            \"Parameters\": {<br>              \"Task.$\": \"States.ArrayGetItem(States.StringSplit($ ,'/'),2)\",<br>              \"Reason\": \"State Machine SSM Update\",<br>              \"Cluster\": \"YOUR_ECS_CLUSTER_NAME\"<br>            },<br>            \"Resource\": \"arn:aws:states:::aws-sdk:ecs:stopTask\"<br>          }<br>        }<br>      },<br>      \"End\": true,<br>      \"ItemsPath\": \"$.TaskArns\"<br>    }<br>  }<br>}</pre>\n<p>For this whole workflow to work, you will also require IAM role which has access\u00a0to</p>\n<ol>\n<li>SSM Put Parameter</li>\n<li>KMS Key Encrypt and Decrypt (If SSM is encrypted with Customer Managed\u00a0Key)</li>\n<li>ECS List\u00a0Task</li>\n<li>ECS Stop\u00a0Task</li>\n<li>Cloudwatch for\u00a0logging</li>\n</ol>\n<h3>Closing Note</h3>\n<p>While the solution above is not as intuitive and automated like Kubernetes where it knows the dependencies of parameters, it is one of the closest solution to automate repetitive operational task.</p>\n<p>There is more that you can further enhance on this workflow such as taking in more inputs to replace Parameters, or replacing certain set of ECS Service Tasks. This can greatly save you the trouble of updating Terraform parameters or manually updating parameter store\u00a0values.</p>\n<p>I hope this article will give you an idea on how you can automate other parts of your operational tasks!</p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=c5b42e85cd65\" width=\"1\" height=\"1\" alt=\"\">\n","content":"\n<p>One of the really neat feature of Kubernetes is that any changes to Secrets and ConfigMaps will result in recreation of the Pods. However, ECS does not natively come with such functionality.</p>\n<p>While you can hook Parameter Store changes to EventBridge, it doesn\u2019t come with the capability to stop \u201coutdated\u201d ECS task. Hence, another approach is to do it via Step Function. One of the use case of such frequent Parameter Store changes can be feature toggling which is frequently used in testing environments to simulate different scenarios.</p>\n<p>In this article, I will show one of such use case where I have to change couple of Parameter Store values and reloading the ECS\u00a0tasks.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*wIrgEjz7Iy3oVUuv5ph3rA.png\"><figcaption>Step Function</figcaption></figure><p>The general idea from the above\u00a0is</p>\n<ol>\n<li>Enabling/Disabling of certain functionality is passed as input.<br><em>{\u201cenable_feature\u201d: \u201ctrue\u201d}</em>\n</li>\n<li>Based on the choice given, it will route of either of the flow which will update the parameters</li>\n<li>Once Parameters are updated, it fetches all ECS Tasks ID from the given ECS Service. In this example, I am hardcoding to one ECS Service, but it can be easily enhanced to be more\u00a0dynamic</li>\n<li>Once ECS Task ID is retrieved, you will need to use the intrinsic function to split the string for the TASK ID as List Task returns you the ARN of the task. Using the Task ID, you can stop them in order for ECS Service to recreate new ones. In this example, I am not introducing maximum parallelism as I do not require such functionality. But if you are doing this for high availability environments, do add delays and parallelism as ECS tasks may take a while to be\u00a0ready.</li>\n</ol>\n<p>The step function code will look like\u00a0such</p>\n<pre>{<br>  \"Comment\": \"A description of my state machine\",<br>  \"StartAt\": \"IS_FEATURE_ENABLED\",<br>  \"States\": {<br>    \"IS_FEATURE_ENABLED\": {<br>      \"Type\": \"Choice\",<br>      \"Choices\": [<br>        {<br>          \"Variable\": \"$.enable_feature\",<br>          \"StringMatches\": \"true\",<br>          \"Next\": \"ENABLE_FEATURE_PARALLEL\"<br>        }<br>      ],<br>      \"Default\": \"DISABLE_FEATURE_PARALLEL\"<br>    },<br>    \"ENABLE_FEATURE_PARALLEL\": {<br>      \"Type\": \"Parallel\",<br>      \"Branches\": [<br>        {<br>          \"StartAt\": \"ENABLE_FEATURE_A_PROVIDER\",<br>          \"States\": {<br>            \"ENABLE_FEATURE_A_PROVIDER\": {<br>              \"Type\": \"Task\",<br>              \"End\": true,<br>              \"Parameters\": {<br>                \"Name\": \"/param/A\",<br>                \"Value\": \"valueA\",<br>                \"Overwrite\": true<br>              },<br>              \"Resource\": \"arn:aws:states:::aws-sdk:ssm:putParameter\"<br>            }<br>          }<br>        },<br>        {<br>          \"StartAt\": \"ENABLE_FEATURE_B_PROVIDER\",<br>          \"States\": {<br>            \"ENABLE_FEATURE_B_PROVIDER\": {<br>              \"Type\": \"Task\",<br>              \"End\": true,<br>              \"Parameters\": {<br>                \"Name\": \"/param/B\",<br>                \"Value\": \"valueB\",<br>                \"Overwrite\": true<br>              },<br>              \"Resource\": \"arn:aws:states:::aws-sdk:ssm:putParameter\"<br>            }<br>          }<br>        }<br>      ],<br>      \"Next\": \"Pass\"<br>    },<br>    \"DISABLE_FEATURE_PARALLEL\": {<br>      \"Type\": \"Parallel\",<br>      \"Branches\": [<br>        {<br>          \"StartAt\": \"DISABLE_FEATURE_A_PROVIDER\",<br>          \"States\": {<br>            \"DISABLE_FEATURE_A_PROVIDER\": {<br>              \"Type\": \"Task\",<br>              \"End\": true,<br>              \"Parameters\": {<br>                \"Name\": \"/param/A\",<br>                \"Value\": \"valueX\",<br>                \"Overwrite\": true<br>              },<br>              \"Resource\": \"arn:aws:states:::aws-sdk:ssm:putParameter\"<br>            }<br>          }<br>        },<br>        {<br>          \"StartAt\": \"DISABLE_FEATURE_B_PROVIDER\",<br>          \"States\": {<br>            \"DISABLE_FEATURE_B_PROVIDER\": {<br>              \"Type\": \"Task\",<br>              \"End\": true,<br>              \"Parameters\": {<br>                \"Name\": \"/param/B\",<br>                \"Value\": \"valueZ\",<br>                \"Overwrite\": true<br>              },<br>              \"Resource\": \"arn:aws:states:::aws-sdk:ssm:putParameter\"<br>            }<br>          }<br>        }<br>      ],<br>      \"Next\": \"Pass\"<br>    },<br>    \"Pass\": {<br>      \"Type\": \"Pass\",<br>      \"Next\": \"ListTasks\"<br>    },<br>    \"ListTasks\": {<br>      \"Type\": \"Task\",<br>      \"Next\": \"Map\",<br>      \"Parameters\": {<br>        \"Cluster\": \"YOUR_ECS_CLUSTER_NAME\",<br>        \"ServiceName\": \"YOUR_ECS_SERVICE_NAME\"<br>      },<br>      \"Resource\": \"arn:aws:states:::aws-sdk:ecs:listTasks\"<br>    },<br>    \"Map\": {<br>      \"Type\": \"Map\",<br>      \"Iterator\": {<br>        \"StartAt\": \"StopTask\",<br>        \"States\": {<br>          \"StopTask\": {<br>            \"Type\": \"Task\",<br>            \"End\": true,<br>            \"Parameters\": {<br>              \"Task.$\": \"States.ArrayGetItem(States.StringSplit($ ,'/'),2)\",<br>              \"Reason\": \"State Machine SSM Update\",<br>              \"Cluster\": \"YOUR_ECS_CLUSTER_NAME\"<br>            },<br>            \"Resource\": \"arn:aws:states:::aws-sdk:ecs:stopTask\"<br>          }<br>        }<br>      },<br>      \"End\": true,<br>      \"ItemsPath\": \"$.TaskArns\"<br>    }<br>  }<br>}</pre>\n<p>For this whole workflow to work, you will also require IAM role which has access\u00a0to</p>\n<ol>\n<li>SSM Put Parameter</li>\n<li>KMS Key Encrypt and Decrypt (If SSM is encrypted with Customer Managed\u00a0Key)</li>\n<li>ECS List\u00a0Task</li>\n<li>ECS Stop\u00a0Task</li>\n<li>Cloudwatch for\u00a0logging</li>\n</ol>\n<h3>Closing Note</h3>\n<p>While the solution above is not as intuitive and automated like Kubernetes where it knows the dependencies of parameters, it is one of the closest solution to automate repetitive operational task.</p>\n<p>There is more that you can further enhance on this workflow such as taking in more inputs to replace Parameters, or replacing certain set of ECS Service Tasks. This can greatly save you the trouble of updating Terraform parameters or manually updating parameter store\u00a0values.</p>\n<p>I hope this article will give you an idea on how you can automate other parts of your operational tasks!</p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=c5b42e85cd65\" width=\"1\" height=\"1\" alt=\"\">\n","enclosure":{},"categories":["aws-ecs","aws"]},{"title":"The Lambda \u201cRouter\u201d (NodeJS)","pubDate":"2022-11-06 07:21:39","link":"https://medium.com/@kay.renfa/the-lambda-router-nodejs-e1022ea2f683?source=rss-2d401c9d729d------2","guid":"https://medium.com/p/e1022ea2f683","author":"Kay Ren Fa","thumbnail":"","description":"\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/579/1*MbkKbTRCM0pK0g7SD8MlcA.jpeg\"></figure><p>In this article, I will cover one of the requirements my team had to abide as part of organization security policy. Above diagram is a simplified overview of the how our application can communicate with each\u00a0other.</p>\n<ol>\n<li>APP_A doesn\u2019t have IGW/NATGW attached to the VPC for direct access to public internet. Instead, all internet traffic has to be rerouted to\u00a0APIGW_A.</li>\n<li>APIGW_A will be integrated to a lambda function which sits in a special VPC which only allow traffic to flow through internet via a forward\u00a0proxy.</li>\n<li>If APP_A needs to communicate to APP_B, we are not allowed to point directly to APP_B endpoint, instead we have to point to APIGW_B which will then have VPC link to NLB which then leads to the\u00a0APP_B.</li>\n<li>To further complicate the design, each client talking to the APIGW should be using a different API Key. As such, APP_A to APIGW_A should be using KEY_A and Lambda to APIGW_B should be using\u00a0KEY_B.</li>\n</ol>\n<p>Though I do not 100% agree with such architecture design but I do see some benefit of doing so, such as having a more controlled network traffic in and out of the whole ecosystem and having some level of restriction between each nodes. This implementation can also be simplified with just route table for all external network to forward proxy without the need for API\u00a0Gateway.</p>\n<h3><strong>Pre-requisite before the implementation</strong></h3>\n<ol>\n<li>VPC Endpoint for API Gateway execution to be\u00a0created</li>\n<li>Forward Proxy such as Squid with the required address and port to be whitelisted</li>\n<li>Working APP_A and APP_B and EXT_ENDPOINT. For APP_B, it needs to be fronted by a NLB for private API gateway setup to\u00a0work.</li>\n<li>In this example, I will use jsonplaceholder.typicode.com for the external integration.</li>\n</ol>\n<p>We will start off with the variables. We will need the details of the forward proxy ip and port to setup the security group for the lambda\u00a0request.</p>\n<pre>variable \"proxy_ip\" {<br>  type        = string<br>  description = \"Forward proxy ip address\"<br>}</pre>\n<pre>variable \"proxy_port\" {<br>  type        = number<br>  description = \"Forward proxy port\"<br>}</pre>\n<pre>variable \"lambda_vpc\" {<br>  type        = string<br>  description = \"VPC ID of lambda function\"<br>}</pre>\n<pre>variable \"lambda_subnets\" {<br>  type        = list(string)<br>  description = \"List of subnet id where lambda will be procured\"<br>}</pre>\n<pre>variable \"dummy_endpoint\" {<br>  type        = string<br>  description = \"dummy Endpoint\"<br>}</pre>\n<p>Next, we will add some data source. As you can see, i am using remote state to fetch the NLB ARN. Do replace that block of code accordingly.</p>\n<pre>data \"aws_vpc_endpoint\" \"api_gateway_endpoint\" {<br>  vpc_id       = var.vpc_id<br>  service_name = \"com.amazonaws.${var.aws_region}.execute-api\"<br>}</pre>\n<pre>data \"aws_vpc_endpoint\" \"lambda_api_gateway_endpoint\" {<br>  vpc_id       = var.lambda_vpc<br>  service_name = \"com.amazonaws.${var.aws_region}.execute-api\"<br>}</pre>\n<pre>data \"terraform_remote_state\" \"app_b_nlb\" {<br>  backend = \"s3\"<br>  config = {<br>    bucket = var.bucket<br>    key    = format(\"environments/%s/%s/%s/terraform.tfstate\", var.environment, \"app_b\", \"nlb\")<br>    region = var.aws_region<br>  }<br>}</pre>\n<p>Next, we will have couple of helper local variable to facilitate the naming of the resources. Completely optional but we practice this to ensure consistency in the naming convention as we have more parameters as part of the\u00a0naming.</p>\n<pre>locals {<br>  app_a_name = format(\"%s-%s\", var.environment, \"app_a\")<br>  app_b_name = format(\"%s-%s\", var.environment, \"app_b\")<br>}</pre>\n<p>Next, we will start with the key resource which is api gateway and api keys. I have also added the vpc_endpoints for both lambda and APP_A vpc so that it can hit the API gateway endpoint.</p>\n<pre>resource \"aws_api_gateway_rest_api\" \"app_a\" {<br>  name = \"apigw-${local.app_a_name}\"<br>  endpoint_configuration {<br>    types = [\"PRIVATE\"]<br>    vpc_endpoint_ids = [data.aws_vpc_endpoint.lambda_api_gateway_endpoint.id, data.aws_vpc_endpoint.api_gateway_endpoint.id]<br>  }<br>}</pre>\n<pre>resource \"aws_api_gateway_rest_api\" \"app_b\" {<br>  name = \"apigw-${local.app_b_name}\"<br>  endpoint_configuration {<br>    types = [\"PRIVATE\"]<br>    vpc_endpoint_ids = [data.aws_vpc_endpoint.lambda_api_gateway_endpoint.id, data.aws_vpc_endpoint.api_gateway_endpoint.id]<br>  }<br>}</pre>\n<pre>resource \"aws_api_gateway_api_key\" \"app_a\" {<br>  name = \"apigw-${local.app_a_name}-app-key\"<br>}</pre>\n<pre>resource \"aws_api_gateway_api_key\" \"app_a_gw\" {<br>  name = \"apigw-${local.app_b_name}-gw-key\"<br>}</pre>\n<pre>resource \"aws_api_gateway_rest_api_policy\" \"policy\" {<br>  rest_api_id = aws_api_gateway_rest_api.app_a.id<br>  policy      = &lt;&lt;EOF<br>{<br>  \"Version\": \"2012-10-17\",<br>  \"Statement\": [<br>  {<br>    \"Effect\": \"Allow\",<br>    \"Principal\": \"*\",<br>    \"Action\": \"execute-api:Invoke\",<br>    \"Resource\": \"${aws_api_gateway_rest_api.app_a.execution_arn}/*\"<br>  }<br>  ]<br>}<br>EOF<br>}</pre>\n<p>Next, we will work on Stage and usage plan. As we have 2 APIGW, we need to setup 2 sets of resources to\u00a0work.</p>\n<pre>resource \"aws_api_gateway_deployment\" \"app_a\" {<br>  rest_api_id = data.aws_api_gateway_rest_api.app_a.id<br>  triggers = {<br>  redeployment = sha1(jsonencode([<br>    aws_api_gateway_resource.external.id,<br>    aws_api_gateway_resource.external_proxy.id,<br>    aws_api_gateway_method.external.id,<br>    aws_api_gateway_integration.external.id,<br>    aws_api_gateway_resource.app_b.id,<br>    aws_api_gateway_resource.app_b_proxy.id,<br>    aws_api_gateway_method.app_b.id,<br>    aws_api_gateway_integration.app_b.id,<br>  ]))<br>  }<br>  lifecycle {<br>    create_before_destroy = true<br>  }<br>}</pre>\n<pre>resource \"aws_api_gateway_stage\" \"app_a\" {<br>  deployment_id        = aws_api_gateway_deployment.app_a.id<br>  rest_api_id          = data.aws_api_gateway_rest_api.app_a.id<br>  stage_name           = var.environment<br>  xray_tracing_enabled = true<br>}</pre>\n<pre>resource \"aws_api_gateway_method_settings\" \"app_a\" {<br>  rest_api_id = data.aws_api_gateway_rest_api.app_a.id<br>  stage_name  = aws_api_gateway_stage.app_a.stage_name<br>  method_path = \"*/*\"<br>  settings {<br>    metrics_enabled = true<br>    logging_level   = \"INFO\"<br>  }<br>}</pre>\n<pre>resource \"aws_api_gateway_deployment\" \"app_b\" {<br>  rest_api_id = data.aws_api_gateway_rest_api.app_b.id<br>  triggers = {<br>  redeployment = sha1(jsonencode([<br>    aws_api_gateway_resource.app_b_gw.id,<br>    aws_api_gateway_resource.app_b_gw_proxy.id,<br>    aws_api_gateway_method.app_b_gw.id,<br>    aws_api_gateway_integration.app_b_gw.id,<br>  ]))<br>  }<br>  lifecycle {<br>    create_before_destroy = true<br>  }<br>}</pre>\n<pre>resource \"aws_api_gateway_stage\" \"app_b\" {<br>  deployment_id        = aws_api_gateway_deployment.app_a.id<br>  rest_api_id          = data.aws_api_gateway_rest_api.app_b.id<br>  stage_name           = var.environment<br>  xray_tracing_enabled = true<br>}</pre>\n<pre>resource \"aws_api_gateway_method_settings\" \"app_b\" {<br>  rest_api_id = data.aws_api_gateway_rest_api.app_b.id<br>  stage_name  = aws_api_gateway_stage.app_b.stage_name<br>  method_path = \"*/*\"<br>  settings {<br>    metrics_enabled = true<br>    logging_level   = \"INFO\"<br>  }<br>}</pre>\n<pre>resource \"aws_api_gateway_usage_plan\" \"app_a\" {<br>  name        = \"apigw-${local.app_a_name}-usageplan\"<br>  description = \"Usage Plan for ${var.environment} Api GW\"<br>  api_stages {<br>   api_id = data.aws_api_gateway_rest_api.app_a.id<br>    stage  = aws_api_gateway_stage.app_a.stage_name<br>  }<br>}</pre>\n<pre>resource \"aws_api_gateway_usage_plan\" \"app_b\" {<br>  name        = \"apigw-${local.app_b_name}-usageplan\"<br>  description = \"Usage Plan for ${var.environment} Api GW\"<br>  api_stages {<br>   api_id = data.aws_api_gateway_rest_api.app_b.id<br>    stage  = aws_api_gateway_stage.app_b.stage_name<br>  }<br>}</pre>\n<pre>resource \"aws_api_gateway_usage_plan_key\" \"app_a\" {<br>  key_id        = aws_api_gateway_api_key.app_a.id<br>  key_type      = \"API_KEY\"<br>  usage_plan_id = aws_api_gateway_usage_plan.app_a.id<br>}</pre>\n<pre>resource \"aws_api_gateway_usage_plan_key\" \"app_a_gw\" {<br>  key_id        = aws_api_gateway_api_key.app_a_gw.id<br>  key_type      = \"API_KEY\"<br>  usage_plan_id = aws_api_gateway_usage_plan.app_b.id<br>}</pre>\n<p>Next we will work on security group. To be extreme secure, we only allow traffic to VPC Endpoint and Forward Proxy. For VPC Endpoint security group should just allow 443\u00a0Ingress.</p>\n<pre>resource \"aws_security_group\" \"forward_proxy\" {<br>  name        = \"sgrp_forward_proxy\"<br>  vpc_id      = var.lambda_vpc<br>  ingress {<br>    description     = \"APIGW to Lambda\"<br>    from_port       = 443<br>    to_port         = 443<br>    protocol        = \"tcp\"<br>    security_groups = [tolist(data.aws_vpc_endpoint.lambda_api_gateway_endpoint.security_group_ids)[0]]<br>  }<br>  egress {<br>    description = \"Lambda to forward proxy\"<br>    from_port   = var.proxy_port<br>    to_port     = var.proxy_port<br>    protocol    = \"tcp\"<br>    cidr_blocks = [\"${var.proxy_ip}/32\"]<br>  }<br>}</pre>\n<pre>resource \"aws_security_group\" \"reverse_proxy\" {<br>  name        = \"sgrp_reverse_proxy\"<br>  vpc_id      = var.lambda_vpc<br>  ingress {<br>    description     = \"APIGW to Lambda\"<br>    from_port       = 443<br>    to_port         = 443<br>    protocol        = \"tcp\"<br>    security_groups = [tolist(data.aws_vpc_endpoint.lambda_api_gateway_endpoint.security_group_ids)[0]]<br>  }<br>  egress {<br>    description     = \"Lambda to APIGW VPCE\"<br>    from_port       = 443<br>    to_port         = 443<br>    protocol        = \"tcp\"<br>    security_groups = [tolist(data.aws_vpc_endpoint.lambda_api_gateway_endpoint.security_group_ids)[0]]<br>  }<br>}</pre>\n<p>We will also need to setup couple of IAM Role for logging and lambda\u00a0function</p>\n<pre>resource \"aws_iam_role\" \"lambda_role\" {<br>  name = \"iam_role_lambda\"<br>  assume_role_policy = &lt;&lt;POLICY<br>  {<br>    \"Version\": \"2012-10-17\",<br>    \"Statement\": [<br>  {<br>    \"Action\": \"sts:AssumeRole\",<br>    \"Principal\": {<br>      \"Service\": \"lambda.amazonaws.com\"<br>    },<br>    \"Effect\": \"Allow\"<br>    }<br>  ]<br>}<br>POLICY</pre>\n<pre>  inline_policy {<br>  name = \"iam_policy_${local.gcc_name}_lambda\"<br>    policy = jsonencode({<br>    Version = \"2012-10-17\"<br>    Statement = [<br>{<br>  Action = [\"ec2:CreateNetworkInterface\",<br>  \"ec2:DeleteNetworkInterface\",<br>  \"ec2:DescribeNetworkInterfaces\",<br>  \"logs:CreateLogGroup\",<br>  \"logs:CreateLogStream\",<br>  \"logs:PutLogEvents\"]<br>  Effect   = \"Allow\"<br>  Resource = \"*\"<br>  },<br>  ]<br>  })<br>  }<br>}</pre>\n<pre>resource \"aws_iam_role\" \"apigw_log\" {<br>  name = \"iam_role_apigw_log\"<br>  assume_role_policy = &lt;&lt;POLICY<br>{<br>  \"Version\": \"2012-10-17\",<br>  \"Statement\": [<br>  {<br>    \"Action\": \"sts:AssumeRole\",<br>    \"Principal\": {<br>    \"Service\": \"apigateway.amazonaws.com\"<br>  },<br>  \"Effect\": \"Allow\"<br>  }<br>  ]<br>}</pre>\n<pre>POLICY<br>  inline_policy {<br>    name = \"  policy_apigw_log\"<br>    policy = jsonencode({<br>    Version = \"2012-10-17\"<br>    Statement = [<br>    {<br>      Action = [\"logs:CreateLogGroup\",<br>\"logs:CreateLogStream\",<br>\"logs:DescribeLogGroups\",<br>\"logs:DescribeLogStreams\",<br>\"logs:PutLogEvents\"]<br>      Effect   = \"Allow\"<br>      Resource = \"*\"<br>    },<br>  ]<br>  })<br>  }<br>}</pre>\n<pre>// For setting logging<br>resource \"aws_api_gateway_account\" \"apigw_log\" {<br>  cloudwatch_role_arn = aws_iam_role.apigw_log.arn<br>}</pre>\n<p>Finally, after all these setup, we can start on the lambda function. To begin, create a folder and lambda/proxy-service/src and lambda/proxy-service/nodejs which will store the router and lambda layers respectively. From the code below, it does a npm ci --production to download node_modules which will be packaged into lambda\u00a0layers.</p>\n<p>In your local setup, you can also opt to setup this via serverless framework and docker-compose for squid proxy. In this example, I will not go into details for these two\u00a0parts.</p>\n<pre>resource \"null_resource\" \"lambda_layer\" {<br>  provisioner \"local-exec\" {<br>  command = \"cd lambda/proxy-service/nodejs &amp;&amp; npm ci --production\"<br>  }<br>}</pre>\n<pre>data \"archive_file\" \"lambda_layer\" {<br>  type        = \"zip\"<br>  source_dir  = \"lambda/proxy-service\"<br>  output_path = \"tf_lambda_layer.zip\"<br>  excludes    = [\"README.md\", \"package.json\", \"package-lock.json\", \"src\", \"node_modules\"]<br>  depends_on  = [null_resource.lambda_layer]<br>}</pre>\n<pre>resource \"aws_lambda_layer_version\" \"lambda_layer\" {<br>  filename   = \"tf_lambda_layer.zip\"<br>  layer_name = \"axios\"<br>  compatible_runtimes = [\"nodejs16.x\"]<br>  source_code_hash    = data.archive_file.lambda_layer.output_base64sha256<br>}</pre>\n<pre>data \"archive_file\" \"lambda\" {<br>  type        = \"zip\"<br>  source_dir  = \"lambda/proxy-service\"<br>  excludes    = [\"README.md\", \"nodejs\", \"node_modules\"]<br>  output_path = \"tf_lambda.zip\"<br>}</pre>\n<p>Next, we will go into the details for the lambda function. I am using Axios for sending request. This \u201crouter\u201d function takes in the request from application, then send the same request to respective party.</p>\n<p>I also also removed some of the headers from original set as it may reject the request and response. For example, some of the x-amzn header includes VPC details, which APIGW_B will reject the request. Similarly, the Content-Type header will cause the request to be rejected when its not application/json by\u00a0default.</p>\n<p>In the lambda code, it replaces the requested url (with the stage and route) with the expected url. For example <a href=\"https://apigw_a.com/dev/external/posts\">https://APIGW_A.com/dev/external/path/posts</a>?id=1 will be translated to ${env.ENDPOINT}/path/posts?id=1\u00a0. And if the API_KEY is provided, it will be appended into the header which will be sent\u00a0APIGW_B</p>\n<pre>var axios = require(\"axios\");<br>const HttpsProxyAgent = require(\"https-proxy-agent\");<br><br>exports.handler = async function(event, context) {<br>    if (process.env.DEBUG){<br>      console.log('## ENVIRONMENT VARIABLES: ' + serialize(process.env))<br>      console.log('## CONTEXT: ' + serialize(context))<br>      console.log('## EVENT: ' + serialize(event))<br>    }<br><br>    let request = {<br>      method: event.httpMethod,<br>      url: await getUrl(process.env.ENDPOINT_URL, event),<br>      data: tryParseJSONObject(event.body),<br>      headers: getHeader(event.headers)<br>    }<br><br>    if(process.env.PROXY_HOST &amp;&amp; process.env.PROXY_PORT ){<br>      let httpsAgent = new HttpsProxyAgent({host: process.env.PROXY_HOST, port: process.env.PROXY_PORT })<br>      axios = axios.create({httpsAgent});<br>    }<br><br>    let response = await axios(request).then(function (response) {<br>        return {<br>          statusCode: response.status,<br>          body:       (typeof response.data == \"object\") ? serialize(response.data) : response.data<br>        }<br>    }).catch(function(error){<br>      console.log(error.toJSON());<br>      let response = {<br>        statusCode: 500,<br>        body:       serialize({message: `${process.env.AWS_LAMBDA_FUNCTION_NAME} Lambda unexpectedly failed: ${error}`}),<br>      }<br>      if(error.response){<br>        response = {<br>          statusCode: error.response.status,<br>          body: (typeof error.response.data == \"object\") ? serialize(error.response.data) : error.response.data<br>        }<br>      }<br>      return response<br>    });<br>  return response;<br>};<br><br>function serialize(object) {<br>  return JSON.stringify(object, null, 2)<br>}<br><br>function getHeader(header, isRequestHeader = true) {<br>  let sanitized_header = { }<br>  if (header != null &amp;&amp; header != undefined){<br>    let ignore_headers = ['authorization', 'accept-encoding', 'accept', 'user-agent', 'x-forwarded-for']<br>    for (const [key, value] of Object.entries(header)){<br>      let lowercase = key.toLowerCase()<br>      if(ignore_headers.includes(lowercase)){<br>        sanitized_header[key] = value;<br>      }<br>    }<br>  }<br>  if (process.env.API_KEY &amp;&amp; isRequestHeader){<br>    sanitized_header['x-api-key'] = process.env.API_KEY<br>  }<br>  if (process.env.DEBUG){<br>    console.log('## HEADERS: ' + serialize(sanitized_header))<br>  }<br>  return sanitized_header<br>}<br><br>async function getUrl(url, event) {<br>  let base_path = event.path.replace(event.resource.replace('/{proxy+}', ''), '');<br>  let final_url = (event.queryStringParameters) ? `${url}${base_path}?${new URLSearchParams(event.queryStringParameters).toString()}`:`${url}${base_path}`<br>  return final_url<br>}<br><br>// To parse the original body back as json object or string based on the given type<br>function tryParseJSONObject (jsonString){<br>    try {<br>        var o = JSON.parse(jsonString);<br>        if (o &amp;&amp; typeof o === \"object\") {<br>            return o;<br>        }<br>    }<br>    catch (e) { }<br>    return jsonString;<br>};</pre>\n<p>The final piece of the puzzle is the route and lambda integration. For external traffic, it will go through /external path of the\u00a0APIGW_A.</p>\n<pre>resource \"aws_api_gateway_resource\" \"external\" {<br>  path_part   = \"external\"<br>  parent_id   = data.aws_api_gateway_rest_api.app_a.root_resource_id<br>  rest_api_id = data.aws_api_gateway_rest_api.app_a.id<br>}</pre>\n<pre>resource \"aws_api_gateway_resource\" \"external_proxy\" {<br>  path_part   = \"{proxy+}\"<br>  parent_id   = aws_api_gateway_resource.external.id<br>  rest_api_id = data.aws_api_gateway_rest_api.app_a.id<br>}</pre>\n<pre>resource \"aws_api_gateway_method\" \"external\" {<br>  rest_api_id      = data.aws_api_gateway_rest_api.app_a.id<br>  resource_id      = aws_api_gateway_resource.corppass_proxy.id<br>  http_method      = \"ANY\"<br>  authorization    = \"NONE\"<br>  api_key_required = true<br>}</pre>\n<pre>resource \"aws_api_gateway_integration\" \"external\" {<br>  rest_api_id             = data.aws_api_gateway_rest_api.app_a.id<br>  resource_id             = aws_api_gateway_resource.external_proxy.id<br>  http_method             = aws_api_gateway_method.external.http_method<br>  integration_http_method = \"POST\"<br>  type                    = \"AWS_PROXY\"<br>  uri                     = aws_lambda_function.external.invoke_arn<br>}</pre>\n<pre>resource \"aws_lambda_function\" \"external\" {<br>  filename      = \"tf_lambda.zip\"</pre>\n<pre>function_name = \"lambda-${local.app_a_name}-external\"</pre>\n<pre>  role          = data.terraform_remote_state.apigw.outputs.lambda_role_arn<br>  handler       = \"src/index.handler\"<br>  runtime       = \"nodejs16.x\"<br>  timeout       = 60<br>  layers        = [aws_lambda_layer_version.lambda_layer.arn]<br>  source_code_hash = data.archive_file.lambda.output_base64sha256</pre>\n<pre>  vpc_config {<br>  subnet_ids         = var.lambda_vpc_subnets<br>  security_group_ids = [aws_security_group.forward_proxy.id] <br>  }<br>  environment {<br>    variables = {<br>    ENDPOINT_URL = var.dummy_endpoint<br>    PROXY_HOST   = var.proxy_ip<br>    PROXY_PORT   = var.proxy_port<br>    }<br>  }<br>  depends_on = [data.archive_file.lambda]<br>}</pre>\n<pre>resource \"aws_lambda_permission\" \"external\" {<br>  statement_id  = \"AllowExecutionFromAPIGateway\"<br>  action        = \"lambda:InvokeFunction\"<br>  function_name = aws_lambda_function.external.function_name<br>  principal     = \"apigateway.amazonaws.com\"<br>  source_arn = \"arn:aws:execute-api:${var.aws_region}:${data.aws_caller_identity.current.account_id}:${data.aws_api_gateway_rest_api.app_a.id}/*/*/*\"<br>}</pre>\n<p>And for APP_A to APP_B traffic, we will hit /app_b\u00a0. As you can see in the lambda function, we are inputing the API Key and endpoint.</p>\n<pre>resource \"aws_api_gateway_resource\" \"app_a\" {<br>  path_part   = \"app_b\"<br>  parent_id   = data.aws_api_gateway_rest_api.app_a.root_resource_id<br>  rest_api_id = data.aws_api_gateway_rest_api.app_a.id<br>}</pre>\n<pre>resource \"aws_api_gateway_resource\" \"app_a_proxy\" {<br>  path_part   = \"{proxy+}\"<br>  parent_id   = aws_api_gateway_resource.app_a.id<br>  rest_api_id = data.aws_api_gateway_rest_api.app_a.id<br>}</pre>\n<pre>resource \"aws_api_gateway_method\" \"app_a\" {<br>  rest_api_id      = data.aws_api_gateway_rest_api.app_a.id<br>  resource_id      = aws_api_gateway_resource.app_a_proxy.id<br>  http_method      = \"ANY\"<br>  api_key_required = true<br>  authorization = \"NONE\"<br>  request_parameters = {<br>    \"method.request.path.proxy\" = true<br>  }<br>}</pre>\n<pre>resource \"aws_api_gateway_integration\" \"app_a\" {<br>  rest_api_id             = data.aws_api_gateway_rest_api.app_a.id<br>  resource_id             = aws_api_gateway_resource.app_a_proxy.id<br>  http_method             = aws_api_gateway_method.app_a.http_method<br>  integration_http_method = \"POST\"<br>  type                    = \"AWS_PROXY\"<br>  uri                     = aws_lambda_function.app_b.invoke_arn<br>}</pre>\n<pre>resource \"aws_lambda_function\" \"app_b\" {<br>  filename      = \"tf_lambda.zip\"<br>  function_name = \"lambda-${local.app_a_name}-app_b\"<br>  role          = data.terraform_remote_state.apigw.outputs.lambda_role_arn<br>  handler       = \"src/index.handler\"<br>  runtime       = \"nodejs16.x\"<br>  timeout       = 60<br>  layers        = [aws_lambda_layer_version.lambda_layer.arn]<br>  source_code_hash = data.archive_file.lambda.output_base64sha256<br>  vpc_config {<br>    subnet_ids         = var.lambda_vpc_subnets<br>    security_group_ids = [security_group.reverse_proxy.id]<br>  }<br>  environment {<br>    variables = {<br>      ENDPOINT_URL = \"https://${data.aws_api_gateway_rest_api.app_b.id}.execute-api.${var.aws_region}.amazonaws.com/${var.environment}/app_b\"<br>      API_KEY      = aws_api_gateway_api_key.app_a_gw.value<br>    }<br>  }<br>  depends_on = [data.archive_file.lambda]<br>}</pre>\n<pre>resource \"aws_lambda_permission\" \"app_a\" {<br>  statement_id  = \"AllowExecutionFromAPIGateway\"<br>  action        = \"lambda:InvokeFunction\"<br>  function_name = aws_lambda_function.app_a.function_name<br>  principal     = \"apigateway.amazonaws.com\"<br>  source_arn = \"arn:aws:execute-api:${var.aws_region}:${data.aws_caller_identity.current.account_id}:${data.aws_api_gateway_rest_api.app_a.id}/*/*/*\"<br>}</pre>\n<pre>resource \"aws_api_gateway_resource\" \"app_b\" {<br>  path_part   = \"app_b\"<br>  parent_id   = data.aws_api_gateway_rest_api.app_b.root_resource_id<br>  rest_api_id = data.aws_api_gateway_rest_api.app_b.id<br>}</pre>\n<pre>resource \"aws_api_gateway_resource\" \"app_b_proxy\" {<br>  path_part   = \"{proxy+}\"<br>  parent_id   = aws_api_gateway_resource.app_b2.id<br>  rest_api_id = data.aws_api_gateway_rest_api.app_b.id<br>}</pre>\n<pre>resource \"aws_api_gateway_method\" \"app_b\" {<br>  rest_api_id      = data.aws_api_gateway_rest_api.app_b.id<br>  resource_id      = aws_api_gateway_resource.app_b_proxy.id<br>  http_method      = \"ANY\"<br>  api_key_required = true<br>  authorization = \"NONE\"<br>  request_parameters = {<br>    \"method.request.path.proxy\" = true<br>  }<br>}</pre>\n<pre>resource \"aws_api_gateway_integration\" \"app_b\" {<br>  rest_api_id             = data.aws_api_gateway_rest_api.app_b.id<br>  resource_id             = aws_api_gateway_resource.app_b_proxy.id<br>  integration_http_method = \"ANY\"<br>  http_method             = aws_api_gateway_method.app_b.http_method<br>  connection_type         = \"VPC_LINK\"<br>  connection_id           = aws_api_gateway_vpc_link.app_a.id<br>  type                    = \"HTTP_PROXY\"<br>  uri                     = \"https://${data.aws_lb.<a href=\"https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/lb#dns_name\">dns_name</a>}/{proxy}\"<br>  request_parameters = {<br>    \"integration.request.path.proxy\" = \"method.request.path.proxy\"<br>  }<br>}</pre>\n<pre>resource \"aws_api_gateway_vpc_link\" \"app_a\" {<br>  name        = \"api-${local.app_a_name}-vpclink\"<br>  target_arns = [data.aws_lb.NLB.arn]<br>}</pre>\n<pre>data \"aws_lb\" \"nlb\" {<br>  name = \"NLB_NAME\"<br>}</pre>\n<p>You can imagine how things can go haywire when this is example ballooned to multiple different external application integration. However, this can be resolved with terraform modules. Nevertheless, it is a convoluted setup which introduce unnecessary complexity and little benefit. The 2 API Gateway does not provide any benefit in the picture as Private API Gateway provided by AWS does not have any network segment, which contributes no benefit when you split them into\u00a0two.</p>\n<h3>Gotchas</h3>\n<p>As you can see, actually the lambda function can be the actual application logic instead of this \u201crouter\u201d function. However, because our application already have the logic, which is why we didn\u2019t want to duplicate and maintain 2 of the same function.</p>\n<p>In addition, for signed request where URL is part of the request signing, do that not that you should not sign with API gateway URL. Instead, you should sign with the eventual URL and the request should be send to API gateway. This way, the lambda function will take the same request body, retranslate the url to eventual\u00a0url.</p>\n<p>Lastly, we also faced self-signed cert issue when integrating to internal services. The workaround is as simple as packing your self signed cert as a lambda layer and attached to the lambda. then you can configure your lambda environment variable NODE_EXTRA_CA_CERTS as /opt/certname.pem\u00a0. This way, the NodeJS lambda will append with this CA cert when performing any HTTPS\u00a0call.</p>\n<h3>Final Words</h3>\n<p>I have spent a bit chunk of my time trying to resolve the lambda function in the Axios request as I was facing \u201csocket hang up\u201d error and I was not able to pinpoint the issue. I originally thought the error is has to do with missing VPC configuration or API gateway does not allow request from lambda, or it could be some bug behind the Axios library. In the end, I did the exact request via curl and successfully replicated the error which I slowly took out the x-amzn\u00a0header.</p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=e1022ea2f683\" width=\"1\" height=\"1\" alt=\"\">\n","content":"\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/579/1*MbkKbTRCM0pK0g7SD8MlcA.jpeg\"></figure><p>In this article, I will cover one of the requirements my team had to abide as part of organization security policy. Above diagram is a simplified overview of the how our application can communicate with each\u00a0other.</p>\n<ol>\n<li>APP_A doesn\u2019t have IGW/NATGW attached to the VPC for direct access to public internet. Instead, all internet traffic has to be rerouted to\u00a0APIGW_A.</li>\n<li>APIGW_A will be integrated to a lambda function which sits in a special VPC which only allow traffic to flow through internet via a forward\u00a0proxy.</li>\n<li>If APP_A needs to communicate to APP_B, we are not allowed to point directly to APP_B endpoint, instead we have to point to APIGW_B which will then have VPC link to NLB which then leads to the\u00a0APP_B.</li>\n<li>To further complicate the design, each client talking to the APIGW should be using a different API Key. As such, APP_A to APIGW_A should be using KEY_A and Lambda to APIGW_B should be using\u00a0KEY_B.</li>\n</ol>\n<p>Though I do not 100% agree with such architecture design but I do see some benefit of doing so, such as having a more controlled network traffic in and out of the whole ecosystem and having some level of restriction between each nodes. This implementation can also be simplified with just route table for all external network to forward proxy without the need for API\u00a0Gateway.</p>\n<h3><strong>Pre-requisite before the implementation</strong></h3>\n<ol>\n<li>VPC Endpoint for API Gateway execution to be\u00a0created</li>\n<li>Forward Proxy such as Squid with the required address and port to be whitelisted</li>\n<li>Working APP_A and APP_B and EXT_ENDPOINT. For APP_B, it needs to be fronted by a NLB for private API gateway setup to\u00a0work.</li>\n<li>In this example, I will use jsonplaceholder.typicode.com for the external integration.</li>\n</ol>\n<p>We will start off with the variables. We will need the details of the forward proxy ip and port to setup the security group for the lambda\u00a0request.</p>\n<pre>variable \"proxy_ip\" {<br>  type        = string<br>  description = \"Forward proxy ip address\"<br>}</pre>\n<pre>variable \"proxy_port\" {<br>  type        = number<br>  description = \"Forward proxy port\"<br>}</pre>\n<pre>variable \"lambda_vpc\" {<br>  type        = string<br>  description = \"VPC ID of lambda function\"<br>}</pre>\n<pre>variable \"lambda_subnets\" {<br>  type        = list(string)<br>  description = \"List of subnet id where lambda will be procured\"<br>}</pre>\n<pre>variable \"dummy_endpoint\" {<br>  type        = string<br>  description = \"dummy Endpoint\"<br>}</pre>\n<p>Next, we will add some data source. As you can see, i am using remote state to fetch the NLB ARN. Do replace that block of code accordingly.</p>\n<pre>data \"aws_vpc_endpoint\" \"api_gateway_endpoint\" {<br>  vpc_id       = var.vpc_id<br>  service_name = \"com.amazonaws.${var.aws_region}.execute-api\"<br>}</pre>\n<pre>data \"aws_vpc_endpoint\" \"lambda_api_gateway_endpoint\" {<br>  vpc_id       = var.lambda_vpc<br>  service_name = \"com.amazonaws.${var.aws_region}.execute-api\"<br>}</pre>\n<pre>data \"terraform_remote_state\" \"app_b_nlb\" {<br>  backend = \"s3\"<br>  config = {<br>    bucket = var.bucket<br>    key    = format(\"environments/%s/%s/%s/terraform.tfstate\", var.environment, \"app_b\", \"nlb\")<br>    region = var.aws_region<br>  }<br>}</pre>\n<p>Next, we will have couple of helper local variable to facilitate the naming of the resources. Completely optional but we practice this to ensure consistency in the naming convention as we have more parameters as part of the\u00a0naming.</p>\n<pre>locals {<br>  app_a_name = format(\"%s-%s\", var.environment, \"app_a\")<br>  app_b_name = format(\"%s-%s\", var.environment, \"app_b\")<br>}</pre>\n<p>Next, we will start with the key resource which is api gateway and api keys. I have also added the vpc_endpoints for both lambda and APP_A vpc so that it can hit the API gateway endpoint.</p>\n<pre>resource \"aws_api_gateway_rest_api\" \"app_a\" {<br>  name = \"apigw-${local.app_a_name}\"<br>  endpoint_configuration {<br>    types = [\"PRIVATE\"]<br>    vpc_endpoint_ids = [data.aws_vpc_endpoint.lambda_api_gateway_endpoint.id, data.aws_vpc_endpoint.api_gateway_endpoint.id]<br>  }<br>}</pre>\n<pre>resource \"aws_api_gateway_rest_api\" \"app_b\" {<br>  name = \"apigw-${local.app_b_name}\"<br>  endpoint_configuration {<br>    types = [\"PRIVATE\"]<br>    vpc_endpoint_ids = [data.aws_vpc_endpoint.lambda_api_gateway_endpoint.id, data.aws_vpc_endpoint.api_gateway_endpoint.id]<br>  }<br>}</pre>\n<pre>resource \"aws_api_gateway_api_key\" \"app_a\" {<br>  name = \"apigw-${local.app_a_name}-app-key\"<br>}</pre>\n<pre>resource \"aws_api_gateway_api_key\" \"app_a_gw\" {<br>  name = \"apigw-${local.app_b_name}-gw-key\"<br>}</pre>\n<pre>resource \"aws_api_gateway_rest_api_policy\" \"policy\" {<br>  rest_api_id = aws_api_gateway_rest_api.app_a.id<br>  policy      = &lt;&lt;EOF<br>{<br>  \"Version\": \"2012-10-17\",<br>  \"Statement\": [<br>  {<br>    \"Effect\": \"Allow\",<br>    \"Principal\": \"*\",<br>    \"Action\": \"execute-api:Invoke\",<br>    \"Resource\": \"${aws_api_gateway_rest_api.app_a.execution_arn}/*\"<br>  }<br>  ]<br>}<br>EOF<br>}</pre>\n<p>Next, we will work on Stage and usage plan. As we have 2 APIGW, we need to setup 2 sets of resources to\u00a0work.</p>\n<pre>resource \"aws_api_gateway_deployment\" \"app_a\" {<br>  rest_api_id = data.aws_api_gateway_rest_api.app_a.id<br>  triggers = {<br>  redeployment = sha1(jsonencode([<br>    aws_api_gateway_resource.external.id,<br>    aws_api_gateway_resource.external_proxy.id,<br>    aws_api_gateway_method.external.id,<br>    aws_api_gateway_integration.external.id,<br>    aws_api_gateway_resource.app_b.id,<br>    aws_api_gateway_resource.app_b_proxy.id,<br>    aws_api_gateway_method.app_b.id,<br>    aws_api_gateway_integration.app_b.id,<br>  ]))<br>  }<br>  lifecycle {<br>    create_before_destroy = true<br>  }<br>}</pre>\n<pre>resource \"aws_api_gateway_stage\" \"app_a\" {<br>  deployment_id        = aws_api_gateway_deployment.app_a.id<br>  rest_api_id          = data.aws_api_gateway_rest_api.app_a.id<br>  stage_name           = var.environment<br>  xray_tracing_enabled = true<br>}</pre>\n<pre>resource \"aws_api_gateway_method_settings\" \"app_a\" {<br>  rest_api_id = data.aws_api_gateway_rest_api.app_a.id<br>  stage_name  = aws_api_gateway_stage.app_a.stage_name<br>  method_path = \"*/*\"<br>  settings {<br>    metrics_enabled = true<br>    logging_level   = \"INFO\"<br>  }<br>}</pre>\n<pre>resource \"aws_api_gateway_deployment\" \"app_b\" {<br>  rest_api_id = data.aws_api_gateway_rest_api.app_b.id<br>  triggers = {<br>  redeployment = sha1(jsonencode([<br>    aws_api_gateway_resource.app_b_gw.id,<br>    aws_api_gateway_resource.app_b_gw_proxy.id,<br>    aws_api_gateway_method.app_b_gw.id,<br>    aws_api_gateway_integration.app_b_gw.id,<br>  ]))<br>  }<br>  lifecycle {<br>    create_before_destroy = true<br>  }<br>}</pre>\n<pre>resource \"aws_api_gateway_stage\" \"app_b\" {<br>  deployment_id        = aws_api_gateway_deployment.app_a.id<br>  rest_api_id          = data.aws_api_gateway_rest_api.app_b.id<br>  stage_name           = var.environment<br>  xray_tracing_enabled = true<br>}</pre>\n<pre>resource \"aws_api_gateway_method_settings\" \"app_b\" {<br>  rest_api_id = data.aws_api_gateway_rest_api.app_b.id<br>  stage_name  = aws_api_gateway_stage.app_b.stage_name<br>  method_path = \"*/*\"<br>  settings {<br>    metrics_enabled = true<br>    logging_level   = \"INFO\"<br>  }<br>}</pre>\n<pre>resource \"aws_api_gateway_usage_plan\" \"app_a\" {<br>  name        = \"apigw-${local.app_a_name}-usageplan\"<br>  description = \"Usage Plan for ${var.environment} Api GW\"<br>  api_stages {<br>   api_id = data.aws_api_gateway_rest_api.app_a.id<br>    stage  = aws_api_gateway_stage.app_a.stage_name<br>  }<br>}</pre>\n<pre>resource \"aws_api_gateway_usage_plan\" \"app_b\" {<br>  name        = \"apigw-${local.app_b_name}-usageplan\"<br>  description = \"Usage Plan for ${var.environment} Api GW\"<br>  api_stages {<br>   api_id = data.aws_api_gateway_rest_api.app_b.id<br>    stage  = aws_api_gateway_stage.app_b.stage_name<br>  }<br>}</pre>\n<pre>resource \"aws_api_gateway_usage_plan_key\" \"app_a\" {<br>  key_id        = aws_api_gateway_api_key.app_a.id<br>  key_type      = \"API_KEY\"<br>  usage_plan_id = aws_api_gateway_usage_plan.app_a.id<br>}</pre>\n<pre>resource \"aws_api_gateway_usage_plan_key\" \"app_a_gw\" {<br>  key_id        = aws_api_gateway_api_key.app_a_gw.id<br>  key_type      = \"API_KEY\"<br>  usage_plan_id = aws_api_gateway_usage_plan.app_b.id<br>}</pre>\n<p>Next we will work on security group. To be extreme secure, we only allow traffic to VPC Endpoint and Forward Proxy. For VPC Endpoint security group should just allow 443\u00a0Ingress.</p>\n<pre>resource \"aws_security_group\" \"forward_proxy\" {<br>  name        = \"sgrp_forward_proxy\"<br>  vpc_id      = var.lambda_vpc<br>  ingress {<br>    description     = \"APIGW to Lambda\"<br>    from_port       = 443<br>    to_port         = 443<br>    protocol        = \"tcp\"<br>    security_groups = [tolist(data.aws_vpc_endpoint.lambda_api_gateway_endpoint.security_group_ids)[0]]<br>  }<br>  egress {<br>    description = \"Lambda to forward proxy\"<br>    from_port   = var.proxy_port<br>    to_port     = var.proxy_port<br>    protocol    = \"tcp\"<br>    cidr_blocks = [\"${var.proxy_ip}/32\"]<br>  }<br>}</pre>\n<pre>resource \"aws_security_group\" \"reverse_proxy\" {<br>  name        = \"sgrp_reverse_proxy\"<br>  vpc_id      = var.lambda_vpc<br>  ingress {<br>    description     = \"APIGW to Lambda\"<br>    from_port       = 443<br>    to_port         = 443<br>    protocol        = \"tcp\"<br>    security_groups = [tolist(data.aws_vpc_endpoint.lambda_api_gateway_endpoint.security_group_ids)[0]]<br>  }<br>  egress {<br>    description     = \"Lambda to APIGW VPCE\"<br>    from_port       = 443<br>    to_port         = 443<br>    protocol        = \"tcp\"<br>    security_groups = [tolist(data.aws_vpc_endpoint.lambda_api_gateway_endpoint.security_group_ids)[0]]<br>  }<br>}</pre>\n<p>We will also need to setup couple of IAM Role for logging and lambda\u00a0function</p>\n<pre>resource \"aws_iam_role\" \"lambda_role\" {<br>  name = \"iam_role_lambda\"<br>  assume_role_policy = &lt;&lt;POLICY<br>  {<br>    \"Version\": \"2012-10-17\",<br>    \"Statement\": [<br>  {<br>    \"Action\": \"sts:AssumeRole\",<br>    \"Principal\": {<br>      \"Service\": \"lambda.amazonaws.com\"<br>    },<br>    \"Effect\": \"Allow\"<br>    }<br>  ]<br>}<br>POLICY</pre>\n<pre>  inline_policy {<br>  name = \"iam_policy_${local.gcc_name}_lambda\"<br>    policy = jsonencode({<br>    Version = \"2012-10-17\"<br>    Statement = [<br>{<br>  Action = [\"ec2:CreateNetworkInterface\",<br>  \"ec2:DeleteNetworkInterface\",<br>  \"ec2:DescribeNetworkInterfaces\",<br>  \"logs:CreateLogGroup\",<br>  \"logs:CreateLogStream\",<br>  \"logs:PutLogEvents\"]<br>  Effect   = \"Allow\"<br>  Resource = \"*\"<br>  },<br>  ]<br>  })<br>  }<br>}</pre>\n<pre>resource \"aws_iam_role\" \"apigw_log\" {<br>  name = \"iam_role_apigw_log\"<br>  assume_role_policy = &lt;&lt;POLICY<br>{<br>  \"Version\": \"2012-10-17\",<br>  \"Statement\": [<br>  {<br>    \"Action\": \"sts:AssumeRole\",<br>    \"Principal\": {<br>    \"Service\": \"apigateway.amazonaws.com\"<br>  },<br>  \"Effect\": \"Allow\"<br>  }<br>  ]<br>}</pre>\n<pre>POLICY<br>  inline_policy {<br>    name = \"  policy_apigw_log\"<br>    policy = jsonencode({<br>    Version = \"2012-10-17\"<br>    Statement = [<br>    {<br>      Action = [\"logs:CreateLogGroup\",<br>\"logs:CreateLogStream\",<br>\"logs:DescribeLogGroups\",<br>\"logs:DescribeLogStreams\",<br>\"logs:PutLogEvents\"]<br>      Effect   = \"Allow\"<br>      Resource = \"*\"<br>    },<br>  ]<br>  })<br>  }<br>}</pre>\n<pre>// For setting logging<br>resource \"aws_api_gateway_account\" \"apigw_log\" {<br>  cloudwatch_role_arn = aws_iam_role.apigw_log.arn<br>}</pre>\n<p>Finally, after all these setup, we can start on the lambda function. To begin, create a folder and lambda/proxy-service/src and lambda/proxy-service/nodejs which will store the router and lambda layers respectively. From the code below, it does a npm ci --production to download node_modules which will be packaged into lambda\u00a0layers.</p>\n<p>In your local setup, you can also opt to setup this via serverless framework and docker-compose for squid proxy. In this example, I will not go into details for these two\u00a0parts.</p>\n<pre>resource \"null_resource\" \"lambda_layer\" {<br>  provisioner \"local-exec\" {<br>  command = \"cd lambda/proxy-service/nodejs &amp;&amp; npm ci --production\"<br>  }<br>}</pre>\n<pre>data \"archive_file\" \"lambda_layer\" {<br>  type        = \"zip\"<br>  source_dir  = \"lambda/proxy-service\"<br>  output_path = \"tf_lambda_layer.zip\"<br>  excludes    = [\"README.md\", \"package.json\", \"package-lock.json\", \"src\", \"node_modules\"]<br>  depends_on  = [null_resource.lambda_layer]<br>}</pre>\n<pre>resource \"aws_lambda_layer_version\" \"lambda_layer\" {<br>  filename   = \"tf_lambda_layer.zip\"<br>  layer_name = \"axios\"<br>  compatible_runtimes = [\"nodejs16.x\"]<br>  source_code_hash    = data.archive_file.lambda_layer.output_base64sha256<br>}</pre>\n<pre>data \"archive_file\" \"lambda\" {<br>  type        = \"zip\"<br>  source_dir  = \"lambda/proxy-service\"<br>  excludes    = [\"README.md\", \"nodejs\", \"node_modules\"]<br>  output_path = \"tf_lambda.zip\"<br>}</pre>\n<p>Next, we will go into the details for the lambda function. I am using Axios for sending request. This \u201crouter\u201d function takes in the request from application, then send the same request to respective party.</p>\n<p>I also also removed some of the headers from original set as it may reject the request and response. For example, some of the x-amzn header includes VPC details, which APIGW_B will reject the request. Similarly, the Content-Type header will cause the request to be rejected when its not application/json by\u00a0default.</p>\n<p>In the lambda code, it replaces the requested url (with the stage and route) with the expected url. For example <a href=\"https://apigw_a.com/dev/external/posts\">https://APIGW_A.com/dev/external/path/posts</a>?id=1 will be translated to ${env.ENDPOINT}/path/posts?id=1\u00a0. And if the API_KEY is provided, it will be appended into the header which will be sent\u00a0APIGW_B</p>\n<pre>var axios = require(\"axios\");<br>const HttpsProxyAgent = require(\"https-proxy-agent\");<br><br>exports.handler = async function(event, context) {<br>    if (process.env.DEBUG){<br>      console.log('## ENVIRONMENT VARIABLES: ' + serialize(process.env))<br>      console.log('## CONTEXT: ' + serialize(context))<br>      console.log('## EVENT: ' + serialize(event))<br>    }<br><br>    let request = {<br>      method: event.httpMethod,<br>      url: await getUrl(process.env.ENDPOINT_URL, event),<br>      data: tryParseJSONObject(event.body),<br>      headers: getHeader(event.headers)<br>    }<br><br>    if(process.env.PROXY_HOST &amp;&amp; process.env.PROXY_PORT ){<br>      let httpsAgent = new HttpsProxyAgent({host: process.env.PROXY_HOST, port: process.env.PROXY_PORT })<br>      axios = axios.create({httpsAgent});<br>    }<br><br>    let response = await axios(request).then(function (response) {<br>        return {<br>          statusCode: response.status,<br>          body:       (typeof response.data == \"object\") ? serialize(response.data) : response.data<br>        }<br>    }).catch(function(error){<br>      console.log(error.toJSON());<br>      let response = {<br>        statusCode: 500,<br>        body:       serialize({message: `${process.env.AWS_LAMBDA_FUNCTION_NAME} Lambda unexpectedly failed: ${error}`}),<br>      }<br>      if(error.response){<br>        response = {<br>          statusCode: error.response.status,<br>          body: (typeof error.response.data == \"object\") ? serialize(error.response.data) : error.response.data<br>        }<br>      }<br>      return response<br>    });<br>  return response;<br>};<br><br>function serialize(object) {<br>  return JSON.stringify(object, null, 2)<br>}<br><br>function getHeader(header, isRequestHeader = true) {<br>  let sanitized_header = { }<br>  if (header != null &amp;&amp; header != undefined){<br>    let ignore_headers = ['authorization', 'accept-encoding', 'accept', 'user-agent', 'x-forwarded-for']<br>    for (const [key, value] of Object.entries(header)){<br>      let lowercase = key.toLowerCase()<br>      if(ignore_headers.includes(lowercase)){<br>        sanitized_header[key] = value;<br>      }<br>    }<br>  }<br>  if (process.env.API_KEY &amp;&amp; isRequestHeader){<br>    sanitized_header['x-api-key'] = process.env.API_KEY<br>  }<br>  if (process.env.DEBUG){<br>    console.log('## HEADERS: ' + serialize(sanitized_header))<br>  }<br>  return sanitized_header<br>}<br><br>async function getUrl(url, event) {<br>  let base_path = event.path.replace(event.resource.replace('/{proxy+}', ''), '');<br>  let final_url = (event.queryStringParameters) ? `${url}${base_path}?${new URLSearchParams(event.queryStringParameters).toString()}`:`${url}${base_path}`<br>  return final_url<br>}<br><br>// To parse the original body back as json object or string based on the given type<br>function tryParseJSONObject (jsonString){<br>    try {<br>        var o = JSON.parse(jsonString);<br>        if (o &amp;&amp; typeof o === \"object\") {<br>            return o;<br>        }<br>    }<br>    catch (e) { }<br>    return jsonString;<br>};</pre>\n<p>The final piece of the puzzle is the route and lambda integration. For external traffic, it will go through /external path of the\u00a0APIGW_A.</p>\n<pre>resource \"aws_api_gateway_resource\" \"external\" {<br>  path_part   = \"external\"<br>  parent_id   = data.aws_api_gateway_rest_api.app_a.root_resource_id<br>  rest_api_id = data.aws_api_gateway_rest_api.app_a.id<br>}</pre>\n<pre>resource \"aws_api_gateway_resource\" \"external_proxy\" {<br>  path_part   = \"{proxy+}\"<br>  parent_id   = aws_api_gateway_resource.external.id<br>  rest_api_id = data.aws_api_gateway_rest_api.app_a.id<br>}</pre>\n<pre>resource \"aws_api_gateway_method\" \"external\" {<br>  rest_api_id      = data.aws_api_gateway_rest_api.app_a.id<br>  resource_id      = aws_api_gateway_resource.corppass_proxy.id<br>  http_method      = \"ANY\"<br>  authorization    = \"NONE\"<br>  api_key_required = true<br>}</pre>\n<pre>resource \"aws_api_gateway_integration\" \"external\" {<br>  rest_api_id             = data.aws_api_gateway_rest_api.app_a.id<br>  resource_id             = aws_api_gateway_resource.external_proxy.id<br>  http_method             = aws_api_gateway_method.external.http_method<br>  integration_http_method = \"POST\"<br>  type                    = \"AWS_PROXY\"<br>  uri                     = aws_lambda_function.external.invoke_arn<br>}</pre>\n<pre>resource \"aws_lambda_function\" \"external\" {<br>  filename      = \"tf_lambda.zip\"</pre>\n<pre>function_name = \"lambda-${local.app_a_name}-external\"</pre>\n<pre>  role          = data.terraform_remote_state.apigw.outputs.lambda_role_arn<br>  handler       = \"src/index.handler\"<br>  runtime       = \"nodejs16.x\"<br>  timeout       = 60<br>  layers        = [aws_lambda_layer_version.lambda_layer.arn]<br>  source_code_hash = data.archive_file.lambda.output_base64sha256</pre>\n<pre>  vpc_config {<br>  subnet_ids         = var.lambda_vpc_subnets<br>  security_group_ids = [aws_security_group.forward_proxy.id] <br>  }<br>  environment {<br>    variables = {<br>    ENDPOINT_URL = var.dummy_endpoint<br>    PROXY_HOST   = var.proxy_ip<br>    PROXY_PORT   = var.proxy_port<br>    }<br>  }<br>  depends_on = [data.archive_file.lambda]<br>}</pre>\n<pre>resource \"aws_lambda_permission\" \"external\" {<br>  statement_id  = \"AllowExecutionFromAPIGateway\"<br>  action        = \"lambda:InvokeFunction\"<br>  function_name = aws_lambda_function.external.function_name<br>  principal     = \"apigateway.amazonaws.com\"<br>  source_arn = \"arn:aws:execute-api:${var.aws_region}:${data.aws_caller_identity.current.account_id}:${data.aws_api_gateway_rest_api.app_a.id}/*/*/*\"<br>}</pre>\n<p>And for APP_A to APP_B traffic, we will hit /app_b\u00a0. As you can see in the lambda function, we are inputing the API Key and endpoint.</p>\n<pre>resource \"aws_api_gateway_resource\" \"app_a\" {<br>  path_part   = \"app_b\"<br>  parent_id   = data.aws_api_gateway_rest_api.app_a.root_resource_id<br>  rest_api_id = data.aws_api_gateway_rest_api.app_a.id<br>}</pre>\n<pre>resource \"aws_api_gateway_resource\" \"app_a_proxy\" {<br>  path_part   = \"{proxy+}\"<br>  parent_id   = aws_api_gateway_resource.app_a.id<br>  rest_api_id = data.aws_api_gateway_rest_api.app_a.id<br>}</pre>\n<pre>resource \"aws_api_gateway_method\" \"app_a\" {<br>  rest_api_id      = data.aws_api_gateway_rest_api.app_a.id<br>  resource_id      = aws_api_gateway_resource.app_a_proxy.id<br>  http_method      = \"ANY\"<br>  api_key_required = true<br>  authorization = \"NONE\"<br>  request_parameters = {<br>    \"method.request.path.proxy\" = true<br>  }<br>}</pre>\n<pre>resource \"aws_api_gateway_integration\" \"app_a\" {<br>  rest_api_id             = data.aws_api_gateway_rest_api.app_a.id<br>  resource_id             = aws_api_gateway_resource.app_a_proxy.id<br>  http_method             = aws_api_gateway_method.app_a.http_method<br>  integration_http_method = \"POST\"<br>  type                    = \"AWS_PROXY\"<br>  uri                     = aws_lambda_function.app_b.invoke_arn<br>}</pre>\n<pre>resource \"aws_lambda_function\" \"app_b\" {<br>  filename      = \"tf_lambda.zip\"<br>  function_name = \"lambda-${local.app_a_name}-app_b\"<br>  role          = data.terraform_remote_state.apigw.outputs.lambda_role_arn<br>  handler       = \"src/index.handler\"<br>  runtime       = \"nodejs16.x\"<br>  timeout       = 60<br>  layers        = [aws_lambda_layer_version.lambda_layer.arn]<br>  source_code_hash = data.archive_file.lambda.output_base64sha256<br>  vpc_config {<br>    subnet_ids         = var.lambda_vpc_subnets<br>    security_group_ids = [security_group.reverse_proxy.id]<br>  }<br>  environment {<br>    variables = {<br>      ENDPOINT_URL = \"https://${data.aws_api_gateway_rest_api.app_b.id}.execute-api.${var.aws_region}.amazonaws.com/${var.environment}/app_b\"<br>      API_KEY      = aws_api_gateway_api_key.app_a_gw.value<br>    }<br>  }<br>  depends_on = [data.archive_file.lambda]<br>}</pre>\n<pre>resource \"aws_lambda_permission\" \"app_a\" {<br>  statement_id  = \"AllowExecutionFromAPIGateway\"<br>  action        = \"lambda:InvokeFunction\"<br>  function_name = aws_lambda_function.app_a.function_name<br>  principal     = \"apigateway.amazonaws.com\"<br>  source_arn = \"arn:aws:execute-api:${var.aws_region}:${data.aws_caller_identity.current.account_id}:${data.aws_api_gateway_rest_api.app_a.id}/*/*/*\"<br>}</pre>\n<pre>resource \"aws_api_gateway_resource\" \"app_b\" {<br>  path_part   = \"app_b\"<br>  parent_id   = data.aws_api_gateway_rest_api.app_b.root_resource_id<br>  rest_api_id = data.aws_api_gateway_rest_api.app_b.id<br>}</pre>\n<pre>resource \"aws_api_gateway_resource\" \"app_b_proxy\" {<br>  path_part   = \"{proxy+}\"<br>  parent_id   = aws_api_gateway_resource.app_b2.id<br>  rest_api_id = data.aws_api_gateway_rest_api.app_b.id<br>}</pre>\n<pre>resource \"aws_api_gateway_method\" \"app_b\" {<br>  rest_api_id      = data.aws_api_gateway_rest_api.app_b.id<br>  resource_id      = aws_api_gateway_resource.app_b_proxy.id<br>  http_method      = \"ANY\"<br>  api_key_required = true<br>  authorization = \"NONE\"<br>  request_parameters = {<br>    \"method.request.path.proxy\" = true<br>  }<br>}</pre>\n<pre>resource \"aws_api_gateway_integration\" \"app_b\" {<br>  rest_api_id             = data.aws_api_gateway_rest_api.app_b.id<br>  resource_id             = aws_api_gateway_resource.app_b_proxy.id<br>  integration_http_method = \"ANY\"<br>  http_method             = aws_api_gateway_method.app_b.http_method<br>  connection_type         = \"VPC_LINK\"<br>  connection_id           = aws_api_gateway_vpc_link.app_a.id<br>  type                    = \"HTTP_PROXY\"<br>  uri                     = \"https://${data.aws_lb.<a href=\"https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/lb#dns_name\">dns_name</a>}/{proxy}\"<br>  request_parameters = {<br>    \"integration.request.path.proxy\" = \"method.request.path.proxy\"<br>  }<br>}</pre>\n<pre>resource \"aws_api_gateway_vpc_link\" \"app_a\" {<br>  name        = \"api-${local.app_a_name}-vpclink\"<br>  target_arns = [data.aws_lb.NLB.arn]<br>}</pre>\n<pre>data \"aws_lb\" \"nlb\" {<br>  name = \"NLB_NAME\"<br>}</pre>\n<p>You can imagine how things can go haywire when this is example ballooned to multiple different external application integration. However, this can be resolved with terraform modules. Nevertheless, it is a convoluted setup which introduce unnecessary complexity and little benefit. The 2 API Gateway does not provide any benefit in the picture as Private API Gateway provided by AWS does not have any network segment, which contributes no benefit when you split them into\u00a0two.</p>\n<h3>Gotchas</h3>\n<p>As you can see, actually the lambda function can be the actual application logic instead of this \u201crouter\u201d function. However, because our application already have the logic, which is why we didn\u2019t want to duplicate and maintain 2 of the same function.</p>\n<p>In addition, for signed request where URL is part of the request signing, do that not that you should not sign with API gateway URL. Instead, you should sign with the eventual URL and the request should be send to API gateway. This way, the lambda function will take the same request body, retranslate the url to eventual\u00a0url.</p>\n<p>Lastly, we also faced self-signed cert issue when integrating to internal services. The workaround is as simple as packing your self signed cert as a lambda layer and attached to the lambda. then you can configure your lambda environment variable NODE_EXTRA_CA_CERTS as /opt/certname.pem\u00a0. This way, the NodeJS lambda will append with this CA cert when performing any HTTPS\u00a0call.</p>\n<h3>Final Words</h3>\n<p>I have spent a bit chunk of my time trying to resolve the lambda function in the Axios request as I was facing \u201csocket hang up\u201d error and I was not able to pinpoint the issue. I originally thought the error is has to do with missing VPC configuration or API gateway does not allow request from lambda, or it could be some bug behind the Axios library. In the end, I did the exact request via curl and successfully replicated the error which I slowly took out the x-amzn\u00a0header.</p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=e1022ea2f683\" width=\"1\" height=\"1\" alt=\"\">\n","enclosure":{},"categories":["lambda-function","aws","api-gateway"]},{"title":"Ansible Playbook using EC2 Plugin via Portforwarding to SSH Port via Session Manager","pubDate":"2022-08-26 15:49:57","link":"https://medium.com/@kay.renfa/ansible-playbook-using-ec2-plugin-via-session-manager-from-ssh-port-e68d5a962d2b?source=rss-2d401c9d729d------2","guid":"https://medium.com/p/e68d5a962d2b","author":"Kay Ren Fa","thumbnail":"","description":"\n<p>AWS Session Manager provides a nifty way to allow user to hop into the EC2, ECS even though workload resides in private subnet. You can also enable SSH connection via Session manager via CLI. In this article, I will cover how to implement this\u00a0feature.</p>\n<h3>Pre-requisite</h3>\n<ul>\n<li>A set of EC2 instances with amazon-ssm-manager installed and running with key pair configured and security group opened for connectivity.</li>\n<li>Ansible installed on local\u00a0machine</li>\n</ul>\n<p>How can we perform both functionalities at the same time via Ansible? After some researching, I realised that there are no mention of such solution.</p>\n<p>Firstly, you can start off with using EC2 dynamic inventory plugin. It is extremely useful when your CI/CD strategy involves constant recreation of EC2 instances via AMI. You will need to enable aws_ec2 plugin in your ansible.cfg file.</p>\n<pre>#ansible.cfg<br>[defaults]<br>inventory_plugins = plugins/inventory<br>host_key_checking = False</pre>\n<pre>[inventory]<br>enable_plugins = aws_ec2, host_list, script, auto, yaml, ini, toml</pre>\n<p>Next, you need an inventory file suffixed with \u201caws_ec2.yml\u201d. Based on the inventory below, the plugin will retrieve EC2 instance that has \u201cEnvironment: dev\u201d tags. We then add them into \u201cdev\u201d group. This will be very useful when you have \u201cgroup_vars\u201d variables that you want this group to inherit dev variables.</p>\n<pre>#inventories/dev_aws_ec2.yml<br>---<br>plugin: aws_ec2<br>regions:<br>  - us-east-2<br>filters:<br>  tag:Environment: dev<br>hostnames:<br>  - instance-id<br>groups:<br>  dev: true</pre>\n<p>Inside your group_vars, you can have environment specific variables. You can then define the SSM bucket which is used to transfer files during playbook execution and the private key to perform ssh connection. This setup will be great as you segregate different set of resource and keys per environment.</p>\n<pre>#group_vars/dev/vars<br>ansible_ssh_private_key_file: ~/.ssh/dev.pem<br>url: dev.company.com</pre>\n<p>The last pieces to the puzzle is the ansible_user which will be used for ssh connection with the private key and the ssh_common_args which is similar to what is suggested in AWS documentation.</p>\n<pre># start.yml<br>- name: Start service<br>  hosts: dev<br>  vars:<br>    ansible_user: ec2-user<br>    ansible_become_method: sudo<br>    ansible_ssh_common_args: '-o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -o ProxyCommand=\"aws ssm start-session --target %h --document-name AWS-StartSSHSession --parameters ''portNumber=%p''\"'<br>  gather_facts: no<br>  become: yes<br>  tasks:<br>  - name: Start crond<br>    service:<br>      name: '{{ item }}'<br>      state: started<br>      enabled: yes<br>    loop:<br>     - crond</pre>\n<p>By running the command below, it should be able to loop through the set of EC2 instances filtered by Environment tag.</p>\n<pre>ansible-playbook -i inventories/dev start.yml</pre>\n<h3>Gotchas</h3>\n<p>Unlike typical playbook, using the ec2 plugin, instead of IP address, you will connect with instance id. Hence, take note when writing playbook when expecting the results to be IP\u00a0address.</p>\n<p>You can list the variables generated via the plugin to determine what is the underlying values.</p>\n<pre>ansible-inventory -i inventories/dev_aws_ec2.yml --list / --graph</pre>\n<h3>References</h3>\n<ol>\n<li><a href=\"https://docs.ansible.com/ansible/latest/collections/community/aws/aws_ssm_connection.html\">https://docs.ansible.com/ansible/latest/collections/community/aws/aws_ssm_connection.html</a></li>\n<li><a href=\"https://docs.ansible.com/ansible/latest/collections/amazon/aws/aws_ec2_inventory.html\">https://docs.ansible.com/ansible/latest/collections/amazon/aws/aws_ec2_inventory.html</a></li>\n<li><a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/session-manager-getting-started-enable-ssh-connections.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/session-manager-getting-started-enable-ssh-connections.html</a></li>\n</ol>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=e68d5a962d2b\" width=\"1\" height=\"1\" alt=\"\">\n","content":"\n<p>AWS Session Manager provides a nifty way to allow user to hop into the EC2, ECS even though workload resides in private subnet. You can also enable SSH connection via Session manager via CLI. In this article, I will cover how to implement this\u00a0feature.</p>\n<h3>Pre-requisite</h3>\n<ul>\n<li>A set of EC2 instances with amazon-ssm-manager installed and running with key pair configured and security group opened for connectivity.</li>\n<li>Ansible installed on local\u00a0machine</li>\n</ul>\n<p>How can we perform both functionalities at the same time via Ansible? After some researching, I realised that there are no mention of such solution.</p>\n<p>Firstly, you can start off with using EC2 dynamic inventory plugin. It is extremely useful when your CI/CD strategy involves constant recreation of EC2 instances via AMI. You will need to enable aws_ec2 plugin in your ansible.cfg file.</p>\n<pre>#ansible.cfg<br>[defaults]<br>inventory_plugins = plugins/inventory<br>host_key_checking = False</pre>\n<pre>[inventory]<br>enable_plugins = aws_ec2, host_list, script, auto, yaml, ini, toml</pre>\n<p>Next, you need an inventory file suffixed with \u201caws_ec2.yml\u201d. Based on the inventory below, the plugin will retrieve EC2 instance that has \u201cEnvironment: dev\u201d tags. We then add them into \u201cdev\u201d group. This will be very useful when you have \u201cgroup_vars\u201d variables that you want this group to inherit dev variables.</p>\n<pre>#inventories/dev_aws_ec2.yml<br>---<br>plugin: aws_ec2<br>regions:<br>  - us-east-2<br>filters:<br>  tag:Environment: dev<br>hostnames:<br>  - instance-id<br>groups:<br>  dev: true</pre>\n<p>Inside your group_vars, you can have environment specific variables. You can then define the SSM bucket which is used to transfer files during playbook execution and the private key to perform ssh connection. This setup will be great as you segregate different set of resource and keys per environment.</p>\n<pre>#group_vars/dev/vars<br>ansible_ssh_private_key_file: ~/.ssh/dev.pem<br>url: dev.company.com</pre>\n<p>The last pieces to the puzzle is the ansible_user which will be used for ssh connection with the private key and the ssh_common_args which is similar to what is suggested in AWS documentation.</p>\n<pre># start.yml<br>- name: Start service<br>  hosts: dev<br>  vars:<br>    ansible_user: ec2-user<br>    ansible_become_method: sudo<br>    ansible_ssh_common_args: '-o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -o ProxyCommand=\"aws ssm start-session --target %h --document-name AWS-StartSSHSession --parameters ''portNumber=%p''\"'<br>  gather_facts: no<br>  become: yes<br>  tasks:<br>  - name: Start crond<br>    service:<br>      name: '{{ item }}'<br>      state: started<br>      enabled: yes<br>    loop:<br>     - crond</pre>\n<p>By running the command below, it should be able to loop through the set of EC2 instances filtered by Environment tag.</p>\n<pre>ansible-playbook -i inventories/dev start.yml</pre>\n<h3>Gotchas</h3>\n<p>Unlike typical playbook, using the ec2 plugin, instead of IP address, you will connect with instance id. Hence, take note when writing playbook when expecting the results to be IP\u00a0address.</p>\n<p>You can list the variables generated via the plugin to determine what is the underlying values.</p>\n<pre>ansible-inventory -i inventories/dev_aws_ec2.yml --list / --graph</pre>\n<h3>References</h3>\n<ol>\n<li><a href=\"https://docs.ansible.com/ansible/latest/collections/community/aws/aws_ssm_connection.html\">https://docs.ansible.com/ansible/latest/collections/community/aws/aws_ssm_connection.html</a></li>\n<li><a href=\"https://docs.ansible.com/ansible/latest/collections/amazon/aws/aws_ec2_inventory.html\">https://docs.ansible.com/ansible/latest/collections/amazon/aws/aws_ec2_inventory.html</a></li>\n<li><a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/session-manager-getting-started-enable-ssh-connections.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/session-manager-getting-started-enable-ssh-connections.html</a></li>\n</ol>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=e68d5a962d2b\" width=\"1\" height=\"1\" alt=\"\">\n","enclosure":{},"categories":["aws","ansible"]},{"title":"Hassle-free AWS ECS BlueGreen CodePipeline","pubDate":"2022-07-05 05:33:13","link":"https://medium.com/@kay.renfa/aws-ecs-bluegreen-codepipeline-with-private-git-repository-9268a3a65da6?source=rss-2d401c9d729d------2","guid":"https://medium.com/p/9268a3a65da6","author":"Kay Ren Fa","thumbnail":"","description":"\n<h3>Pre-requisite knowledge</h3>\n<ul>\n<li>Terraform</li>\n<li>Docker</li>\n<li>Basic AWS ECS knowledge</li>\n<li>DevOps BlueGreen Deployment</li>\n</ul>\n<h3>The problem</h3>\n<p>Given this scenario\u00a0:</p>\n<ul>\n<li>Your application is running on\u00a0ECS.</li>\n<li>Your git repository is stored in other Git Repository.</li>\n<li>Continuous Integration is already handled by another orchestration tool such as Gitlab, Jenkins for your development environment.</li>\n<li>You have several AWS accounts for different environments (e.g. staging, production)</li>\n<li>Your accounts are managed by Terraform.</li>\n<li>You need a simplified Continuous Deployment workflow to deploy your application to these accounts.</li>\n<li>The problem you are facing is how to have the least tedious way to rollout deployments to these environments.</li>\n</ul>\n<p>Task definition is the blueprint of what the task should be created. Prior to ECS CodeDeploy, you need to provide a task definition and appspec file. But what are some of the options to do\u00a0so?</p>\n<p>One option is to have another Git repository in CodeCommit with these files inside for CodePipeline to pull as a source artifact and passed down to CodeDeploy. While this option works, you essentially need to manage 2 source of places (i.e. terraform and CodeCommit) whenever any changes to task definition.</p>\n<p>Alternatively, you can dynamically generate the task definition and appspec file. This means that if your infrastructure is managed by Terraform, any changes to the task definition, CodePipeline will always get the latest information. In this article, I will cover how I have approached it.</p>\n<h3>Pre-requisite</h3>\n<p>You need to have following stuff setup and information beforehand</p>\n<ul>\n<li>ECR with\u00a0images</li>\n<li>ECS Cluster</li>\n<li>Running ECS\u00a0Service</li>\n</ul>\n<h3>Source</h3>\n<p>For the whole CD process to work, you need to define which ECR and image tag you want CodePipeline to deploy at the end. CodePipeline will pull image tag via the\u00a0SHA256.</p>\n<p>For instance you set \u201cnginx:stable\u201d as the source. Whenever there is an external CI/CD workflow that builds new image and retag as \u201cnginx:stable\u201d, CodePipeline will fetch the newest\u00a0image.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/846/1*ey6qHhRmANFrD5d59zC3DQ.png\"></figure><p>To further level up this process, you can integrate CodePipeline to Eventbridge to monitor the ECR push events trigger CodePipeline. Below is a snippet of the code to implement via terraform.</p>\n<pre>resource \"aws_cloudwatch_event_rule\" \"image_push\" {<br>name          = \"cwrule-\"<br>role_arn      = var.iam_role_cloudwatch_name<br>event_pattern = &lt;&lt;EOF<br>{<br>  \"source\": [ \"aws.ecr\" ],<br>  \"detail\": {<br>     \"action-type\": [ \"PUSH\" ],<br>     \"image-tag\": [ \"${local.cloudwatch_ecr_tag}\" ],<br>     \"repository-name\": [ \"${local.repo_name}\" ],<br>     \"result\": [ \"SUCCESS\" ] <br>  },<br>  \"detail-type\": [ \"ECR Image Action\" ] <br>}<br>EOF<br>}</pre>\n<h3>Build</h3>\n<p>CodeBuild is the essential part of this whole integration. CodeBuild runs on an environment which will generate the taskdef.json and appspec.yaml.</p>\n<p>In the previous stage, AWS generates imageDetail.json which contains information of the images in the ECR. This is essential to \u201cinject\u201d into the task definition file. The appspec.yaml defines which revision of task definition to be used for deployment. However, this information isn\u2019t readily accessible for us every time! Therefore, CodeBuild and CodeDeploy use placeholders to replace these values on the fly. Hence, there is no need to constantly pushing the new revision to some git repository.</p>\n<pre># appspec.yaml<br>Resources:<br>- TargetService:<br>   Type: AWS::ECS::Service<br>   Properties:<br>     TaskDefinition: &lt;TASK_DEFINITION&gt;<br>     LoadBalancerInfo:         <br>       ContainerName: \"app\"<br>       ContainerPort: 8080</pre>\n<p>This fixes one part of the puzzle. How about task definition? Well, the best way is to query your task definition for the latest revision. This fixes the need to manage multiple task definition from various source and about any potential docker image cache with identical image tag. With couple of jq commands, you can replace the latest task definition image with a placeholder. This essentially fixes all loose\u00a0ends!</p>\n<p>Without much ado, this is how the buildspec.yml can handles\u00a0it</p>\n<pre># buildspec.yml<br>version: 0.2<br>phases:<br>  install:<br>    runtime-versions:<br>      docker: 18<br>  build:<br>    commands:<br>      - \"printf 'version: 0.0\\nResources:\\n  - TargetService:\\n      Type: AWS::ECS::Service\\n      Properties:\\n        TaskDefinition: &lt;TASK_DEFINITION&gt;\\n        LoadBalancerInfo:\\n          ContainerName: \\\"${container_name}\\\"\\n          ContainerPort: ${container_port}' &gt; appspec.yaml\"<br>      - aws ecs describe-task-definition --output json --task-definition ${task_definition} --query taskDefinition &gt; template.json<br>      - jq '.containerDefinitions | map((select(.name == \"${container_name}\") | .image) |= \"&lt;IMAGE1_NAME&gt;\") | {\"containerDefinitions\":.}' template.json &gt; template2.json<br>      - jq -s '.[0] * .[1]' template.json template2.json &gt; taskdef.json<br>artifacts:<br>  files:<br>    - imageDetail.json<br>    - appspec.yaml<br>    - taskdef.json</pre>\n<p>What happened above is that that after the latest task definition is retrieved. jq command retrieves the containerDefinition section, and look for the container with the given name e.g. \u201capp\u201d. Then it will replace that portion with the placeholder &lt;IMAGE1_NAME&gt;. CodeBuild will then replace these placeholders with actual values before it zips all artifacts to a S3\u00a0bucket.</p>\n<h3>Deploy</h3>\n<p>CodeDeploy will pick up these 3 files and proceed with deployment. For BlueGreen deployment, you need to provide a load balancer with 2 target groups, ideally application load balancer for more flexibility.</p>\n<p>ECS will manipulate these 2 target groups and 2 listeners to perform a BlueGreen deployment. For example, LISTENER_LIVE(port: 443) and LISTENER_TEST (port: 8443), TG_A (blue) and TG_B\u00a0(green).</p>\n<p>When a new deployment triggers, new task are created as TG_B and this get routed to LISTENER_TEST. You can then perform testing with LB DNS port 8443 for the new spun up instances. Once approved on CodeDeploy, ECS will map LISTENER_LIVE to TG_B which means now TG_B will be live. Once deployment is done, traffic from ECS tasks in TG_A will be drained and terminated.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*X36DFXDu-u7e6M2tcLruMA.png\"></figure><p>Subject to your needs, you can customise your deployment strategies such\u00a0as</p>\n<ul>\n<li>How task are replaced (e.g. AllatOnce, HalfAtATime)</li>\n<li>Whether you want a manual approval before the BlueGreen swap occurs. This gives you a window to test the Green application</li>\n<li>Whether you want the blue to retain for sometime before termination</li>\n</ul>\n<h3>CodePipeline</h3>\n<p>Lastly, CodePipeline glues these 3 portions together. Following terraform snippet will help in integrating them together.</p>\n<pre>resource \"aws_codepipeline\" \"ecs_pipeline\" {<br>name     = \"codepipeline-${var.application_name}\"<br>role_arn = var.iam_role_codepipeline_name<br>artifact_store {<br>  location = var.bucket_name<br>  type     = \"S3\"<br>}<br>stage {<br>  name = \"Source\"<br>  action {<br>    name             = \"ImagePush\"<br>    category         = \"Source\"<br>    owner            = \"AWS\"<br>    provider         = \"ECR\"<br>    version          = \"1\"<br>    output_artifacts = [\"Image\"]<br>    configuration = {<br>      RepositoryName = var.ecr_registery_name<br>      ImageTag       = var.ecr_registery_tag<br>    }<br>  }<br>}<br>stage {<br>  name = \"Build\"<br>  action {<br>    name             = \"Build\"<br>    category         = \"Build\"<br>    owner            = \"AWS\"<br>    provider         = \"CodeBuild\"<br>    input_artifacts  = [\"Image\"]<br>    output_artifacts = [\"BuildArtifact\"]<br>    version          = \"1\"<br>    configuration = {<br>      ProjectName = var.codeDeploy_project_name<br>      PrimarySource = \"SourceArtifact\"<br>    }<br>  }<br>}<br>stage {<br>  name = \"Deploy\"<br>  action {<br>    name      = \"Deploy\"<br>    category  = \"Deploy\"<br>    owner     = \"AWS\"<br>    provider  = \"CodeDeployToECS\"<br>    version   = \"1\"<br>    run_order = 1<br>    input_artifacts = [\"BuildArtifact\"]<br>    configuration = {<br>      ApplicationName = var.codeDeploy_name<br>      DeploymentGroupName = var.codeDeploy_group_name<br>      TaskDefinitionTemplateArtifact = \"BuildArtifact\"<br>      TaskDefinitionTemplatePath     = \"taskdef.json\"<br>      AppSpecTemplateArtifact        = \"BuildArtifact\"<br>      AppSpecTemplatePath            = \"appspec.yaml\"<br>      Image1ArtifactName             = \"BuildArtifact\"<br>      Image1ContainerName            = \"IMAGE1_NAME\"<br>    }<br>  }<br> }<br>}</pre>\n<h3>Final Thoughts</h3>\n<p>It is definitely daunting for anyone who are new to setting up a CI/CD workflow on AWS as it appears to be a black-box behind the whole integration and a lot of reading required to understand what is required at each phases. But it make sense once everything fall into\u00a0place.</p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=9268a3a65da6\" width=\"1\" height=\"1\" alt=\"\">\n","content":"\n<h3>Pre-requisite knowledge</h3>\n<ul>\n<li>Terraform</li>\n<li>Docker</li>\n<li>Basic AWS ECS knowledge</li>\n<li>DevOps BlueGreen Deployment</li>\n</ul>\n<h3>The problem</h3>\n<p>Given this scenario\u00a0:</p>\n<ul>\n<li>Your application is running on\u00a0ECS.</li>\n<li>Your git repository is stored in other Git Repository.</li>\n<li>Continuous Integration is already handled by another orchestration tool such as Gitlab, Jenkins for your development environment.</li>\n<li>You have several AWS accounts for different environments (e.g. staging, production)</li>\n<li>Your accounts are managed by Terraform.</li>\n<li>You need a simplified Continuous Deployment workflow to deploy your application to these accounts.</li>\n<li>The problem you are facing is how to have the least tedious way to rollout deployments to these environments.</li>\n</ul>\n<p>Task definition is the blueprint of what the task should be created. Prior to ECS CodeDeploy, you need to provide a task definition and appspec file. But what are some of the options to do\u00a0so?</p>\n<p>One option is to have another Git repository in CodeCommit with these files inside for CodePipeline to pull as a source artifact and passed down to CodeDeploy. While this option works, you essentially need to manage 2 source of places (i.e. terraform and CodeCommit) whenever any changes to task definition.</p>\n<p>Alternatively, you can dynamically generate the task definition and appspec file. This means that if your infrastructure is managed by Terraform, any changes to the task definition, CodePipeline will always get the latest information. In this article, I will cover how I have approached it.</p>\n<h3>Pre-requisite</h3>\n<p>You need to have following stuff setup and information beforehand</p>\n<ul>\n<li>ECR with\u00a0images</li>\n<li>ECS Cluster</li>\n<li>Running ECS\u00a0Service</li>\n</ul>\n<h3>Source</h3>\n<p>For the whole CD process to work, you need to define which ECR and image tag you want CodePipeline to deploy at the end. CodePipeline will pull image tag via the\u00a0SHA256.</p>\n<p>For instance you set \u201cnginx:stable\u201d as the source. Whenever there is an external CI/CD workflow that builds new image and retag as \u201cnginx:stable\u201d, CodePipeline will fetch the newest\u00a0image.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/846/1*ey6qHhRmANFrD5d59zC3DQ.png\"></figure><p>To further level up this process, you can integrate CodePipeline to Eventbridge to monitor the ECR push events trigger CodePipeline. Below is a snippet of the code to implement via terraform.</p>\n<pre>resource \"aws_cloudwatch_event_rule\" \"image_push\" {<br>name          = \"cwrule-\"<br>role_arn      = var.iam_role_cloudwatch_name<br>event_pattern = &lt;&lt;EOF<br>{<br>  \"source\": [ \"aws.ecr\" ],<br>  \"detail\": {<br>     \"action-type\": [ \"PUSH\" ],<br>     \"image-tag\": [ \"${local.cloudwatch_ecr_tag}\" ],<br>     \"repository-name\": [ \"${local.repo_name}\" ],<br>     \"result\": [ \"SUCCESS\" ] <br>  },<br>  \"detail-type\": [ \"ECR Image Action\" ] <br>}<br>EOF<br>}</pre>\n<h3>Build</h3>\n<p>CodeBuild is the essential part of this whole integration. CodeBuild runs on an environment which will generate the taskdef.json and appspec.yaml.</p>\n<p>In the previous stage, AWS generates imageDetail.json which contains information of the images in the ECR. This is essential to \u201cinject\u201d into the task definition file. The appspec.yaml defines which revision of task definition to be used for deployment. However, this information isn\u2019t readily accessible for us every time! Therefore, CodeBuild and CodeDeploy use placeholders to replace these values on the fly. Hence, there is no need to constantly pushing the new revision to some git repository.</p>\n<pre># appspec.yaml<br>Resources:<br>- TargetService:<br>   Type: AWS::ECS::Service<br>   Properties:<br>     TaskDefinition: &lt;TASK_DEFINITION&gt;<br>     LoadBalancerInfo:         <br>       ContainerName: \"app\"<br>       ContainerPort: 8080</pre>\n<p>This fixes one part of the puzzle. How about task definition? Well, the best way is to query your task definition for the latest revision. This fixes the need to manage multiple task definition from various source and about any potential docker image cache with identical image tag. With couple of jq commands, you can replace the latest task definition image with a placeholder. This essentially fixes all loose\u00a0ends!</p>\n<p>Without much ado, this is how the buildspec.yml can handles\u00a0it</p>\n<pre># buildspec.yml<br>version: 0.2<br>phases:<br>  install:<br>    runtime-versions:<br>      docker: 18<br>  build:<br>    commands:<br>      - \"printf 'version: 0.0\\nResources:\\n  - TargetService:\\n      Type: AWS::ECS::Service\\n      Properties:\\n        TaskDefinition: &lt;TASK_DEFINITION&gt;\\n        LoadBalancerInfo:\\n          ContainerName: \\\"${container_name}\\\"\\n          ContainerPort: ${container_port}' &gt; appspec.yaml\"<br>      - aws ecs describe-task-definition --output json --task-definition ${task_definition} --query taskDefinition &gt; template.json<br>      - jq '.containerDefinitions | map((select(.name == \"${container_name}\") | .image) |= \"&lt;IMAGE1_NAME&gt;\") | {\"containerDefinitions\":.}' template.json &gt; template2.json<br>      - jq -s '.[0] * .[1]' template.json template2.json &gt; taskdef.json<br>artifacts:<br>  files:<br>    - imageDetail.json<br>    - appspec.yaml<br>    - taskdef.json</pre>\n<p>What happened above is that that after the latest task definition is retrieved. jq command retrieves the containerDefinition section, and look for the container with the given name e.g. \u201capp\u201d. Then it will replace that portion with the placeholder &lt;IMAGE1_NAME&gt;. CodeBuild will then replace these placeholders with actual values before it zips all artifacts to a S3\u00a0bucket.</p>\n<h3>Deploy</h3>\n<p>CodeDeploy will pick up these 3 files and proceed with deployment. For BlueGreen deployment, you need to provide a load balancer with 2 target groups, ideally application load balancer for more flexibility.</p>\n<p>ECS will manipulate these 2 target groups and 2 listeners to perform a BlueGreen deployment. For example, LISTENER_LIVE(port: 443) and LISTENER_TEST (port: 8443), TG_A (blue) and TG_B\u00a0(green).</p>\n<p>When a new deployment triggers, new task are created as TG_B and this get routed to LISTENER_TEST. You can then perform testing with LB DNS port 8443 for the new spun up instances. Once approved on CodeDeploy, ECS will map LISTENER_LIVE to TG_B which means now TG_B will be live. Once deployment is done, traffic from ECS tasks in TG_A will be drained and terminated.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*X36DFXDu-u7e6M2tcLruMA.png\"></figure><p>Subject to your needs, you can customise your deployment strategies such\u00a0as</p>\n<ul>\n<li>How task are replaced (e.g. AllatOnce, HalfAtATime)</li>\n<li>Whether you want a manual approval before the BlueGreen swap occurs. This gives you a window to test the Green application</li>\n<li>Whether you want the blue to retain for sometime before termination</li>\n</ul>\n<h3>CodePipeline</h3>\n<p>Lastly, CodePipeline glues these 3 portions together. Following terraform snippet will help in integrating them together.</p>\n<pre>resource \"aws_codepipeline\" \"ecs_pipeline\" {<br>name     = \"codepipeline-${var.application_name}\"<br>role_arn = var.iam_role_codepipeline_name<br>artifact_store {<br>  location = var.bucket_name<br>  type     = \"S3\"<br>}<br>stage {<br>  name = \"Source\"<br>  action {<br>    name             = \"ImagePush\"<br>    category         = \"Source\"<br>    owner            = \"AWS\"<br>    provider         = \"ECR\"<br>    version          = \"1\"<br>    output_artifacts = [\"Image\"]<br>    configuration = {<br>      RepositoryName = var.ecr_registery_name<br>      ImageTag       = var.ecr_registery_tag<br>    }<br>  }<br>}<br>stage {<br>  name = \"Build\"<br>  action {<br>    name             = \"Build\"<br>    category         = \"Build\"<br>    owner            = \"AWS\"<br>    provider         = \"CodeBuild\"<br>    input_artifacts  = [\"Image\"]<br>    output_artifacts = [\"BuildArtifact\"]<br>    version          = \"1\"<br>    configuration = {<br>      ProjectName = var.codeDeploy_project_name<br>      PrimarySource = \"SourceArtifact\"<br>    }<br>  }<br>}<br>stage {<br>  name = \"Deploy\"<br>  action {<br>    name      = \"Deploy\"<br>    category  = \"Deploy\"<br>    owner     = \"AWS\"<br>    provider  = \"CodeDeployToECS\"<br>    version   = \"1\"<br>    run_order = 1<br>    input_artifacts = [\"BuildArtifact\"]<br>    configuration = {<br>      ApplicationName = var.codeDeploy_name<br>      DeploymentGroupName = var.codeDeploy_group_name<br>      TaskDefinitionTemplateArtifact = \"BuildArtifact\"<br>      TaskDefinitionTemplatePath     = \"taskdef.json\"<br>      AppSpecTemplateArtifact        = \"BuildArtifact\"<br>      AppSpecTemplatePath            = \"appspec.yaml\"<br>      Image1ArtifactName             = \"BuildArtifact\"<br>      Image1ContainerName            = \"IMAGE1_NAME\"<br>    }<br>  }<br> }<br>}</pre>\n<h3>Final Thoughts</h3>\n<p>It is definitely daunting for anyone who are new to setting up a CI/CD workflow on AWS as it appears to be a black-box behind the whole integration and a lot of reading required to understand what is required at each phases. But it make sense once everything fall into\u00a0place.</p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=9268a3a65da6\" width=\"1\" height=\"1\" alt=\"\">\n","enclosure":{},"categories":["ec","codepipeline","aws","devops"]}]}